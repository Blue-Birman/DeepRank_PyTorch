{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from importlib import reload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeprank.dataset import DataLoader, PairGenerator, ListGenerator\n",
    "from deeprank import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4b85d28a90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1234\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[./data/letor/r5w/word_dict.txt]\n",
      "\tWord dict size: 193367\n",
      "[./data/letor/r5w/qid_query.txt]\n",
      "\tData size: 1692\n",
      "[./data/letor/r5w/docid_doc.txt]\n",
      "\tData size: 65323\n",
      "[./data/letor/r5w/embed_wiki-pdc_d50_norm]\n",
      "\tEmbedding size: 109282\n",
      "Generate numpy embed: (193368, 50)\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader('./config/letor07_mp_fold1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "letor_config = json.loads(open('./config/letor07_mp_fold1.model').read())\n",
    "#device = torch.device(\"cuda\")\n",
    "#device = torch.device(\"cpu\")\n",
    "select_device = torch.device(\"cpu\")\n",
    "rank_device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[./data/letor/r5w/relation.train.fold1.txt]\n",
      "\tInstance size: 47828\n",
      "Pair Instance Count: 325439\n"
     ]
    }
   ],
   "source": [
    "Letor07Path = letor_config['data_dir']\n",
    "\n",
    "letor_config['fill_word'] = loader._PAD_\n",
    "letor_config['embedding'] = loader.embedding\n",
    "letor_config['feat_size'] = loader.feat_size\n",
    "letor_config['vocab_size'] = loader.embedding.shape[0]\n",
    "letor_config['embed_dim'] = loader.embedding.shape[1]\n",
    "letor_config['pad_value'] = loader._PAD_\n",
    "\n",
    "pair_gen = PairGenerator(rel_file=Letor07Path + '/relation.train.fold%d.txt'%(letor_config['fold']), \n",
    "                         config=letor_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeprank import select_module\n",
    "from deeprank import rank_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryCentricNet()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letor_config['max_match'] = 20\n",
    "letor_config['win_size'] = 5\n",
    "select_net = select_module.QueryCentricNet(config=letor_config, out_device=rank_device)\n",
    "select_net = select_net.to(select_device)\n",
    "select_net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nletor_config['q_limit'] = 20\\nletor_config['d_limit'] = 2000\\nletor_config['max_match'] = 20\\nletor_config['win_size'] = 5\\nletor_config['finetune_embed'] = True\\nletor_config['lr'] = 0.0001\\nselect_net = select_module.PointerNet(config=letor_config)\\nselect_net = select_net.to(device)\\nselect_net.embedding.weight.data.copy_(torch.from_numpy(loader.embedding))\\nselect_net.train()\\nselect_optimizer = optim.RMSprop(select_net.parameters(), lr=letor_config['lr'])\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "letor_config['q_limit'] = 20\n",
    "letor_config['d_limit'] = 2000\n",
    "letor_config['max_match'] = 20\n",
    "letor_config['win_size'] = 5\n",
    "letor_config['finetune_embed'] = True\n",
    "letor_config['lr'] = 0.0001\n",
    "select_net = select_module.PointerNet(config=letor_config)\n",
    "select_net = select_net.to(device)\n",
    "select_net.embedding.weight.data.copy_(torch.from_numpy(loader.embedding))\n",
    "select_net.train()\n",
    "select_optimizer = optim.RMSprop(select_net.parameters(), lr=letor_config['lr'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "letor_config[\"dim_q\"] = 1\n",
    "letor_config[\"dim_d\"] = 1\n",
    "letor_config[\"dim_weight\"] = 1\n",
    "letor_config[\"c_reduce\"] = [1, 1]\n",
    "letor_config[\"k_reduce\"] = [1, 50]\n",
    "letor_config[\"s_reduce\"] = 1\n",
    "letor_config[\"p_reduce\"] = [0, 0]\n",
    "\n",
    "letor_config[\"c_en_conv_out\"] = 4\n",
    "letor_config[\"k_en_conv\"] = 3\n",
    "letor_config[\"s_en_conv\"] = 1\n",
    "letor_config[\"p_en_conv\"] = 1\n",
    "\n",
    "letor_config[\"en_pool_out\"] = [1, 1]\n",
    "letor_config[\"en_leaky\"] = 0.2\n",
    "\n",
    "letor_config[\"dim_gru_hidden\"] = 3\n",
    "\n",
    "letor_config['lr'] = 0.005\n",
    "letor_config['finetune_embed'] = False\n",
    "\n",
    "rank_net = rank_module.DeepRankNet(config=letor_config)\n",
    "rank_net = rank_net.to(rank_device)\n",
    "rank_net.embedding.weight.data.copy_(torch.from_numpy(loader.embedding))\n",
    "rank_net.train()\n",
    "rank_optimizer = optim.Adam(rank_net.parameters(), lr=letor_config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(*variables, device):\n",
    "    return (torch.from_numpy(variable).to(device) for variable in variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_text(x):\n",
    "    print(' '.join([loader.word_dict[w.item()] for w in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coin vend amusement machine service repair occupational outlook handbook edition department labor bureau labor statistics bulletin coin vend amusement machine service repair nature work working conditions employment training qualifications advancement job outlook earnings related occupation source additional information significant points most worker learn skill job opportunity good person knowledge electronics nature work section back top coin vend amusement machine familiar sight offices convenience stores arcade casino coin operate machine give change dispense refreshments test senses spit lottery ticket nearly turn coin vend amusement machine service repair install service stock machine keep good working order vend machine service call route driver visit machine dispense soft drink candy snack item collect money machine restock merchandise change label indicate new selection keep machine clean appealing vend machine repair call mechanics technician make sure machine operate correct check complicated electrical electronic machine beverage dispenser make sure machine mix drink properly refrigeration heating unit work correct relatively simple gravity operate machine service check keypad motor merchandise chute test coin bill change making mechanism install machine vend machine repair make necessary water electrical connection check machine proper operation make sure installation comply local plumbing electrical code vend machine dispense food worker vend machine service comply state local public health sanitation standard amusement machine service repair work jukebox video games pinball machine slot machine make sure various lever joystick mechanism function properly games remain fair jukebox selection accurate update selection repair replace malfunction parts rebuild existing equipment work game industry adhere strict guidelines federal state agency regulate game machine preventive maintenance avoid trouble start major job repair example periodical clean refrigeration condenser lubricate mechanical parts adjust machine perform properly machine break vend amusement machine repair inspect obvious problem loose electrical wire malfunction coin mechanism bill validate leak service electronic machine repair test hand held diagnostic computer determine extent location problem repair replace circuit board component fix problem problem readily located worker refer technical manual wiring diagram use test device electrical circuit tester find defective parts repair decide replace part fix malfunction onsite send machine repair shop repair shop vend amusement machine repair use power tool grind wheels drill voltmeter ohmmeter oscilloscope test equipment use ordinary repair tool screwdriver pliers wrench vend machine service repair employ small company fill fix machine regular basis combination service repair stock machine collect money fill coin currency change repair machine necessary service repair paperwork file report prepare repair cost estimate order parts keeping daily record merchandise distribute money collected new machine computerize inventory control reduce paperwork service complete working conditions section back top vend amusement machine repair work primarily company repair shop spend substantial time road visiting machine placed repair generally work total hour vend amusement machine operate clock repair call work night weekend holiday vend amusement machine repair shop generally quiet light adequate workspace service machine location work done pedestrian traffic heavy busy supermarket industrial complex offices casino arcade repair work relatively safe service repair take care avoid hazard electrical shock cut sharp tool metal object follow safe work procedure especially moving heavy vend amusement machine employment section back top coin vend amusement machine service repair held job most repair work vend company sell food item machine work soft drink bottle company coin operate machine grow number service repair work amusement establishment video games pinball machine jukebox slot machine similar type amusement equipment vend amusement machine service repair employ country most located area large population vend amusement machine training qualifications advancement section back top most worker learn skill job new worker train fill fix machine informal job observing working receiving instruction experienced repair employer normally prefer hire high school graduate high school vocational school course electricity refrigeration machine repair advantage qualify entry level job employer usually require applicant demonstrate mechanical ability work experience score mechanical aptitude test coin vend amusement machine service repair handle thousand dollar merchandise cash employer hire person record honesty ability deal tactful people important service repair play significant role relay customer request concern driver license good driving record essential most vend amusement machine service repair job employer require service bonded electronics prevalent vend amusement machine employer increasingly prefer applicant training electronics technological advanced machine features multilevel price inventory control scroll message use electronics microchip computer extensive vocational high school junior college offer training program basic electronics beginner start training simple job clean stocking machine learn rebuild machine remove defective parts repair adjust test machine accompany experienced repair service call finally make visit learning process take month years depend individual abilities previous education type machine service quality instruction national automatic merchandise association self study technician training program vend machine repair repair use manual instruction subject customer relations safety electronics schematic reading completion program repair pass written test certify technician journeyman learn new machine repair service attend training sessions sponsor manufacturer machine distributor days trainee experienced worker take evening course basic electricity electronics microwave oven refrigeration related subject stay top new technique equipment skilled service repair promote supervisory job business job outlook section back top job opening coin vend amusement machine service repair arise employment growth replace experienced worker transfer occupation leave labor force opportunity good person knowledge electronics electronic circuitry important component vend amusement machine firm find train experienced worker job likely train qualified route driver hire inexperienced people acquire mechanical electrical electronics training taking high school vocational course employment coin vend amusement machine service repair expect grow fast average occupation increase number vend amusement machine operation establishment likely install additional vend machine industrial plant hospital stores school meet public demand inexpensive snack food item increase vend machine business employee range product dispense machine expect increase vend machine continue increasingly automate begin incorporate microwave oven mini refrigerator freezer addition casino arcade amusement establishment increase source entertainment state multistate lottery increasingly use coin operate machine sell scratch ticket grocery stores public place circuit boards vend machine replace reprogramm machine accept new bill increase service repair improve technology new machine moderate employment growth machine require maintenance frequent old new machine repair restock contain computer record sales inventory data reduce amount time consume paperwork internet beginning play large role monitor vend machine remote location additional new machine use wireless data transmitter signal vend machine company machine needs restock repair allow service repair dispatch check machine regular schedule earnings section back top median hourly earnings coin vend amusement machine service repair middle percent earn hour lowest percent earn hour highest percent earn hour median hourly earnings coin vend amusement machine service repair miscellaneous amusement recreation service nonstore retailer respectively typically states form legalize game highest wages most coin vend amusement machine service repair work hour days receive premium pay overtime union contract stipulate higher pay emergency repair job weekend holiday regular hour vend machine repair service member international brotherhood teamster related occupation section back top worker repair equipment electrical electronic component electrical electronics install repair electronic home entertainment equipment install repair heating air condition refrigeration mechanics install home appliance repair source additional information section back top disclaimer links non bls internet site provided convenience constitute endorse information job opportunity field obtain local vend machine firm local offices state employment service general information vend machine repair write national automatic merchandise association wack dr suite chicago il internet http www vend org automatic merchandise vend group cygnus business media po box janesville ave fort atkinson wi select industry employ coin vend amusement machine service repair appear career guide industry amusement recreation service food process hotel lodging place wholesale trade ooh onet code section back top suggest citation bureau labor statistics department labor occupational outlook handbook edition coin vend amusement machine service repair internet http www bls gov oco htm visit december source bureau labor statistics stat bls gov oco print htm $ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slot machine malfunction $$ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "0 jukebox video games pinball machine slot machine make sure various lever\n",
      "1 video games pinball machine jukebox slot machine similar type amusement equipment\n",
      "2 [PAD] [PAD] coin vend amusement machine service repair occupational outlook handbook\n",
      "3 statistics bulletin coin vend amusement machine service repair nature work working\n",
      "4 back top coin vend amusement machine familiar sight offices convenience stores\n"
     ]
    }
   ],
   "source": [
    "X1, X1_len, X1_id, X2, X2_len, X2_id, Y, F = \\\n",
    "        pair_gen.get_batch(data1=loader.query_data, data2=loader.doc_data)\n",
    "X1, X1_len, X2, X2_len, Y, F = \\\n",
    "        to_device(X1, X1_len, X2, X2_len, Y, F, device=rank_device)\n",
    "\n",
    "show_text(X2[0])\n",
    "\n",
    "X1, X2_new, X1_len, X2_len_new, X2_pos = select_net(X1, X2, X1_len, X2_len, X1_id, X2_id)\n",
    "\n",
    "show_text(X1[0])\n",
    "for i in range(5):\n",
    "    print(i, end=' ')\n",
    "    show_text(X2_new[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([215., 546.,   3.,  19.,  58.,  68.,  83.,  89.,  95., 101., 110., 119.,\n",
      "        123., 130., 137., 142., 155., 169., 171., 179., 191., 196., 235., 284.,\n",
      "        333.], device='cuda:0'), tensor([ 0., 11.,  1.], device='cuda:0'), tensor([347., 349., 139., 353., 386.], device='cuda:0'), tensor([ 15., 273., 294., 322., 350., 356., 383., 393., 405., 457., 506., 520.,\n",
      "        571., 575., 599., 610., 612., 684., 691., 711., 712., 121.],\n",
      "       device='cuda:0'), tensor([339., 346., 367., 380., 391.,  19.,  61.,  71., 149., 177., 208., 225.,\n",
      "        503., 610.], device='cuda:0'), tensor([ 47., 327., 369., 407.,  58., 184., 260., 281., 333., 384., 435., 452.],\n",
      "       device='cuda:0'), tensor([  2.,  19.,  21.,  25.,  36., 111.,   1.,  18.,  20.,  28.,  34.,  35.,\n",
      "        110.], device='cuda:0'), tensor([ 3., 14.,  1., 12., 19., 49.], device='cuda:0'), tensor([  1.,   8.,  10.,  21.,  34.,  49.,  69.,  74., 110., 120., 148., 172.,\n",
      "        192., 208., 254., 267., 279., 293., 297.,   2.,   9.,  11.,  19.,  29.,\n",
      "        317.], device='cuda:0'), tensor([ 386.,  662.,  676.,  694.,  702.,  706.,  863.,  986., 1030., 1094.,\n",
      "        1138., 1517., 1570., 1583., 1646., 1669.,  111.,  230.,  365.,  439.,\n",
      "         443.,  469.,  498.,  521.,  548.,  626.,  665.,  865.,  886.,  923.,\n",
      "         978., 1001., 1015., 1024., 1055., 1096.,   81.,   88.,  151.,  172.,\n",
      "         186.,  191.,  241.,  408.,  453.,  466.,  478.,  486.,  492.,  533.,\n",
      "         902.,  916.,  931.,  951.,  962.,  975.], device='cuda:0'), tensor([335., 339., 392., 470.,   6.,  17.,  30.,  42.,  58.,  73., 102., 123.,\n",
      "        132., 162., 173., 209., 229., 237., 239., 264., 298., 303., 306., 317.,\n",
      "          7.,  18.,  43., 133., 174., 210., 240., 371., 449.], device='cuda:0'), tensor([2., 3.], device='cuda:0'), tensor([ 31.,  56.,  63.,  71.,  91., 154., 174., 191., 203., 240., 285., 304.,\n",
      "        306., 343., 347., 382., 499., 525., 531., 536.,   3.,   8.,  22.,  50.,\n",
      "         53.,  66.,  79.,  84.,  98., 104., 109., 112., 177., 195., 206., 209.,\n",
      "        213., 222., 223., 232.,  23., 483.,  21.,  26.,  49.,  65.,  74.,  78.,\n",
      "         83.,  97., 103., 108., 111., 116., 122., 125., 138., 194., 208., 212.,\n",
      "        218., 226.], device='cuda:0'), tensor([ 1., 23.,  3.,  5.,  8.,  6.,  9., 17.], device='cuda:0'), tensor([139., 141., 142.], device='cuda:0'), tensor([23.,  1.,  6.,  2.,  7.], device='cuda:0'), tensor([6.8000e+01, 5.7800e+02, 1.0680e+03, 1.0820e+03, 1.1330e+03, 1.5120e+03,\n",
      "        1.0000e+00, 1.0000e+01, 1.7000e+01, 4.3000e+01, 4.6000e+01, 7.1000e+01,\n",
      "        1.1400e+02, 1.4600e+02, 1.9500e+02, 4.1200e+02, 5.5900e+02, 5.8300e+02,\n",
      "        6.0700e+02, 6.2200e+02, 6.3800e+02, 8.4300e+02, 9.4100e+02, 1.0670e+03,\n",
      "        1.1120e+03, 1.1810e+03, 7.5000e+01, 5.4800e+02, 5.8100e+02, 8.6200e+02],\n",
      "       device='cuda:0'), tensor([432.,   1.,   7.,  88., 106., 109., 134., 154., 160., 171., 222., 264.,\n",
      "        270., 273., 289., 297., 353., 380., 441., 444., 476., 243., 248., 325.],\n",
      "       device='cuda:0'), tensor([2., 3.], device='cuda:0'), tensor([ 2.,  6., 10.,  3.,  7., 11.], device='cuda:0'), tensor([  0.,   1.,   2.,   3.,   4.,  11.,  16.,  34.,  37.,  42.,  45.,  62.,\n",
      "         65.,  76.,  79.,  90.,  99., 103., 112., 137.], device='cuda:0'), tensor([ 27.,  39.,  46.,  54., 156., 172., 193., 201., 214., 221., 233., 259.,\n",
      "        269., 284., 302., 320., 327., 348., 350., 353.], device='cuda:0'), tensor([582., 588., 618., 623., 789., 621.], device='cuda:0'), tensor([  3.,   4.,   9., 107., 153., 162., 173., 287., 383., 436., 443., 521.,\n",
      "        536., 566., 577., 693., 713., 753., 766., 800.], device='cuda:0'), tensor([114.,  16., 101., 102., 144., 148., 151., 157., 166.], device='cuda:0'), tensor([ 959., 1007., 1035., 1296., 1538.,  960.,  998., 1008., 1036., 1040.,\n",
      "        1256., 1297., 1304., 1338., 1358., 1367., 1486., 1495., 1539., 1546.,\n",
      "        1550., 1579.], device='cuda:0'), tensor([   3.,   10.,   44.,   60.,   79.,  106.,  118.,  160.,  165.,  176.,\n",
      "         186.,  197.,  207.,  211.,  226.,  243.,  281.,  286.,  321.,  350.,\n",
      "         595.,  754.,  811., 1686.,  653.,  695.], device='cuda:0'), tensor([   8.,   54.,   66.,   92.,  110.,  113.,  120.,  128.,  133.,  143.,\n",
      "         144.,  150.,  192.,  229.,  238.,  251.,  262.,  284.,  297.,  305.,\n",
      "         310.,  432.,  493.,  644.,  723.,  779., 1096., 1176., 1269., 1293.,\n",
      "        1314., 1501., 1602., 1629., 1866., 1905., 1933., 1954.,  118.,  926.,\n",
      "        1082., 1321., 1359., 1518., 1295., 1503.,  106.,  569., 1259.],\n",
      "       device='cuda:0'), tensor([ 294.,  302.,  313.,  485.,  511.,  516.,  525.,  528.,  645.,  653.,\n",
      "         657.,  665.,  677.,  690., 1081., 1090., 1125., 1129., 1150., 1153.,\n",
      "        1881.], device='cuda:0'), tensor([  1.,  27.,  33.,  38.,  43.,  51.,  71.,  99., 114., 133., 147., 219.,\n",
      "        322., 325., 329., 345., 427., 468., 476., 479.], device='cuda:0'), tensor([  5., 128., 145., 155., 199., 227., 255., 275.,   6., 129., 256.],\n",
      "       device='cuda:0'), tensor([ 290.,  311.,  350.,  545.,  548.,  586.,  593.,  600.,  607.,  613.,\n",
      "         619.,  662.,  698.,  704.,  710.,  717.,  722.,  939., 1092., 1307.,\n",
      "          64.,   80.,  857., 1046., 1522., 1526., 1534., 1540.],\n",
      "       device='cuda:0'), tensor([ 393.,  467.,  911., 1387., 1803.,  137.,  148.,  394.,  411.,  424.,\n",
      "         452.,  468.,  545.,  560.,  650.,  653.,  655.,  677.,  806.,  814.,\n",
      "         881., 1071., 1085., 1167., 1174.,    2.,   12.,   20.,   73.,   84.,\n",
      "         101.,  108.,  113.,  135.,  269.,  287.,  298.,  309.,  352.,  387.,\n",
      "         413.,  418.,  430.,  436.,  457.], device='cuda:0'), tensor([  49.,  272.,  779.,  787.,  818.,  823.,  844.,  846.,  852.,  874.,\n",
      "         889.,  901.,  904.,  908.,  912.,  915.,  931.,  937., 1024., 1054.,\n",
      "        1878.], device='cuda:0'), tensor([28.,  5., 10., 22., 26.,  6., 11., 23., 27., 12.], device='cuda:0'), tensor([ 0., 58., 62., 67.,  1., 59., 63., 65., 68., 70.], device='cuda:0'), tensor([  0.,  26., 617., 836.,   1.,   5.,  27.,  52.,  58.,  67.,  74.,  77.,\n",
      "         86., 124., 132., 174., 180., 182., 202., 219., 245., 251., 261., 293.,\n",
      "        678., 679., 680., 683., 691., 692., 698., 703., 713., 722., 774., 784.,\n",
      "        791., 794., 796., 798., 801., 809., 816., 821., 437., 443., 493.],\n",
      "       device='cuda:0'), tensor([  0.,   3.,   9.,  15.,  53., 134., 230., 460.,   1.,   4.,  10.,  16.,\n",
      "         54., 135., 231., 253., 333.,   2.,  17.,  55., 102., 187.],\n",
      "       device='cuda:0'), tensor([1524., 1536., 1541., 1627.,  315.,  914., 1545., 1554., 1611., 1677.,\n",
      "        1768., 1777.,  313.,  316.,  338.,  442., 1749., 1769., 1779.,  289.,\n",
      "         292.,  295.,  297.,  299.,  301.,  303.,  305.,  390.,  392.,  394.,\n",
      "         420.,  424.,  434.,  443.,  453.,  586.], device='cuda:0'), tensor([1222., 1428., 1812., 1839., 1996.,   27.,  345.,  362.,  371.,  964.,\n",
      "         970., 1004., 1006., 1175., 1180., 1184., 1190., 1202., 1204., 1206.,\n",
      "        1209., 1211., 1213., 1223., 1225.,   35.,   37.,  303.,  347.,  352.,\n",
      "         360.,  369.,  492.,  498.,  982., 1054., 1102., 1123., 1235., 1239.,\n",
      "        1240., 1244., 1256., 1272., 1281.], device='cuda:0'), tensor([ 23.,  30.,  34.,  44.,  60.,  63.,  67.,  70.,  73.,  24.,  31.,  35.,\n",
      "         45.,  61.,  64.,  68.,  71.,  74., 111.], device='cuda:0'), tensor([1986.], device='cuda:0'), tensor([  4.,  13., 108., 118., 135., 143., 162., 231., 250., 264., 316., 341.,\n",
      "        385., 399., 483., 528., 559., 580., 607., 618.,   5.,  14., 109., 119.,\n",
      "        128., 136., 144., 163., 166., 170., 186., 191., 196., 201., 212., 219.,\n",
      "        226., 232., 238., 251.], device='cuda:0'), tensor([  0.,  19.,  31.,  44.,  48.,  53.,  62.,  72.,  77.,  81.,  84., 118.,\n",
      "        120., 123.,   1.,  20.,  24.,  32.,  34.,  39.,  45.,  49.,  54.,  57.,\n",
      "         63.,  73.,  78.,  82.,  85., 119.], device='cuda:0'), tensor([  0.,   2., 121., 127., 140., 153., 179., 213.,   1.,   3.,  71.,  83.,\n",
      "        122., 134., 158., 180., 192., 199., 205., 212., 237.], device='cuda:0'), tensor([525., 537., 549., 567., 908.,   7.,  23.,  31.,  33.,  45.,  74., 157.,\n",
      "        195., 213., 251., 259., 424., 444., 621., 684., 689., 691., 730., 783.,\n",
      "        808.], device='cuda:0'), tensor([  3.,  21.,  52.,  59., 241., 252., 282., 287., 289., 294., 307.,  19.,\n",
      "         93., 110., 179.,  20.,  94., 111., 180.], device='cuda:0'), tensor([  2., 103., 131., 136., 138., 143., 152., 199.,  23., 101.,  24., 102.],\n",
      "       device='cuda:0'), tensor([  14.,   18.,   26.,   34.,   44.,  103.,  295.,  312.,  319.,  347.,\n",
      "         355.,  359.,  366.,  374.,  378.,  467.,  487.,  607.,  678.,  959.,\n",
      "        1680.,  898.], device='cuda:0'), tensor([], device='cuda:0'), tensor([156., 205., 256., 271., 329., 433., 484., 530., 576., 630., 714.,  11.,\n",
      "         15., 148., 158., 190., 197., 207., 241., 248., 258., 293., 307., 311.,\n",
      "        332., 340., 344., 358., 436., 445., 449.,  71., 128., 191., 242., 294.],\n",
      "       device='cuda:0'), tensor([  2.,  35.,  39.,  43.,  45.,  48.,  53.,  63.,  79.,  84.,  90.,  99.,\n",
      "        126., 137., 133.], device='cuda:0'), tensor([168., 202., 285., 338., 467., 575., 595.,  37.,  95., 145., 249., 266.,\n",
      "        443., 449., 454., 462., 475., 604., 634., 656., 664., 672.,   3.,   7.,\n",
      "         13.,  15.,  42.,  51., 108., 203., 222., 225., 227., 232., 238., 243.,\n",
      "        247., 270., 282., 287., 290., 293.], device='cuda:0'), tensor([273.,   7.,  17.,   6.,  16.,  39.,  66., 160., 191., 236., 248., 266.,\n",
      "        358., 383., 464.], device='cuda:0'), tensor([220., 221., 230., 249.,   2.,  39.,  96., 127., 154., 170., 246., 342.,\n",
      "        386., 695., 822.,   1.,  20.,  22.,  38.,  70.,  72.,  95., 103., 126.,\n",
      "        153., 169., 245., 385., 821., 957., 226., 234., 257., 481., 892., 947.,\n",
      "          7.,  44.,  48.,  56.,  62., 101., 285., 293., 320., 325., 352., 383.,\n",
      "        427., 432., 486., 508., 563., 596., 645., 647.], device='cuda:0'), tensor([   2.,   50.,   52.,  143.,  320.,  323.,  338.,  567.,  599.,  654.,\n",
      "         739., 1013., 1052., 1179., 1420., 1425., 1502., 1642., 1667., 1763.,\n",
      "          12.,   48.,   51.,  144.,  180.,  260.,  317.,  321.,  339.,  568.,\n",
      "         600.,  655., 1014., 1180., 1423., 1426., 1503., 1643., 1668., 1764.,\n",
      "          47.,  116.,  145.,  316.,  587.,  640.,  648.,  804.,  837.,  928.,\n",
      "        1023., 1182., 1422., 1433., 1552., 1772., 1799., 1806., 1821., 1827.,\n",
      "         146., 1024., 1183., 1773., 1800., 1807., 1822., 1828., 1846., 1854.,\n",
      "        1861., 1868., 1882., 1919., 1924., 1971., 1978.,  580.,  801.,  829.,\n",
      "        1430.], device='cuda:0'), tensor([ 59.,  92., 128., 147., 261., 296., 316., 337., 376., 412., 440.,   6.,\n",
      "          8.,  11.,  16.,  19.,  21.,  22.,  25.,  37.,  51.,  58.,  63.,  75.,\n",
      "         91., 127., 146., 170., 179., 196., 206.,   0.,  38., 132., 457.,   1.,\n",
      "         39.,  42.,  99., 130., 133., 152., 175., 190., 211., 264., 327., 384.,\n",
      "        452., 458.], device='cuda:0'), tensor([  8.,  47.,   5.,  44., 159.,   6.,  45., 160.], device='cuda:0'), tensor([  0.,  19.,  36.,  41.,  55.,  84.,  99., 107., 109., 126., 149., 172.,\n",
      "        184., 217., 252., 260., 115., 266.,   8.,  88.,  94., 113., 139., 156.,\n",
      "        176., 179., 191., 194., 197., 200., 203., 207., 258., 264., 267., 336.,\n",
      "        343., 363.], device='cuda:0'), tensor([222., 560., 208., 548., 238., 573.,   1.,   4.,  49.,  56.,  78.,  87.,\n",
      "        176., 190., 197., 248., 265., 371., 459., 529., 539., 601., 630., 638.],\n",
      "       device='cuda:0'), tensor([], device='cuda:0'), tensor([], device='cuda:0'), tensor([  0.,  10.,  13.,  47., 110., 116., 129., 153., 178., 218., 222., 271.,\n",
      "        310., 330., 357.,  11.,  14.,  48., 111., 117., 154., 179., 219., 223.,\n",
      "        272., 331., 360., 363.,  12.,  15.,  49.,  56.,  71.,  86.,  99., 112.,\n",
      "        118., 155., 161., 180., 202., 220., 224., 265., 273., 278., 283., 322.],\n",
      "       device='cuda:0'), tensor([ 88., 114., 171., 176., 238., 316., 378., 439.,  89., 115., 172., 177.,\n",
      "        239., 317., 379., 440.,  90., 116., 121., 173., 178., 240., 318., 380.,\n",
      "        441.], device='cuda:0'), tensor([  4.,  16.,  41.,  46.,  58.,  81.,  91.,  95., 105., 111., 119., 125.,\n",
      "        133., 140., 148.,  47., 149., 171.], device='cuda:0'), tensor([  2.,  11.,  33.,  41.,  43.,  48.,  60.,  72.,  77.,  92., 105., 118.,\n",
      "          4.,  13.,  65.,  73.], device='cuda:0'), tensor([  3.,  52.,  71.,  81.,  94.,  97., 150., 155., 157., 169., 177., 181.,\n",
      "        206., 213., 215., 229., 254., 256., 277., 401.], device='cuda:0'), tensor([ 73.,  82., 175., 216.], device='cuda:0'), tensor([1338., 1807., 1819., 1827., 1880., 1804.], device='cuda:0'), tensor([44., 46., 49., 51., 53., 56., 59., 62., 64., 67.], device='cuda:0'), tensor([ 50.,  25.,  51.,  56., 135., 143., 170.,   1.,  58.], device='cuda:0'), tensor([914.,  58., 340., 387., 405., 466., 475., 519., 583., 653., 813., 915.,\n",
      "        494., 520., 916.], device='cuda:0'), tensor([156., 205., 256., 271., 329., 433., 484., 530., 576., 630., 714.,  11.,\n",
      "         15., 148., 158., 190., 197., 207., 241., 248., 258., 293., 307., 311.,\n",
      "        332., 340., 344., 358., 436., 445., 449.,  71., 128., 191., 242., 294.],\n",
      "       device='cuda:0'), tensor([ 44.,  65.,  93.,  97., 137., 183., 195., 209., 231., 240., 269., 272.,\n",
      "        285., 308., 349., 378., 424., 428., 469., 134., 160., 218., 347.],\n",
      "       device='cuda:0'), tensor([ 76.,  77.,  20.,  65.,  71.,  94., 117., 161.,  21.,  66.,  72.,  95.,\n",
      "        118., 162.,  22.,  67.,  73.,  96., 119., 163.], device='cuda:0'), tensor([  7.,  57.,  77., 231., 298., 313., 324.,   8.,  58.,  78., 232., 325.,\n",
      "        104.,  49.,  52.], device='cuda:0'), tensor([  5.,  12.,  21.,  51.,  62.,  69., 190., 298., 339.,  50.,  68., 170.,\n",
      "        179., 193., 223., 279., 297., 310., 317.], device='cuda:0'), tensor([  3.,  16.,  58.,  87., 145., 146., 173., 175.,   2.,  15., 174.],\n",
      "       device='cuda:0'), tensor([ 4., 11., 23., 25., 94.,  5., 12.,  0.,  2.,  7.,  9., 14., 20., 44.,\n",
      "        48., 57., 59., 62., 63., 68., 91.], device='cuda:0'), tensor([  3.,  96.,  98., 114., 120.,   4., 115., 121., 167., 174.,   2.,  12.,\n",
      "         15.,  29.,  36.,  44.,  54.,  57.,  58.,  71.,  75.,  84.,  87., 113.,\n",
      "        118., 130., 150.], device='cuda:0'), tensor([   5.,    7.,   11.,   15.,   36.,   47.,   52.,   60.,   71.,   75.,\n",
      "          79.,   91.,   92.,  107.,  111.,  116.,  121.,  130.,  136.,  138.,\n",
      "          12.,   16.,   22.,   31.,   37.,   53.,   58.,   76.,  112.,  137.,\n",
      "         151.,  169.,  177.,  180.,  230.,  238.,  241.,  247.,  253.,  282.,\n",
      "          95.,  424.,  831.,  892.,  913.,  921.,  960., 1007., 1021., 1151.,\n",
      "        1202., 1279., 1403., 1440., 1476., 1506., 1534.], device='cuda:0'), tensor([ 162.,  213.,  334.,  354.,  373.,  403.,  447.,  455.,  563.,  588.,\n",
      "         595.,  604.,  613.,  644.,  655.,  756.,  889., 1028., 1085., 1101.,\n",
      "         156.,  174.,  192.,  203.,  218.,  234.,  243.,  254.,  262.,  272.,\n",
      "         280.,  288.,  311.,  323.,  342.,  358.,  379.,  388.,  398.,  406.,\n",
      "         103.,  191.,  741.,  760.,  822.,  837., 1095., 1191., 1322., 1598.,\n",
      "        1784.], device='cuda:0'), tensor([ 6., 17., 25., 43., 52., 56.,  7., 18., 23., 26., 53., 54., 57.],\n",
      "       device='cuda:0'), tensor([  5.,   7.,  15.,  17.,  94.,  96., 155., 157.,   8.,  18.,  97., 158.],\n",
      "       device='cuda:0'), tensor([1727., 1738., 1763., 1771., 1807., 1816., 1848., 1910., 1933., 1946.,\n",
      "        1973., 1749., 1920., 1949.,    0.,    8.,   12.,   50.,  149.,  194.,\n",
      "         276.,  294.,  299.,  305.,  323.,  345.,  350.,  368.,  378.,  393.,\n",
      "         400.,  408.,  422.,  437.,  279.,  297.,  303.,  353.,  364.,  435.,\n",
      "         474.,  493.,  534.,  566.,  707.,  712.,  728.,  751.,  775.,  799.,\n",
      "         846.,  928.,  943.,  976.,  280.,  298.,  304.,  382.,  436.,  475.,\n",
      "         494.,  535.,  567.,  708.,  713.,  729.,  752.,  776.,  784.,  800.,\n",
      "         847.,  865.,  929.,  944.], device='cuda:0'), tensor([ 167.,  187.,  388., 1071., 1082., 1497., 1628., 1814.,   55.,  309.,\n",
      "         669.,  764.,  849., 1331., 1443., 1461., 1516., 1531., 1592., 1621.,\n",
      "        1674., 1735., 1759., 1861.,   56.,  310.,  316.,  670.,  765.,  850.,\n",
      "        1332., 1444., 1462., 1517., 1532., 1593., 1622., 1675., 1736., 1760.,\n",
      "        1862.], device='cuda:0'), tensor([ 124.,  178.,  181.,  360.,  370.,  422.,  425.,  493.,  502.,  505.,\n",
      "         515.,  540.,  581.,  610.,  659.,  662.,  671.,  693.,  713.,  793.,\n",
      "         506.,  623.,  694.,  771.,  816.,   54.,  210.,  232.,  238.,  817.,\n",
      "        1034., 1144., 1152.,  275.,  288.,  560.,  619.,  682., 1585., 1703.,\n",
      "           2.,   69.,   82.,   94.,  102.,  114.,  125.,  146.,  159.,  171.,\n",
      "         182.,  196.,  211.,  239.,  247.,  259.,  270.,  292.,  311.,  322.],\n",
      "       device='cuda:0'), tensor([  3.,  10.,  25.,  48.,  51.,  63.,  78.,  84.,  87.,  90.,  93., 100.,\n",
      "         73.,   5.,  12.,  27.,  53.,  65.,  75., 102.,   6.,  13.,  28.,  36.,\n",
      "         38.,  41.,  49.,  52.,  66.,  74.,  82.,  94., 103.], device='cuda:0'), tensor([  4.,  15.,  93., 132., 136., 143., 149.,   2.,  29., 139.,  18.,  35.,\n",
      "         50.,  65.,  84.,  91., 104., 140.,   0.,  47.,  48.,  49., 100., 103.,\n",
      "        106., 131., 151.], device='cuda:0'), tensor([  6.,   9.,  14.,  41., 427., 434., 437.,  85.,  89.,  95., 141., 196.,\n",
      "        210., 223., 228., 237., 245., 252., 256., 377., 407., 421.,   0.,   4.,\n",
      "         28.,  33.,  45.,  57.,  79., 189.,  25.,  56., 367.], device='cuda:0'), tensor([  1.,  28.,  48.,  65.,  72.,  74.,  89., 121., 143., 162., 172., 184.,\n",
      "        193., 197., 319.,   2.,  29.,  49.,  66.,  73.,  75.,  90., 122., 144.,\n",
      "        163., 173., 185.], device='cuda:0'), tensor([ 40.,   2.,   9.,  13.,  16.,  18.,  20.,  24.,  26.,  41., 277.],\n",
      "       device='cuda:0'), tensor([  0.,  22.,  34.,  41.,  63.,  75., 111., 119., 172., 188., 197., 200.,\n",
      "        220., 229.], device='cuda:0'), tensor([27.], device='cuda:0'), tensor([  1.,   4.,  46.,  95.,  98., 101., 105., 157., 176., 178., 181., 209.,\n",
      "        247., 255., 274., 352., 365., 401., 438., 450.,   2.,   5.,   9.,  34.,\n",
      "         36.,  70.,  96., 158., 160., 179., 193., 210., 233., 239., 248., 253.,\n",
      "        256., 278., 294., 301.,   3.,   6.,  37.,  63.,  91.,  97., 100., 104.,\n",
      "        110., 128., 159., 180., 211., 234., 249., 257., 279., 295., 354., 403.],\n",
      "       device='cuda:0'), tensor([ 23.,  31.,  46., 103., 110., 130., 138., 153., 212., 219., 235., 251.,\n",
      "        287., 298., 320., 373., 381., 396., 435., 442.,  11.,  24.,  32.,  47.,\n",
      "         92., 104., 111., 117., 131., 139., 154., 201., 213., 220., 226., 233.,\n",
      "        236., 252., 288., 299.,  25.,  33.,  48., 105., 112., 132., 140., 155.,\n",
      "        214., 221., 237., 253., 289., 300., 322., 375., 383., 398., 437., 444.],\n",
      "       device='cuda:0'), tensor([  1.,   4.,  10.,  70.,  73.,  80.,  85.,  89.,  95., 101., 122., 125.,\n",
      "        133., 138., 142., 148., 171., 177., 182., 188.,  25.,  47.,  60.,  66.,\n",
      "         75.,  84., 105., 128., 160., 206., 238., 268., 290., 318., 339., 369.,\n",
      "        373.], device='cuda:0'), tensor([  0.,  33.,  36.,  42.,  63.,  64.,  71.,  80.,  83.,  86.,  87.,  90.,\n",
      "         95., 114.,  24.,  89.,  92.,  97.], device='cuda:0'), tensor([ 75.,  83., 116., 119., 146.,   3.,  52.,  69.,  81., 122., 149., 197.,\n",
      "        221., 229., 233.,   4.,  53.,  70.,  77.,  82.,  84.,  91.,  97., 100.,\n",
      "        110., 113., 117., 121., 123., 130., 136., 148., 150., 198., 222.],\n",
      "       device='cuda:0'), tensor([  54.,   73.,  137.,  299.,  312., 1664., 1727., 1767., 1788., 1822.,\n",
      "        1828., 1838., 1854., 1869., 1888.], device='cuda:0'), tensor([  55.,   56., 1619., 1668.,   57.,   60.], device='cuda:0'), tensor([ 934., 1259.,  321.,  326.,  413.,  485.,  603.,  812.,  853.,  856.,\n",
      "         896.,  937.,  975., 1016., 1049., 1061., 1064., 1071., 1074., 1154.,\n",
      "        1260., 1333., 1206., 1209., 1211., 1213., 1215., 1218., 1232., 1234.,\n",
      "        1236., 1238., 1240., 1243., 1246., 1249., 1252., 1258., 1261.],\n",
      "       device='cuda:0'), tensor([  3.,  23.,  51.,  54.,  64.,  73., 195., 209., 213., 280., 328., 479.,\n",
      "        482., 493., 507.,   4.,  74.,  87., 107., 115., 149., 156., 179., 196.,\n",
      "        198., 223., 249., 263., 294., 332., 339., 342., 362., 369., 511., 229.,\n",
      "        295., 402., 425.], device='cuda:0'), tensor([ 2., 19., 31., 41., 46., 51., 67., 80., 90., 95.,  0.,  6., 29., 39.,\n",
      "        49., 55., 65., 78., 88.], device='cuda:0'), tensor([ 60., 132., 157., 176., 619., 691.,   4.,  53.,  81.,  83.,  88., 102.,\n",
      "        107., 117., 120., 125., 138., 147., 150., 152., 158., 191., 194., 198.,\n",
      "        210., 245.,  89.,  92., 108., 114., 139., 143., 179., 189., 211., 296.,\n",
      "        307., 338., 366., 380., 390., 401., 413., 432., 434., 437.],\n",
      "       device='cuda:0'), tensor([ 41., 479., 503.,  25.,  27.,  30.,  97., 106., 178., 669.,   0.,   4.,\n",
      "         14.,  24.,  26.,  28.,  33.,  36.,  37.,  42.,  55.,  73.,  81.,  88.,\n",
      "         98., 111., 150., 163., 171., 183.], device='cuda:0'), tensor([0.0000e+00, 2.0700e+02, 2.2800e+02, 2.6300e+02, 4.5100e+02, 5.0300e+02,\n",
      "        5.7500e+02, 6.1000e+02, 6.9000e+02, 7.4900e+02, 8.1100e+02, 1.0090e+03,\n",
      "        1.0530e+03, 1.1490e+03, 1.1870e+03, 1.2410e+03, 1.3110e+03, 1.4670e+03,\n",
      "        1.5030e+03, 1.5110e+03, 1.0000e+00, 2.0800e+02, 2.2900e+02, 2.6100e+02,\n",
      "        2.6400e+02, 3.1200e+02, 4.5200e+02, 5.0400e+02, 5.7600e+02, 6.1100e+02,\n",
      "        6.9100e+02, 6.9900e+02, 7.1900e+02, 7.5000e+02, 7.7600e+02, 7.9900e+02,\n",
      "        8.1200e+02, 8.9600e+02, 1.0100e+03, 1.0540e+03, 2.0000e+00, 1.0000e+01,\n",
      "        3.2000e+01, 3.8000e+01, 5.0000e+01, 6.4000e+01, 7.6000e+01, 8.0000e+01,\n",
      "        1.3200e+02, 1.7300e+02, 1.9800e+02, 2.0900e+02, 2.1200e+02, 2.2700e+02,\n",
      "        2.3000e+02, 2.6500e+02, 2.8500e+02, 3.7900e+02, 5.0500e+02, 5.7700e+02],\n",
      "       device='cuda:0'), tensor([  2.,  47.,  63.,  86.,  95., 148., 153., 174., 191., 201., 208., 230.,\n",
      "        269., 279., 315., 332., 338., 343., 366., 381.,   3.,  48.,  87.,  96.,\n",
      "        128., 134., 149., 154., 175., 192., 202., 209., 231., 270., 280., 307.,\n",
      "        316., 324., 339., 344.,   4.,  21.,  58.,  97., 100., 106., 135., 150.,\n",
      "        155., 176., 193., 203., 210., 214., 232., 246., 271., 281., 308., 317.],\n",
      "       device='cuda:0'), tensor([ 57.,  59.,  67.,  79.,  88., 101., 133.,  10.,  55.,  58.,  60.,  89.,\n",
      "        102., 113., 123., 126., 128., 178., 208., 125., 127., 138., 162., 172.,\n",
      "         10.,  55.,  58.,  60.,  89., 102., 113., 123., 126., 128., 178., 208.],\n",
      "       device='cuda:0'), tensor([ 86.,   9.,  15.,  20.,  43.,  46.,  58.,  69., 104., 164., 176., 182.,\n",
      "        191.,   8.,  14.,  19.,  24.,  36.,  45.,  57.,  68.,  74.,  77., 103.,\n",
      "        163., 175., 181., 190.,   9.,  15.,  20.,  43.,  46.,  58.,  69., 104.,\n",
      "        164., 176., 182., 191.], device='cuda:0'), tensor([178.,   3.,  14.,  15.,  20.,  39.,  40.,  43.,  46.,  58.,  63.,  79.,\n",
      "         83.,  92., 112., 117., 125., 131., 233., 235., 267.], device='cuda:0'), tensor([12.], device='cuda:0'), tensor([ 14.,  31.,  63.,  89., 112., 115., 126., 138., 153., 171., 196., 218.,\n",
      "        235., 243., 248., 256., 273., 290., 304., 316.,  15.,  32.,  64.,  90.,\n",
      "        113., 116., 127., 139., 154., 168., 172., 184., 198., 219., 236., 244.,\n",
      "        249., 257., 274., 291.], device='cuda:0'), tensor([ 70., 120., 131., 157., 173., 187., 196., 241., 339., 373., 399., 408.,\n",
      "        445., 448., 465., 504.,  71., 121., 132., 158., 174., 188., 190., 197.,\n",
      "        242., 246., 260., 340., 374., 400., 409., 413., 434., 446., 449., 453.],\n",
      "       device='cuda:0'), tensor([150., 152., 172., 593., 844., 858., 922., 924., 927., 932., 935., 945.,\n",
      "        948., 954., 956., 960., 965., 966., 971., 979.,  99., 153., 163., 170.,\n",
      "        180., 276., 284., 302., 313., 352., 359., 360., 371., 375., 389., 397.,\n",
      "        426., 480., 610., 766., 977.], device='cuda:0'), tensor([   2.,   24.,   31.,   39.,   45.,   55.,   56.,   59.,   73.,   77.,\n",
      "          88.,   99.,  110.,  113.,  123.,  134.,  142.,  152.,  199.,  201.,\n",
      "           3.,  101.,  270.,  285.,  328.,  345.,  385.,  523.,  722.,  770.,\n",
      "         821.,  900.,  904.,  927., 1063., 1153., 1198., 1267., 1309., 1329.,\n",
      "         131.,  595.,  968., 1242., 1935.], device='cuda:0'), tensor([   0.,   49.,  365.,  387.,  417.,  597., 1193., 1209., 1217., 1334.,\n",
      "        1592., 1632., 1707.,   91.,  113.,  121.,  132.,  150.,  152.,  195.,\n",
      "         240.,  276.,  295.,  335.,  339.,  376.,  378.,  391.,  393.,  399.,\n",
      "         427.,  434.,  462.,   92.,  116.,  122.,  151.,  336.,  368.,  377.,\n",
      "         385.,  392.,  400.,  420.,  428.,  492.,  523.,  534.,  541.,  560.,\n",
      "         589.,  617.,  670.,   26.,   46.,  123.,  137.,  154.,  171.,  196.,\n",
      "         199.,  221.,  245.,  281.,  298.,  341.,  380.,  395.,  429.,  436.,\n",
      "         618.,  640.,  706.], device='cuda:0'), tensor([ 19.,  84., 105.,  20.,  21.,  22., 108.], device='cuda:0'), tensor([22., 47.,  0.,  4., 27., 70., 84., 86., 88., 97.], device='cuda:0'), tensor([0., 8.], device='cuda:0'), tensor([  2.,  18.,  47.,  60., 115., 143., 119.], device='cuda:0'), tensor([0.], device='cuda:0'), tensor([231., 303., 317., 527., 556., 560., 334., 336., 339., 342., 448., 450.,\n",
      "        452., 454., 539., 542., 545., 548., 551., 555., 559., 563., 541., 544.,\n",
      "        547., 550., 553., 557., 561., 565.,   3.,  16.,  29.,  32.,  36.,  38.,\n",
      "         40.,  61., 194., 325., 351., 493., 514., 578.], device='cuda:0'), tensor([  10.,   39.,   41.,   45.,   90.,   95.,  123.,  156.,  211.,  236.,\n",
      "         280.,  301.,  325.,  335.,  389.,  423.,  447.,  461.,  465.,  476.,\n",
      "          55.,  138.,  367., 1779., 1783., 1823., 1841., 1886., 1903., 1962.,\n",
      "        1982.], device='cuda:0'), tensor([ 66.,  79., 194., 283., 498., 519., 526., 578., 591., 596., 601., 625.,\n",
      "        732., 743., 753., 771., 775., 784., 851., 854.,   1.,   5.,  18.,  22.,\n",
      "         24.,  30.,  37.,  40.,  45.,  48.,  62.,  77.,  81., 109., 114., 117.,\n",
      "        119., 122., 125., 128., 483., 668.,  35., 338., 380., 451., 590., 739.,\n",
      "        940.], device='cuda:0'), tensor([  9.,  16.,  21.,  27.,  37.,  49.,  51.,  82.,  99., 109., 113., 139.,\n",
      "         69.,  87., 136., 166.], device='cuda:0'), tensor([ 101.,  169.,  194.,  319.,  364.,  400.,  525.,  845., 1043., 1146.,\n",
      "        1192., 1217., 1305., 1390., 1547.,  196.,  200.,  213.,  517.,  704.,\n",
      "         820., 1339., 1391., 1434., 1476., 1549., 1553., 1566., 1671., 1681.,\n",
      "        1821.,    6.,   11.,   34.,   39.,   66.,   71.,   89.,  122.,  126.,\n",
      "         159.,  173.,  184.,  211.,  217.,  227.,  234.,  249.,  262.,  288.,\n",
      "         290.,   91.,  166.,  197.,  224.,  260.,  269.,  765.,  784.,  812.,\n",
      "         833.,  917., 1018., 1035., 1070., 1119., 1148., 1160., 1169., 1180.,\n",
      "        1189.], device='cuda:0'), tensor([ 60., 104., 132.,  10.,  32.,  86., 126., 182., 384., 446.,   1.,  22.,\n",
      "         24.,  50.,  69.,  84.,  97., 112., 125., 137., 147., 155., 160., 175.,\n",
      "        188., 194., 208., 222., 223., 270.,   9.,  12.,  21.,  35.,  44.,  48.,\n",
      "         62.,  70.,  79.,  82.,  91.,  99., 114., 136., 139., 165., 181., 199.,\n",
      "        218., 413.], device='cuda:0'), tensor([ 10.,  61., 146., 164., 169., 213.,  23.,  27.,  33.,  49.,  56.,  78.,\n",
      "        102., 147., 171., 175., 216., 238.,  24.,  28.,  34.,  35.,  50.,  57.,\n",
      "         70.,  72.,  79.,  83.,  89., 103., 121., 148., 172., 176., 180., 181.,\n",
      "        189., 205.], device='cuda:0'), tensor([ 31.,  48., 153., 192.,   3.,  11.,  24.,  78.,  83., 107., 109., 113.,\n",
      "        118., 128., 154., 157., 190., 193., 202., 236., 254.,   4.,   6.,  12.,\n",
      "         25.,  26.,  46.,  59.,  71.,  79.,  84.,  90.,  98., 108., 114., 120.,\n",
      "        124., 129., 155., 158., 191.], device='cuda:0'), tensor([ 28.,  30.,  32.,  52.,  54.,  57.,  60.,  62.,  86.,  88.,  90., 108.,\n",
      "        110., 113., 116., 118., 130., 132., 135., 138., 350., 407., 513.,  27.,\n",
      "         51.,  85., 107., 129., 155., 177., 202., 224., 249., 271., 297., 333.,\n",
      "        340., 352., 390., 397., 409., 427., 431.], device='cuda:0'), tensor([1205., 1220., 1229., 1240., 1265., 1269., 1325., 1398.,  140.,  264.,\n",
      "         269.,  271.,  377.,  379.,  424.,  477.,  487.,  504.,  552.,  567.,\n",
      "         585.,  598.,  603.,  611.,  617.,  629.,  651.,  664., 1043., 1171.,\n",
      "        1200., 1212., 1298., 1302., 1334., 1339., 1351., 1411., 1424., 1425.,\n",
      "        1439., 1468., 1474., 1517.], device='cuda:0'), tensor([], device='cuda:0'), tensor([  0.,  19.,  23.,  42.,  53.,  57.,  63.,  80.,  95., 111., 115., 150.,\n",
      "          1.,  20.,  24.,  44.,  55.,  85.,  94., 105., 122., 145., 154., 166.,\n",
      "        102., 125.], device='cuda:0'), tensor([  1.,  34.,  60., 105., 112., 155., 272.,   5.,  33.,  52.,  64.,  85.,\n",
      "        138., 159., 245., 271.], device='cuda:0'), tensor([874.], device='cuda:0'), tensor([], device='cuda:0'), tensor([  81.,   84.,  139.,  284.,  437.,  440.,  443.,  446.,  450.,  454.,\n",
      "         457.,  459.,  618.,  626.,  628.,  635.,  640.,  659.,  684.,  697.,\n",
      "         137.,  140.,  144.,  283.,  286.,  475.,  748.,  822.,  823.,  826.,\n",
      "         830., 1143.,  219.,  362.,  413.,  522.,  524.,  527.,  934., 1078.,\n",
      "        1081., 1084., 1088., 1091., 1175.], device='cuda:0'), tensor([0., 3., 6.], device='cuda:0'), tensor([0., 8.], device='cuda:0'), tensor([ 61.,  74.,  79., 146., 150., 214., 226., 283., 307., 435., 504., 514.,\n",
      "        520., 587., 591., 627., 810.,  62., 814., 816.], device='cuda:0'), tensor([545., 559., 576., 794., 800., 104., 300., 332., 387., 433.],\n",
      "       device='cuda:0'), tensor([  0.,   3.,  14.,  29.,   1.,   4.,  15.,  30., 316.,   2.,   5.,  11.,\n",
      "         16.,  31.,  85., 108., 132., 161., 169., 187., 192., 220., 251., 290.],\n",
      "       device='cuda:0'), tensor([ 39.,  43.,  48.,  52.,  56.,  61.,  64.,  66.,  69.,  75.,  79.,  84.,\n",
      "         91., 103., 105.,  67.,  70.,  76.,  80.,  85.,  92., 104., 106.,  38.,\n",
      "         42.,  47.,  51.,  53.,  55.,  60.,  63.,  65.,  68.,  74.,  78.,  83.,\n",
      "         90.], device='cuda:0'), tensor([ 3., 46.,  7., 22.,  8., 20., 23.], device='cuda:0'), tensor([ 102.,  104.,  109., 1141., 1149., 1679., 1685., 1714., 1721.],\n",
      "       device='cuda:0'), tensor([101., 108., 322.,  94., 102.,  96., 112.], device='cuda:0'), tensor([ 5., 87., 97.,  6., 18., 40., 45., 52., 54., 67., 72., 76.],\n",
      "       device='cuda:0'), tensor([ 42.,  47.,  79.,  93., 169.,   0.,  43.,  48.,  51.,  54.,  80., 148.,\n",
      "        178.], device='cuda:0'), tensor([164., 213., 219., 259., 262.,  22., 239., 251., 257., 263., 265., 269.,\n",
      "        271., 255.], device='cuda:0'), tensor([  3.,  31.,  52., 126., 151.,   5.,  27.,  47.,  54.,  84., 102., 108.,\n",
      "        120., 153., 158.], device='cuda:0'), tensor([17., 44., 64., 21., 23.], device='cuda:0'), tensor([339.,   5.,  26.,  41.,  50.,  54.,  83., 136., 153., 190., 216., 248.,\n",
      "        266., 283., 305., 335., 342., 344., 352., 363., 368.], device='cuda:0'), tensor([ 171.,  223.,  469.,  640., 1119.,   32.,   49.,   65.,   87.,  116.,\n",
      "         125.,  177.,  193.,  240.,  242.,  249.,  258.,  607.,  722.,  732.,\n",
      "         760.,  763.,  768.,  782.,  784.], device='cuda:0'), tensor([ 2.,  6., 39., 43., 50.], device='cuda:0'), tensor([91.], device='cuda:0'), tensor([  2.,   7., 102.,   3.,  45.,  77., 119., 123., 140., 155.],\n",
      "       device='cuda:0'), tensor([153., 912.,  34.,  37.,  56.,  60.,  62.,  66.,  72.,  79.,  86.,  94.,\n",
      "        100., 108., 131., 142., 148., 154., 194., 211., 274., 284.],\n",
      "       device='cuda:0'), tensor([   7.,   39.,   56.,   88.,   96.,  101.,  113.,  124.,  129.,  138.,\n",
      "         187.,  197.,  208.,  260.,  282.,  306.,  313.,  317.,  337.,  345.,\n",
      "         267.,  324.,  452.,  458.,  525.,  675.,  681.,  851., 1016.],\n",
      "       device='cuda:0'), tensor([1., 6., 4., 9.], device='cuda:0'), tensor([  0.,   8.,  16.,  68., 355., 364., 384., 620., 705., 720., 749., 764.,\n",
      "        769., 779.,   1.,   9.,  17.,  69., 127., 136., 269., 356., 360., 365.,\n",
      "        377., 385., 456., 473., 557., 621., 629., 653., 706., 721., 270., 394.],\n",
      "       device='cuda:0'), tensor([ 27.,  55.,  59.,  67.,  73.,  80.,  91.,  98., 104., 117., 123., 127.,\n",
      "        178., 267., 275., 287., 300., 304., 311., 318.,  28.,  42.,  52.,  56.,\n",
      "         60.,  68.,  74.,  81.,  92.,  99., 105., 118., 124., 128., 131., 137.,\n",
      "        143., 148., 150., 156., 122., 126.], device='cuda:0'), tensor([  72.,  103.,  408.,  481.,  928., 1020., 1369., 1383., 1783.,    6.,\n",
      "           8.,   13.,   15.,   18.,   23.,   27.,   37.,   41.,   43.,   51.,\n",
      "          59.,   73.,   75.,  100.,  104.,  111.,  118.,  122.,  124.,    7.,\n",
      "          14.,   16.,   19.,   25.,   28.,   31.,   34.,   38.,   44.,   52.,\n",
      "          76.,  101.,  107.,  123.,  125.,  152.,  179.,  233.,  236.],\n",
      "       device='cuda:0'), tensor([670.,  13.,  38.,  43.,  78.,  84.,  92., 108., 121., 129., 134., 140.,\n",
      "        145., 150., 165., 170., 185., 199., 204., 220., 238.,  39.,  41.,  85.,\n",
      "         95.,  99., 104., 115., 130., 141., 151., 154., 171., 183., 221., 233.,\n",
      "        241., 257., 262., 284., 292.], device='cuda:0'), tensor([ 77.,  84., 201., 310., 333., 351., 372., 399., 466., 483., 578., 672.],\n",
      "       device='cuda:0'), tensor([], device='cuda:0'), tensor([ 18.,  32.,  47.,  53.,  67.,  74.,  81., 275., 285.,  19.,  33.,  48.,\n",
      "         54.,  68.,  75.,  82.,  91., 107., 111., 143., 180., 197., 210., 230.,\n",
      "        260., 276., 286., 294.], device='cuda:0'), tensor([ 86., 109., 124., 130., 147., 191., 202., 210., 214., 224.,  87., 110.,\n",
      "        125., 132., 135., 137., 143., 146., 148., 156., 160., 163., 172., 176.,\n",
      "        192., 195., 211., 215., 221., 229.], device='cuda:0'), tensor([186.,   0.,  16.,  28.,  43.,  85., 102., 114., 126., 156., 179., 183.,\n",
      "        211.], device='cuda:0'), tensor([ 23.,  56.,  84., 106., 119., 125., 325., 335., 346., 360., 432., 455.,\n",
      "        464., 562., 572., 583., 599., 623., 631., 689.,  24.,  45.,  57., 107.,\n",
      "        111., 120., 126., 132., 180., 194., 267., 270., 285., 307., 326., 361.,\n",
      "        415., 424., 456., 534.], device='cuda:0'), tensor([  2.,  14.,  17., 156., 226.,   1.,  40.,  44.,  47.,  59., 218.],\n",
      "       device='cuda:0'), tensor([22., 29., 43., 57., 19., 30., 44., 58., 68.], device='cuda:0'), tensor([150., 198., 208., 210., 288., 293.,   2.,  40., 103., 166., 187., 202.,\n",
      "        211., 235., 250., 269., 289., 314.], device='cuda:0'), tensor([ 249.,  645.,  881.,  882.,  910., 1713.,   29.,   69.,   82.,  105.,\n",
      "         140.,  220.,  303.,  309.,  314.,  322.,  520.,  570.,  583.,  619.,\n",
      "         646., 1110., 1261., 1392., 1420., 1449.], device='cuda:0'), tensor([19., 60.,  3.,  5., 28.,  1.,  7., 14., 32., 36., 40., 44., 48., 52.,\n",
      "        64.], device='cuda:0'), tensor([], device='cuda:0'), tensor([], device='cuda:0'), tensor([ 235.,  320.,  381.,  385.,  584.,  595.,  639.,  799.,    2.,   22.,\n",
      "          38.,   42.,   58.,   70.,   84.,  141.,  238.,  275.,  282.,  310.,\n",
      "         333.,  365.,  405.,  446.,  532.,  548.,  614.,  735.,  307.,  325.,\n",
      "        1118.], device='cuda:0'), tensor([ 85., 121., 150., 257.,   9.,  21.,  28.,  30.,  41.,  62.,  66.,  86.,\n",
      "        128., 141., 182., 189., 224., 230., 251., 258., 284.,   8.,  10.,  20.,\n",
      "         22.,  29.,  31.,  42.,  56.,  63.,  67.,  69.,  87., 126., 129., 142.,\n",
      "        183., 190., 215., 225., 231.], device='cuda:0'), tensor([22., 41., 23., 42.], device='cuda:0'), tensor([13., 19.], device='cuda:0'), tensor([  4.,  12.,  55., 115., 232.], device='cuda:0'), tensor([ 42.,  96.,  13.,  43., 149., 185., 246., 308.], device='cuda:0'), tensor([], device='cuda:0'), tensor([489., 604., 345., 443., 457., 474., 573., 583., 610., 614., 637., 658.,\n",
      "        666., 679., 694., 707.], device='cuda:0'), tensor([ 53., 398., 846.,  39.,  54.,  76., 132., 156., 193., 248., 254., 263.,\n",
      "        267., 291., 318., 340., 364., 448., 489., 551., 638., 652., 658.],\n",
      "       device='cuda:0'), tensor([  35.,  187.,  196.,  351.,  425.,  437.,  480.,  486.,  911., 1000.,\n",
      "        1029., 1070., 1081., 1085., 1295., 1325., 1332., 1351., 1368., 1385.,\n",
      "          38.,   99.,  114.,  126.,  140.,  147.,  152.,  161.,  175.,  190.,\n",
      "         199.,  211.,  254.,  271.,  314.,  337.,  367.,  380.,  386.,  404.,\n",
      "          88.,   96.,  104.,  194.,  202.,  213.,  215.,  272.,  383.,  422.,\n",
      "         430.,  451.,  498.,  515.,  533.,  600.,  795.,  874.,  881.,  882.],\n",
      "       device='cuda:0'), tensor([ 28.,  89., 145., 157., 163., 177., 200., 222., 241., 264., 313., 561.,\n",
      "          2., 393., 398., 424., 460., 464., 541., 562.,   4.,  52.,  62., 478.,\n",
      "        563., 569., 573.], device='cuda:0'), tensor([144., 146., 152., 162., 258., 265., 277., 285., 289., 296., 312., 467.,\n",
      "        471., 480., 485., 489., 492., 514., 518., 524.,  57.,  62., 112., 131.,\n",
      "        139., 151., 157., 261., 282., 326., 348., 352., 367., 400., 608., 635.,\n",
      "        291., 315.], device='cuda:0'), tensor([  4.,  78.,  92., 108., 308., 316.], device='cuda:0'), tensor([0.0000e+00, 5.0000e+00, 1.9000e+01, 2.4000e+01, 3.7000e+01, 5.0000e+01,\n",
      "        1.1000e+02, 1.2500e+02, 2.6700e+02, 3.6200e+02, 4.3200e+02, 4.8900e+02,\n",
      "        5.8800e+02, 7.8300e+02, 1.1590e+03, 1.1660e+03, 1.2810e+03, 1.2910e+03,\n",
      "        1.3030e+03, 3.0000e+00, 6.0000e+00, 2.2000e+01, 2.5000e+01, 5.1000e+01,\n",
      "        3.6300e+02, 1.2820e+03, 1.2920e+03, 1.3040e+03, 1.0000e+00, 2.0000e+01,\n",
      "        5.3000e+01, 3.6400e+02, 3.9800e+02, 4.5200e+02, 5.0200e+02, 6.1300e+02,\n",
      "        7.5500e+02, 7.7500e+02, 7.8400e+02, 7.9900e+02, 1.2750e+03, 1.2930e+03,\n",
      "        1.3050e+03, 1.3130e+03], device='cuda:0'), tensor([ 195.,  257.,  349.,   94.,  116.,  124.,  132.,  140.,  160.,  188.,\n",
      "         204.,  240.,  244.,  368.,  562., 1026., 1639., 1911., 1925.,   95.,\n",
      "         117.,  125.,  133.,  141.,  161.,  189.,  205.,  241.,  245.,  369.,\n",
      "         563., 1027., 1926.,   97.,  100.,  103.,  105.,  108.,  119.,  127.,\n",
      "         135.,  143.,  163.,  173.,  179.,  185.,  191.,  207.,  222.,  242.,\n",
      "         246.,  371.,  374.], device='cuda:0'), tensor([  1.,   5.,   8.,  48.,  67.,  69.,  72., 100., 138., 160., 193., 204.,\n",
      "        235., 269., 285., 311., 333., 347., 406., 419.,   2.,   6.,   9.,  13.,\n",
      "         38.,  40.,  49.,  51.,  70.,  84., 101., 124., 130., 139., 144., 164.,\n",
      "        183., 194., 205., 216.,   3.,   7.,  10.,  41.,  50.,  71., 102., 125.,\n",
      "        140., 165., 195., 206., 237., 271., 287., 307., 313., 325., 335., 349.],\n",
      "       device='cuda:0'), tensor([ 29., 108., 115., 170., 520., 524., 530., 536., 544., 552., 560., 568.,\n",
      "        576., 584., 592., 600., 607., 615., 623., 631.,   2.,  33.,  39.,  65.,\n",
      "         84.,  98., 103., 116., 143., 169., 171., 246., 254., 257., 268., 293.,\n",
      "        307., 312., 317., 322., 117., 158., 163., 172., 532., 538., 546., 554.,\n",
      "        562., 570., 578., 586., 594., 602., 609., 617., 625., 633., 641., 649.],\n",
      "       device='cuda:0'), tensor([  3.,   8.,  31.,  44.,  73.,  84., 113., 206., 214.,   4.,   9.,  33.,\n",
      "         45.,  57.,  74.,  81., 114., 169., 207., 215.,   2.,   7.,  29.,  52.,\n",
      "        193., 196., 205., 213.], device='cuda:0'), tensor([  0.,  57.,   1.,  58.,  93., 152.], device='cuda:0'), tensor([1435., 1442., 1447., 1453., 1462., 1471., 1479., 1484., 1491., 1500.,\n",
      "        1509., 1518., 1525., 1530., 1535., 1545., 1552., 1559., 1563., 1567.,\n",
      "           7.,    8.,   12.,   32.,  184., 1244., 1370., 1440., 1661., 1721.],\n",
      "       device='cuda:0'), tensor([ 86., 102.,  87.], device='cuda:0'), tensor([ 40.,  92., 162.,  87., 159.], device='cuda:0'), tensor([ 90.,  91.,  59.,  65.,  69.,  73.,  76.,  81.,  86.,  89.,  92.,  95.,\n",
      "         98., 102., 107., 110., 115., 119., 123., 127., 131., 135.],\n",
      "       device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "print(X2_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1 = X1[:1]\n",
    "# X1_len = X1_len[:1]\n",
    "# X2 = X2[:1]\n",
    "# X2_len = X2_len[:1]\n",
    "# X1_id = X1_id[:1]\n",
    "# X2_id = X2_id[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_text(X2[0])\n",
    "# X1, X2_new, X1_len, X2_len_new = select_net(X1, X2, X1_len, X2_len, X1_id, X2_id)\n",
    "# show_text(X1[0])\n",
    "# for i in range(5):\n",
    "#     print(i, end=' ')\n",
    "#     show_text(X2_new[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank loss: 0.9951812624931335\n",
      "rank loss: 1.0048960447311401\n",
      "rank loss: 0.9940654039382935\n",
      "rank loss: 1.0042060613632202\n",
      "rank loss: 1.0156229734420776\n",
      "rank loss: 1.0219076871871948\n",
      "rank loss: 1.0000731945037842\n",
      "rank loss: 1.0086194276809692\n",
      "rank loss: 0.9833398461341858\n",
      "rank loss: 0.9997835159301758\n",
      "rank loss: 1.001913070678711\n",
      "rank loss: 1.0131293535232544\n",
      "rank loss: 1.0235469341278076\n",
      "rank loss: 1.0080866813659668\n",
      "rank loss: 1.0018788576126099\n",
      "rank loss: 0.9935066103935242\n",
      "rank loss: 0.9837742447853088\n",
      "rank loss: 0.9826953411102295\n",
      "rank loss: 1.001282811164856\n",
      "rank loss: 0.9963813424110413\n",
      "rank loss: 1.002352237701416\n",
      "rank loss: 0.9916420578956604\n",
      "rank loss: 0.9915006756782532\n",
      "rank loss: 0.9980853199958801\n",
      "rank loss: 0.9874790906906128\n",
      "rank loss: 1.0032700300216675\n",
      "rank loss: 0.9756821990013123\n",
      "rank loss: 0.9791268706321716\n",
      "rank loss: 1.0030436515808105\n",
      "rank loss: 1.0126398801803589\n",
      "rank loss: 1.0038189888000488\n",
      "rank loss: 1.0133439302444458\n",
      "rank loss: 1.0059489011764526\n",
      "rank loss: 1.0007879734039307\n",
      "rank loss: 1.0054444074630737\n",
      "rank loss: 0.9955196380615234\n",
      "rank loss: 1.0203965902328491\n",
      "rank loss: 0.9916654825210571\n",
      "rank loss: 1.0179625749588013\n",
      "rank loss: 0.9867745637893677\n",
      "rank loss: 0.9919870495796204\n",
      "rank loss: 0.9899661540985107\n",
      "rank loss: 1.013022780418396\n",
      "rank loss: 0.9975008964538574\n",
      "rank loss: 0.9911189079284668\n",
      "rank loss: 1.0053439140319824\n",
      "rank loss: 1.0051456689834595\n",
      "rank loss: 0.97597736120224\n",
      "rank loss: 1.0005626678466797\n",
      "rank loss: 0.989388108253479\n",
      "rank loss: 0.974872887134552\n",
      "rank loss: 0.9870625734329224\n",
      "rank loss: 0.9983090162277222\n",
      "rank loss: 1.0013585090637207\n",
      "rank loss: 0.9824705123901367\n",
      "rank loss: 0.9880034923553467\n",
      "rank loss: 1.0044100284576416\n",
      "rank loss: 0.9920674562454224\n",
      "rank loss: 1.0166079998016357\n",
      "rank loss: 0.9883614182472229\n",
      "rank loss: 1.002160906791687\n",
      "rank loss: 0.9571247696876526\n",
      "rank loss: 0.9842514395713806\n",
      "rank loss: 0.9687063097953796\n",
      "rank loss: 0.989347517490387\n",
      "rank loss: 0.9855793714523315\n",
      "rank loss: 1.014103651046753\n",
      "rank loss: 0.9663217067718506\n",
      "rank loss: 0.9938086271286011\n",
      "rank loss: 1.007894515991211\n",
      "rank loss: 0.9636494517326355\n",
      "rank loss: 1.0042868852615356\n",
      "rank loss: 0.9724169969558716\n",
      "rank loss: 0.9819804430007935\n",
      "rank loss: 1.0127629041671753\n",
      "rank loss: 0.9744044542312622\n",
      "rank loss: 0.983900249004364\n",
      "rank loss: 0.9896126985549927\n",
      "rank loss: 0.973060131072998\n",
      "rank loss: 1.0165382623672485\n",
      "rank loss: 0.984729528427124\n",
      "rank loss: 0.9563695788383484\n",
      "rank loss: 0.9723817110061646\n",
      "rank loss: 1.000277042388916\n",
      "rank loss: 1.004102349281311\n",
      "rank loss: 0.9865249395370483\n",
      "rank loss: 0.9956812858581543\n",
      "rank loss: 1.0018305778503418\n",
      "rank loss: 1.003430962562561\n",
      "rank loss: 0.9642912149429321\n",
      "rank loss: 0.9905325174331665\n",
      "rank loss: 1.0245665311813354\n",
      "rank loss: 0.9682053923606873\n",
      "rank loss: 0.955005943775177\n",
      "rank loss: 0.9821969270706177\n",
      "rank loss: 0.9734710454940796\n",
      "rank loss: 0.9694498777389526\n",
      "rank loss: 0.9790861010551453\n",
      "rank loss: 0.9702819585800171\n",
      "rank loss: 0.9838258624076843\n",
      "rank loss: 0.9528335332870483\n",
      "rank loss: 0.961961030960083\n",
      "rank loss: 0.9682096838951111\n",
      "rank loss: 0.9700781106948853\n",
      "rank loss: 0.9479939341545105\n",
      "rank loss: 0.9712380766868591\n",
      "rank loss: 0.9586250185966492\n",
      "rank loss: 0.9646963477134705\n",
      "rank loss: 0.9584406018257141\n",
      "rank loss: 0.9302653074264526\n",
      "rank loss: 0.9664109349250793\n",
      "rank loss: 0.976970374584198\n",
      "rank loss: 0.9559389352798462\n",
      "rank loss: 0.9778738021850586\n",
      "rank loss: 0.9658631086349487\n",
      "rank loss: 0.9576705694198608\n",
      "rank loss: 0.9813248515129089\n",
      "rank loss: 0.9087768197059631\n",
      "rank loss: 0.938023030757904\n",
      "rank loss: 0.9185243248939514\n",
      "rank loss: 0.9389033317565918\n",
      "rank loss: 0.9477492570877075\n",
      "rank loss: 0.9455175399780273\n",
      "rank loss: 0.8653314113616943\n",
      "rank loss: 0.8901364803314209\n",
      "rank loss: 0.9337093830108643\n",
      "rank loss: 0.9511706829071045\n",
      "rank loss: 0.9615658521652222\n",
      "rank loss: 0.9307301044464111\n",
      "rank loss: 0.9607457518577576\n",
      "rank loss: 0.933448314666748\n",
      "rank loss: 0.9925234913825989\n",
      "rank loss: 0.9511129260063171\n",
      "rank loss: 0.9235138297080994\n",
      "rank loss: 0.929961621761322\n",
      "rank loss: 0.9246259927749634\n",
      "rank loss: 0.9465031027793884\n",
      "rank loss: 0.9405415058135986\n",
      "rank loss: 0.9623980522155762\n",
      "rank loss: 0.8796239495277405\n",
      "rank loss: 0.8670162558555603\n",
      "rank loss: 0.9283773303031921\n",
      "rank loss: 0.9443185329437256\n",
      "rank loss: 0.8808760046958923\n",
      "rank loss: 0.9184552431106567\n",
      "rank loss: 0.950641930103302\n",
      "rank loss: 1.014039158821106\n",
      "rank loss: 0.9328837394714355\n",
      "rank loss: 0.9235104322433472\n",
      "rank loss: 0.8805618286132812\n",
      "rank loss: 0.9992523193359375\n",
      "rank loss: 0.8510473370552063\n",
      "rank loss: 0.8820087313652039\n",
      "rank loss: 0.9036222696304321\n",
      "rank loss: 0.9111559987068176\n",
      "rank loss: 0.9155480861663818\n",
      "rank loss: 0.9186394810676575\n",
      "rank loss: 0.8620021343231201\n",
      "rank loss: 0.9566610455513\n",
      "rank loss: 0.8642818927764893\n",
      "rank loss: 0.811593770980835\n",
      "rank loss: 0.8622510433197021\n",
      "rank loss: 0.8913256525993347\n",
      "rank loss: 0.9226802587509155\n",
      "rank loss: 0.8098189830780029\n",
      "rank loss: 0.908879816532135\n",
      "rank loss: 0.9882085919380188\n",
      "rank loss: 0.884467601776123\n",
      "rank loss: 0.8777057528495789\n",
      "rank loss: 0.7424229383468628\n",
      "rank loss: 0.8650107383728027\n",
      "rank loss: 0.9596037268638611\n",
      "rank loss: 1.0051071643829346\n",
      "rank loss: 0.8304104208946228\n",
      "rank loss: 0.7859781384468079\n",
      "rank loss: 0.861987292766571\n",
      "rank loss: 0.8724901676177979\n",
      "rank loss: 0.887699544429779\n",
      "rank loss: 0.9040438532829285\n",
      "rank loss: 0.8571460247039795\n",
      "rank loss: 0.9257554411888123\n",
      "rank loss: 0.889097273349762\n",
      "rank loss: 0.8062453866004944\n",
      "rank loss: 0.7770684361457825\n",
      "rank loss: 0.8670551776885986\n",
      "rank loss: 0.8234765529632568\n",
      "rank loss: 0.8110654354095459\n",
      "rank loss: 0.8284223675727844\n",
      "rank loss: 0.9180731177330017\n",
      "rank loss: 0.824043869972229\n",
      "rank loss: 0.8873646259307861\n",
      "rank loss: 0.8918695449829102\n",
      "rank loss: 0.7974693179130554\n",
      "rank loss: 0.6912471055984497\n",
      "rank loss: 0.8689545392990112\n",
      "rank loss: 0.798142671585083\n",
      "rank loss: 0.9446598887443542\n",
      "rank loss: 0.9138377904891968\n",
      "rank loss: 0.7797383069992065\n",
      "rank loss: 0.8430952429771423\n",
      "rank loss: 0.8586485385894775\n",
      "rank loss: 0.9976542592048645\n",
      "rank loss: 0.8384107947349548\n",
      "rank loss: 0.8095995187759399\n",
      "rank loss: 0.7878254652023315\n",
      "rank loss: 0.756500780582428\n",
      "rank loss: 0.7925176024436951\n",
      "rank loss: 0.7607463002204895\n",
      "rank loss: 0.8579074740409851\n",
      "rank loss: 0.7501844763755798\n",
      "rank loss: 0.8348705768585205\n",
      "rank loss: 0.7615729570388794\n",
      "rank loss: 0.8336861729621887\n",
      "rank loss: 0.8184306025505066\n",
      "rank loss: 0.8248223662376404\n",
      "rank loss: 0.9046083092689514\n",
      "rank loss: 0.8790881037712097\n",
      "rank loss: 0.7317832708358765\n",
      "rank loss: 0.8005561828613281\n",
      "rank loss: 0.8629217147827148\n",
      "rank loss: 0.8564579486846924\n",
      "rank loss: 0.7750133872032166\n",
      "rank loss: 0.8215632438659668\n",
      "rank loss: 0.7930884957313538\n",
      "rank loss: 0.9986332654953003\n",
      "rank loss: 0.9256991147994995\n",
      "rank loss: 0.8136589527130127\n",
      "rank loss: 0.7017871141433716\n",
      "rank loss: 0.832727313041687\n",
      "rank loss: 0.8384107947349548\n",
      "rank loss: 0.789754331111908\n",
      "rank loss: 0.8397220969200134\n",
      "rank loss: 0.8194782733917236\n",
      "rank loss: 0.8003725409507751\n",
      "rank loss: 0.909918487071991\n",
      "rank loss: 0.7406431436538696\n",
      "rank loss: 0.8818066120147705\n",
      "rank loss: 0.9431455731391907\n",
      "rank loss: 0.9163550138473511\n",
      "rank loss: 0.9169208407402039\n",
      "rank loss: 0.821979820728302\n",
      "rank loss: 0.7366023063659668\n",
      "rank loss: 0.8833807110786438\n",
      "rank loss: 0.7930874228477478\n",
      "rank loss: 0.7963559031486511\n",
      "rank loss: 0.8302193880081177\n",
      "rank loss: 0.7555027008056641\n",
      "rank loss: 0.7239252924919128\n",
      "rank loss: 0.7857815623283386\n",
      "rank loss: 0.9069331884384155\n",
      "rank loss: 0.8245704174041748\n",
      "rank loss: 0.8328329920768738\n",
      "rank loss: 0.9008674621582031\n",
      "rank loss: 0.7561900019645691\n",
      "rank loss: 0.7278410196304321\n",
      "rank loss: 0.8643910884857178\n",
      "rank loss: 0.9259807467460632\n",
      "rank loss: 1.0455251932144165\n",
      "rank loss: 0.8325881958007812\n",
      "rank loss: 0.7900217175483704\n",
      "rank loss: 0.8701693415641785\n",
      "rank loss: 0.7732943296432495\n",
      "rank loss: 0.8196339011192322\n",
      "rank loss: 0.7216154932975769\n",
      "rank loss: 0.9287487864494324\n",
      "rank loss: 0.8354797959327698\n",
      "rank loss: 0.7767389416694641\n",
      "rank loss: 0.8522135615348816\n",
      "rank loss: 0.933980405330658\n",
      "rank loss: 0.7070233225822449\n",
      "rank loss: 0.6760415434837341\n",
      "rank loss: 0.787103533744812\n",
      "rank loss: 0.7457575798034668\n",
      "rank loss: 0.7860649228096008\n",
      "rank loss: 0.9154447913169861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank loss: 0.8080474138259888\n",
      "rank loss: 0.8481745719909668\n",
      "rank loss: 0.8028175234794617\n",
      "rank loss: 0.934680700302124\n",
      "rank loss: 0.8829318284988403\n",
      "rank loss: 0.7785511016845703\n",
      "rank loss: 0.8013278841972351\n",
      "rank loss: 0.8055276870727539\n",
      "rank loss: 0.6992752552032471\n",
      "rank loss: 0.6995631456375122\n",
      "rank loss: 0.7595901489257812\n",
      "rank loss: 0.8387193083763123\n",
      "rank loss: 0.8899760842323303\n",
      "rank loss: 0.8644739985466003\n",
      "rank loss: 0.7523307800292969\n",
      "rank loss: 0.8586511015892029\n",
      "rank loss: 0.8151028156280518\n",
      "rank loss: 0.7556069493293762\n",
      "rank loss: 0.7688696384429932\n",
      "rank loss: 0.785063624382019\n",
      "rank loss: 0.7328717112541199\n",
      "rank loss: 0.8722243905067444\n",
      "rank loss: 0.7965525388717651\n",
      "rank loss: 0.7584728598594666\n",
      "rank loss: 0.7989668250083923\n",
      "rank loss: 0.7456743717193604\n",
      "rank loss: 0.7686604261398315\n",
      "rank loss: 0.8510354161262512\n",
      "rank loss: 0.8256372809410095\n",
      "rank loss: 0.8128499388694763\n",
      "rank loss: 0.7991430759429932\n",
      "rank loss: 0.7391169667243958\n",
      "rank loss: 0.8185761570930481\n",
      "rank loss: 0.7573046088218689\n",
      "rank loss: 0.8508949279785156\n",
      "rank loss: 0.8621844053268433\n",
      "rank loss: 0.9311949014663696\n",
      "rank loss: 0.7708406448364258\n",
      "rank loss: 0.8419713973999023\n",
      "rank loss: 0.9506150484085083\n",
      "rank loss: 0.7700185179710388\n",
      "rank loss: 0.7907217144966125\n",
      "rank loss: 0.885464608669281\n",
      "rank loss: 0.7850982546806335\n",
      "rank loss: 0.8589996099472046\n",
      "rank loss: 0.8012281656265259\n",
      "rank loss: 0.9411840438842773\n",
      "rank loss: 0.7472140192985535\n",
      "rank loss: 0.7380704879760742\n",
      "rank loss: 0.96684330701828\n",
      "rank loss: 0.7313218712806702\n",
      "rank loss: 0.774746835231781\n",
      "rank loss: 0.8281583786010742\n",
      "rank loss: 0.7738052010536194\n",
      "rank loss: 0.8596455454826355\n",
      "rank loss: 0.8095287084579468\n",
      "rank loss: 0.7024120688438416\n",
      "rank loss: 0.7403731942176819\n",
      "rank loss: 0.8697695732116699\n",
      "rank loss: 0.8839797973632812\n",
      "rank loss: 0.7878682613372803\n",
      "rank loss: 0.8052821755409241\n",
      "rank loss: 0.7779455184936523\n",
      "rank loss: 0.6530762910842896\n",
      "rank loss: 0.7103883028030396\n",
      "rank loss: 0.7467047572135925\n",
      "rank loss: 0.791237473487854\n",
      "rank loss: 0.6603087186813354\n",
      "rank loss: 0.8556092381477356\n",
      "rank loss: 0.8093651533126831\n",
      "rank loss: 0.7507094144821167\n",
      "rank loss: 0.7254855036735535\n",
      "rank loss: 0.7714038491249084\n",
      "rank loss: 0.8025385737419128\n",
      "rank loss: 0.8590413331985474\n",
      "rank loss: 0.8383923172950745\n",
      "rank loss: 0.8087167143821716\n",
      "rank loss: 0.7342966794967651\n",
      "rank loss: 0.8615996241569519\n",
      "rank loss: 0.8374781012535095\n",
      "rank loss: 0.7620459794998169\n",
      "rank loss: 0.7934051156044006\n",
      "rank loss: 0.7844059467315674\n",
      "rank loss: 0.7210696935653687\n",
      "rank loss: 0.7948676943778992\n",
      "rank loss: 0.8553298711776733\n",
      "rank loss: 0.8807967901229858\n",
      "rank loss: 0.7704116702079773\n",
      "rank loss: 0.7453478574752808\n",
      "rank loss: 0.7601942420005798\n",
      "rank loss: 0.820501983165741\n",
      "rank loss: 0.7181036472320557\n",
      "rank loss: 0.8209700584411621\n",
      "rank loss: 0.8557665348052979\n",
      "rank loss: 0.8018223643302917\n",
      "rank loss: 0.7852823734283447\n",
      "rank loss: 0.6920617818832397\n",
      "rank loss: 0.6604862809181213\n",
      "rank loss: 0.8471819758415222\n",
      "rank loss: 0.7347069978713989\n",
      "rank loss: 0.8160461187362671\n",
      "rank loss: 0.7380342483520508\n",
      "rank loss: 0.8107297420501709\n",
      "rank loss: 0.7904481291770935\n",
      "rank loss: 0.7609744071960449\n",
      "rank loss: 0.7768459320068359\n",
      "rank loss: 0.8465673923492432\n",
      "rank loss: 0.7598100900650024\n",
      "rank loss: 0.7425681948661804\n",
      "rank loss: 0.8307152390480042\n",
      "rank loss: 0.8272251486778259\n",
      "rank loss: 0.8770979046821594\n",
      "rank loss: 0.7838042378425598\n",
      "rank loss: 0.809270977973938\n",
      "rank loss: 0.8789620995521545\n",
      "rank loss: 0.8062304258346558\n",
      "rank loss: 0.700017511844635\n",
      "rank loss: 0.7276612520217896\n",
      "rank loss: 0.7053087949752808\n",
      "rank loss: 0.6592648029327393\n",
      "rank loss: 0.6557911038398743\n",
      "rank loss: 0.7385886311531067\n",
      "rank loss: 0.7453182935714722\n",
      "rank loss: 0.7526354789733887\n",
      "rank loss: 0.7901507616043091\n",
      "rank loss: 0.7763093113899231\n",
      "rank loss: 0.6741892695426941\n",
      "rank loss: 0.7785836458206177\n",
      "rank loss: 0.7547193765640259\n",
      "rank loss: 0.7206643223762512\n",
      "rank loss: 0.7083060145378113\n",
      "rank loss: 0.8129684329032898\n",
      "rank loss: 0.7606984376907349\n",
      "rank loss: 0.7421360611915588\n",
      "rank loss: 0.7662642598152161\n",
      "rank loss: 0.6816295981407166\n",
      "rank loss: 0.7148838043212891\n",
      "rank loss: 0.7940489053726196\n",
      "rank loss: 0.6654855012893677\n",
      "rank loss: 0.8820502758026123\n",
      "rank loss: 0.7854965329170227\n",
      "rank loss: 0.9833292365074158\n",
      "rank loss: 0.7491971254348755\n",
      "rank loss: 0.7306526303291321\n",
      "rank loss: 0.7549395561218262\n",
      "rank loss: 0.7516025900840759\n",
      "rank loss: 0.7702538967132568\n",
      "rank loss: 0.6969525218009949\n",
      "rank loss: 0.7037515044212341\n",
      "rank loss: 0.7478801012039185\n",
      "rank loss: 0.7524815201759338\n",
      "rank loss: 0.85746169090271\n",
      "rank loss: 0.7276946306228638\n",
      "rank loss: 0.7080491185188293\n",
      "rank loss: 0.7156385779380798\n",
      "rank loss: 0.6328304409980774\n",
      "rank loss: 0.8683689832687378\n",
      "rank loss: 0.848200798034668\n",
      "rank loss: 0.7149604558944702\n",
      "rank loss: 0.8202764987945557\n",
      "rank loss: 0.7147451639175415\n",
      "rank loss: 0.662247896194458\n",
      "rank loss: 0.8066888451576233\n",
      "rank loss: 0.7904508113861084\n",
      "rank loss: 0.7937556505203247\n",
      "rank loss: 0.751127302646637\n",
      "rank loss: 0.7424623966217041\n",
      "rank loss: 0.7585179209709167\n",
      "rank loss: 0.8059737682342529\n",
      "rank loss: 0.7061430215835571\n",
      "rank loss: 0.6306174993515015\n",
      "rank loss: 0.8016000390052795\n",
      "rank loss: 0.7419483661651611\n",
      "rank loss: 0.6836766004562378\n",
      "rank loss: 0.7190183997154236\n",
      "rank loss: 0.7633491158485413\n",
      "rank loss: 0.7785922884941101\n",
      "rank loss: 0.7008646130561829\n",
      "rank loss: 0.7261962890625\n",
      "rank loss: 0.7398988008499146\n",
      "rank loss: 0.6547404527664185\n",
      "rank loss: 0.870632529258728\n",
      "rank loss: 0.8149847984313965\n",
      "rank loss: 0.7189091444015503\n",
      "rank loss: 0.7610208988189697\n",
      "rank loss: 0.7711485624313354\n",
      "rank loss: 0.6719966530799866\n",
      "rank loss: 0.7879785895347595\n",
      "rank loss: 0.7517551183700562\n",
      "rank loss: 0.7746594548225403\n",
      "rank loss: 0.6415382027626038\n",
      "rank loss: 0.7210007905960083\n",
      "rank loss: 0.8486135005950928\n",
      "rank loss: 0.835771918296814\n",
      "rank loss: 0.6527056097984314\n",
      "rank loss: 0.7549575567245483\n",
      "rank loss: 0.7809536457061768\n",
      "rank loss: 0.7603335380554199\n",
      "rank loss: 0.7412883043289185\n",
      "rank loss: 0.7192220091819763\n",
      "rank loss: 0.8602802753448486\n",
      "rank loss: 0.5934592485427856\n",
      "rank loss: 0.770681619644165\n",
      "rank loss: 0.7826588153839111\n",
      "rank loss: 0.77082759141922\n",
      "rank loss: 0.6664682626724243\n",
      "rank loss: 0.6435403227806091\n",
      "rank loss: 0.8936673402786255\n",
      "rank loss: 0.7749527096748352\n",
      "rank loss: 0.7132763266563416\n",
      "rank loss: 0.7206247448921204\n",
      "rank loss: 0.706860363483429\n",
      "rank loss: 0.7475420832633972\n",
      "rank loss: 0.8214877247810364\n",
      "rank loss: 0.6865229606628418\n",
      "rank loss: 0.5851646065711975\n",
      "rank loss: 0.7225802540779114\n",
      "rank loss: 0.7762064933776855\n",
      "rank loss: 0.8072196841239929\n",
      "rank loss: 0.7988749742507935\n",
      "rank loss: 0.6527833342552185\n",
      "rank loss: 0.7588766813278198\n",
      "rank loss: 0.839972734451294\n",
      "rank loss: 0.7671068906784058\n",
      "rank loss: 0.7787748575210571\n",
      "rank loss: 0.7825028896331787\n",
      "rank loss: 0.7560703754425049\n",
      "rank loss: 0.7781884074211121\n",
      "rank loss: 0.8075918555259705\n",
      "rank loss: 0.8608341813087463\n",
      "rank loss: 0.787598192691803\n",
      "rank loss: 0.8374680876731873\n",
      "rank loss: 0.76434326171875\n",
      "rank loss: 0.7401264905929565\n",
      "rank loss: 0.7333987355232239\n",
      "rank loss: 0.7633850574493408\n",
      "rank loss: 0.7291339039802551\n",
      "rank loss: 0.6381861567497253\n",
      "rank loss: 0.7080942988395691\n",
      "rank loss: 0.6622519493103027\n",
      "rank loss: 0.7986190915107727\n",
      "rank loss: 0.7050175070762634\n",
      "rank loss: 0.7545159459114075\n",
      "rank loss: 0.7725525498390198\n",
      "rank loss: 0.7822368741035461\n",
      "rank loss: 0.7529318928718567\n",
      "rank loss: 0.6864156126976013\n",
      "rank loss: 0.6834791302680969\n",
      "rank loss: 0.7714455723762512\n",
      "rank loss: 0.7479850053787231\n",
      "rank loss: 0.7736150026321411\n",
      "rank loss: 0.6697694063186646\n",
      "rank loss: 0.7744393944740295\n",
      "rank loss: 0.7676421403884888\n",
      "rank loss: 0.620401918888092\n",
      "rank loss: 0.7868934273719788\n",
      "rank loss: 0.6744664907455444\n",
      "rank loss: 0.7429423332214355\n",
      "rank loss: 0.6675528287887573\n",
      "rank loss: 0.7358055114746094\n",
      "rank loss: 0.7622755169868469\n",
      "rank loss: 0.7634517550468445\n",
      "rank loss: 0.8038456439971924\n",
      "rank loss: 0.6708090901374817\n",
      "rank loss: 0.6063515543937683\n",
      "rank loss: 0.7868724465370178\n",
      "rank loss: 0.6820158362388611\n",
      "rank loss: 0.7796844244003296\n",
      "rank loss: 0.6865296959877014\n",
      "rank loss: 0.9413473010063171\n",
      "rank loss: 0.7921463847160339\n",
      "rank loss: 0.7251255512237549\n",
      "rank loss: 0.8625622987747192\n",
      "rank loss: 0.6817982196807861\n",
      "rank loss: 0.7626045346260071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank loss: 0.737925112247467\n",
      "rank loss: 0.6597367525100708\n",
      "rank loss: 0.7327467203140259\n",
      "rank loss: 0.770077645778656\n",
      "rank loss: 0.6913379430770874\n",
      "rank loss: 0.7471276521682739\n",
      "rank loss: 0.6643778681755066\n",
      "rank loss: 0.5819051265716553\n",
      "rank loss: 0.8806478977203369\n",
      "rank loss: 0.6706067323684692\n",
      "rank loss: 0.6732643246650696\n",
      "rank loss: 0.7268778681755066\n",
      "rank loss: 0.7179862856864929\n",
      "rank loss: 0.8077387809753418\n",
      "rank loss: 0.7792778611183167\n",
      "rank loss: 0.806076169013977\n",
      "rank loss: 0.6893807649612427\n",
      "rank loss: 0.793549656867981\n",
      "rank loss: 0.7210186719894409\n",
      "rank loss: 0.6901152729988098\n",
      "rank loss: 0.7053525447845459\n",
      "rank loss: 0.71075439453125\n",
      "rank loss: 0.7472701072692871\n",
      "rank loss: 0.767518162727356\n",
      "rank loss: 0.6974484324455261\n",
      "rank loss: 0.7199957370758057\n",
      "rank loss: 0.744120180606842\n",
      "rank loss: 0.8410906195640564\n",
      "rank loss: 0.8157952427864075\n",
      "rank loss: 0.7587652206420898\n",
      "rank loss: 0.7769191861152649\n",
      "rank loss: 0.7250230312347412\n",
      "rank loss: 0.7740619778633118\n",
      "rank loss: 0.7155950665473938\n",
      "rank loss: 0.7247358560562134\n",
      "rank loss: 0.6925536394119263\n",
      "rank loss: 0.7484229207038879\n",
      "rank loss: 0.892684280872345\n",
      "rank loss: 0.6706348061561584\n",
      "rank loss: 0.706180989742279\n",
      "rank loss: 0.6935202479362488\n",
      "rank loss: 0.7277265787124634\n",
      "rank loss: 0.7879413366317749\n",
      "rank loss: 0.7792882323265076\n",
      "rank loss: 0.7306078672409058\n",
      "rank loss: 0.7510449290275574\n",
      "rank loss: 0.7498006820678711\n",
      "rank loss: 0.7548168897628784\n",
      "rank loss: 0.775630533695221\n",
      "rank loss: 0.7705621123313904\n",
      "rank loss: 0.6974973082542419\n",
      "rank loss: 0.6091756820678711\n",
      "rank loss: 0.7469552159309387\n",
      "rank loss: 0.6907338500022888\n",
      "rank loss: 0.6584806442260742\n",
      "rank loss: 0.6811636686325073\n",
      "rank loss: 0.7804473638534546\n",
      "rank loss: 0.6828099489212036\n",
      "rank loss: 0.6977542638778687\n",
      "rank loss: 0.7132837772369385\n",
      "rank loss: 0.6234871745109558\n",
      "rank loss: 0.7811900973320007\n",
      "rank loss: 0.628422737121582\n",
      "rank loss: 0.712580680847168\n",
      "rank loss: 0.7677356004714966\n",
      "rank loss: 0.7582566142082214\n",
      "rank loss: 0.8337233066558838\n",
      "rank loss: 0.658390998840332\n",
      "rank loss: 0.7316014170646667\n",
      "rank loss: 0.7876807451248169\n",
      "rank loss: 0.7509428858757019\n",
      "rank loss: 0.7556010484695435\n",
      "rank loss: 0.6184743642807007\n",
      "rank loss: 0.7956995964050293\n",
      "rank loss: 0.7536512613296509\n",
      "rank loss: 0.7894156575202942\n",
      "rank loss: 0.8274073600769043\n",
      "rank loss: 0.6885653734207153\n",
      "rank loss: 0.6982171535491943\n",
      "rank loss: 0.6165612936019897\n",
      "rank loss: 0.7266915440559387\n",
      "rank loss: 0.6851336359977722\n",
      "rank loss: 0.7026510238647461\n",
      "rank loss: 0.7002781629562378\n",
      "rank loss: 0.6472254991531372\n",
      "rank loss: 0.7538996934890747\n",
      "rank loss: 0.7690287828445435\n",
      "rank loss: 0.7390775084495544\n",
      "rank loss: 0.6896514892578125\n",
      "rank loss: 0.7850654721260071\n",
      "rank loss: 0.7766036987304688\n",
      "rank loss: 0.8583245873451233\n",
      "rank loss: 0.7432883977890015\n",
      "rank loss: 0.7382515072822571\n",
      "rank loss: 0.7437818646430969\n",
      "rank loss: 0.7092800736427307\n",
      "rank loss: 0.8059676885604858\n",
      "rank loss: 0.718556821346283\n",
      "rank loss: 0.6721505522727966\n",
      "rank loss: 0.8126214146614075\n",
      "rank loss: 0.659613847732544\n",
      "rank loss: 0.825005292892456\n",
      "rank loss: 0.7172276377677917\n",
      "rank loss: 0.6603649854660034\n",
      "rank loss: 0.6818959712982178\n",
      "rank loss: 0.7203822731971741\n",
      "rank loss: 0.7975262403488159\n",
      "rank loss: 0.6262526512145996\n",
      "rank loss: 0.769688069820404\n",
      "rank loss: 0.7762437462806702\n",
      "rank loss: 0.7329064607620239\n",
      "rank loss: 0.693737804889679\n",
      "rank loss: 0.7354816198348999\n",
      "rank loss: 0.6494367122650146\n",
      "rank loss: 0.7380733489990234\n",
      "rank loss: 0.7667945623397827\n",
      "rank loss: 0.6537653207778931\n",
      "rank loss: 0.6964371800422668\n",
      "rank loss: 0.7328519225120544\n",
      "rank loss: 0.7737157940864563\n",
      "rank loss: 0.7718208432197571\n",
      "rank loss: 0.7385873794555664\n",
      "rank loss: 0.7190026640892029\n",
      "rank loss: 0.6408721804618835\n",
      "rank loss: 0.739414632320404\n",
      "rank loss: 0.6623533368110657\n",
      "rank loss: 0.7281004190444946\n",
      "rank loss: 0.7657856345176697\n",
      "rank loss: 0.6561795473098755\n",
      "rank loss: 0.7008517980575562\n",
      "rank loss: 0.9457716345787048\n",
      "rank loss: 0.8017745614051819\n",
      "rank loss: 0.6987770795822144\n",
      "rank loss: 0.6447740793228149\n",
      "rank loss: 0.6030371785163879\n",
      "rank loss: 0.6784791350364685\n",
      "rank loss: 0.6594073176383972\n",
      "rank loss: 0.739669680595398\n",
      "rank loss: 0.7226297855377197\n",
      "rank loss: 0.7111102342605591\n",
      "rank loss: 0.7681536674499512\n",
      "rank loss: 0.709880530834198\n",
      "rank loss: 0.7203822731971741\n",
      "rank loss: 0.8086597323417664\n",
      "rank loss: 0.6557056307792664\n",
      "rank loss: 0.7287012934684753\n",
      "rank loss: 0.71744704246521\n",
      "rank loss: 0.7034635543823242\n",
      "rank loss: 0.6971935629844666\n",
      "rank loss: 0.6320270299911499\n",
      "rank loss: 0.6845946907997131\n",
      "rank loss: 0.7471185326576233\n",
      "rank loss: 0.7450041174888611\n",
      "rank loss: 0.8066973090171814\n",
      "rank loss: 0.6474940180778503\n",
      "rank loss: 0.7337719798088074\n",
      "rank loss: 0.833105742931366\n",
      "rank loss: 0.7143642902374268\n",
      "rank loss: 0.7119085788726807\n",
      "rank loss: 0.7455219030380249\n",
      "rank loss: 0.6843487620353699\n",
      "rank loss: 0.7034534811973572\n",
      "rank loss: 0.7199310660362244\n",
      "rank loss: 0.7735763192176819\n",
      "rank loss: 0.78842693567276\n",
      "rank loss: 0.6669327616691589\n",
      "rank loss: 0.5110282897949219\n",
      "rank loss: 0.6765914559364319\n",
      "rank loss: 0.6905250549316406\n",
      "rank loss: 0.6949349045753479\n",
      "rank loss: 0.5935171842575073\n",
      "rank loss: 0.8261511325836182\n",
      "rank loss: 0.692184329032898\n",
      "rank loss: 0.8799465894699097\n",
      "rank loss: 0.7407960891723633\n",
      "rank loss: 0.9121506810188293\n",
      "rank loss: 0.6687794327735901\n",
      "rank loss: 0.7821429967880249\n",
      "rank loss: 0.6690878868103027\n",
      "rank loss: 0.8681135177612305\n",
      "rank loss: 0.6963409781455994\n",
      "rank loss: 0.7909685969352722\n",
      "rank loss: 0.7515109181404114\n",
      "rank loss: 0.7107878923416138\n",
      "rank loss: 0.7411239743232727\n",
      "rank loss: 0.6687540411949158\n",
      "rank loss: 0.7651452422142029\n",
      "rank loss: 0.7536088228225708\n",
      "rank loss: 0.7125567197799683\n",
      "rank loss: 0.6593777537345886\n",
      "rank loss: 0.711880624294281\n",
      "rank loss: 0.6323981285095215\n",
      "rank loss: 0.6650959849357605\n",
      "rank loss: 0.7358607053756714\n",
      "rank loss: 0.797839343547821\n",
      "rank loss: 0.643318235874176\n",
      "rank loss: 0.6931887865066528\n",
      "rank loss: 0.6611961722373962\n",
      "rank loss: 0.6064631342887878\n",
      "rank loss: 0.6618221998214722\n",
      "rank loss: 0.653900146484375\n",
      "rank loss: 0.6847787499427795\n",
      "rank loss: 0.6243827939033508\n",
      "rank loss: 0.6569636464118958\n",
      "rank loss: 0.6544821262359619\n",
      "rank loss: 0.7797561287879944\n",
      "rank loss: 0.804889976978302\n",
      "rank loss: 0.6250828504562378\n",
      "rank loss: 0.783243715763092\n",
      "rank loss: 0.5965510606765747\n",
      "rank loss: 0.7618312835693359\n",
      "rank loss: 0.6780470609664917\n",
      "rank loss: 0.6049169301986694\n",
      "rank loss: 0.6546959280967712\n",
      "rank loss: 0.6294293999671936\n",
      "rank loss: 0.6801621317863464\n",
      "rank loss: 0.648180365562439\n",
      "rank loss: 0.6765095591545105\n",
      "rank loss: 0.6421607136726379\n",
      "rank loss: 0.5931229591369629\n",
      "rank loss: 0.7685007452964783\n",
      "rank loss: 0.7237808108329773\n",
      "rank loss: 0.6996473670005798\n",
      "rank loss: 0.6261988878250122\n",
      "rank loss: 0.6771951913833618\n",
      "rank loss: 0.7758859992027283\n",
      "rank loss: 0.6318004727363586\n",
      "rank loss: 0.7317987084388733\n",
      "rank loss: 0.7975777983665466\n",
      "rank loss: 0.6417214870452881\n",
      "rank loss: 0.7875137329101562\n",
      "rank loss: 0.6336554288864136\n",
      "rank loss: 0.7219669818878174\n",
      "rank loss: 0.9671717882156372\n",
      "rank loss: 0.638486385345459\n",
      "rank loss: 0.6606633067131042\n",
      "rank loss: 0.7182794809341431\n",
      "rank loss: 0.7389106750488281\n",
      "rank loss: 0.8421539068222046\n",
      "rank loss: 0.7137001156806946\n",
      "rank loss: 0.590011715888977\n",
      "rank loss: 0.7203585505485535\n",
      "rank loss: 0.6356508731842041\n",
      "rank loss: 0.5737633109092712\n",
      "rank loss: 0.6945591568946838\n",
      "rank loss: 0.6061098575592041\n",
      "rank loss: 0.6462772488594055\n",
      "rank loss: 0.6715423464775085\n",
      "rank loss: 0.7182119488716125\n",
      "rank loss: 0.6863952279090881\n",
      "rank loss: 0.6790376305580139\n",
      "rank loss: 0.7990505695343018\n",
      "rank loss: 0.8223305344581604\n",
      "rank loss: 0.830556333065033\n",
      "rank loss: 0.6759658455848694\n",
      "rank loss: 0.5466009378433228\n",
      "rank loss: 0.7383517026901245\n",
      "rank loss: 0.7166489958763123\n",
      "rank loss: 0.6811591386795044\n",
      "rank loss: 0.6139292120933533\n",
      "rank loss: 0.7461608648300171\n",
      "rank loss: 0.7892230749130249\n",
      "rank loss: 0.7396464943885803\n",
      "rank loss: 0.6723862290382385\n",
      "rank loss: 0.7255169749259949\n",
      "rank loss: 0.6682771444320679\n",
      "rank loss: 0.6947935223579407\n",
      "rank loss: 0.6835759878158569\n",
      "rank loss: 0.6902081966400146\n",
      "rank loss: 0.7095435857772827\n",
      "rank loss: 0.6115164756774902\n",
      "rank loss: 0.7181674242019653\n",
      "rank loss: 0.6825615763664246\n",
      "rank loss: 0.775172233581543\n",
      "rank loss: 0.6167833805084229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank loss: 0.6410656571388245\n",
      "rank loss: 0.7186697125434875\n",
      "rank loss: 0.6708215475082397\n",
      "rank loss: 0.7179554104804993\n",
      "rank loss: 0.6138277053833008\n",
      "rank loss: 0.7500765919685364\n",
      "rank loss: 0.6288455724716187\n",
      "rank loss: 0.7329901456832886\n",
      "rank loss: 0.7868987917900085\n",
      "rank loss: 0.6716737747192383\n",
      "rank loss: 0.6971812844276428\n",
      "rank loss: 0.8783232569694519\n",
      "rank loss: 0.604184091091156\n",
      "rank loss: 0.6997931003570557\n",
      "rank loss: 0.7203173637390137\n",
      "rank loss: 0.7120301723480225\n",
      "rank loss: 0.7207215428352356\n",
      "rank loss: 0.6839776635169983\n",
      "rank loss: 0.6435736417770386\n",
      "rank loss: 0.6985917687416077\n",
      "rank loss: 0.7194593548774719\n",
      "rank loss: 0.7111312747001648\n",
      "rank loss: 0.7160836458206177\n",
      "rank loss: 0.6214325428009033\n",
      "rank loss: 0.7272852659225464\n",
      "rank loss: 0.6682885885238647\n",
      "rank loss: 0.5944069027900696\n",
      "rank loss: 0.741405725479126\n",
      "rank loss: 0.6204160451889038\n",
      "rank loss: 0.6566944718360901\n",
      "rank loss: 0.6274212598800659\n",
      "rank loss: 0.8463570475578308\n",
      "rank loss: 0.7300096154212952\n",
      "rank loss: 0.6893354654312134\n",
      "rank loss: 0.6010086536407471\n",
      "rank loss: 0.711456835269928\n",
      "rank loss: 0.6825979351997375\n",
      "rank loss: 0.6302615404129028\n",
      "rank loss: 0.6130582094192505\n",
      "rank loss: 0.6342164278030396\n",
      "rank loss: 0.6586765050888062\n",
      "rank loss: 0.6632974147796631\n",
      "rank loss: 0.6557524800300598\n",
      "rank loss: 0.7110581994056702\n",
      "rank loss: 0.7693233489990234\n",
      "rank loss: 0.6475328207015991\n",
      "rank loss: 0.7465093731880188\n",
      "rank loss: 0.73811274766922\n",
      "rank loss: 0.5932555794715881\n",
      "rank loss: 0.798713207244873\n",
      "rank loss: 0.7843601703643799\n",
      "rank loss: 0.7352254986763\n",
      "rank loss: 0.6552119255065918\n",
      "rank loss: 0.71482253074646\n",
      "rank loss: 0.5730706453323364\n",
      "rank loss: 0.611466109752655\n",
      "rank loss: 0.6228666305541992\n",
      "rank loss: 0.630669891834259\n",
      "rank loss: 0.6342323422431946\n",
      "rank loss: 0.5910676121711731\n",
      "rank loss: 0.6787199378013611\n",
      "rank loss: 0.5950111746788025\n",
      "rank loss: 0.5935364961624146\n",
      "rank loss: 0.7311526536941528\n",
      "rank loss: 0.7091009616851807\n",
      "rank loss: 0.6676292419433594\n",
      "rank loss: 0.7543035745620728\n",
      "rank loss: 0.6090953350067139\n",
      "rank loss: 0.5362014174461365\n",
      "rank loss: 0.5944912433624268\n",
      "rank loss: 0.5846669673919678\n",
      "rank loss: 0.6350727081298828\n",
      "rank loss: 0.6181442141532898\n",
      "rank loss: 0.6189355850219727\n",
      "rank loss: 0.6776542663574219\n",
      "rank loss: 0.633488655090332\n",
      "rank loss: 0.7022010684013367\n",
      "rank loss: 0.5637120604515076\n",
      "rank loss: 0.6220702528953552\n",
      "rank loss: 0.6252496242523193\n",
      "rank loss: 0.7721942067146301\n",
      "rank loss: 0.6242759823799133\n",
      "rank loss: 0.6024295091629028\n",
      "rank loss: 0.6205334067344666\n",
      "rank loss: 0.772030770778656\n",
      "rank loss: 0.6396328210830688\n",
      "rank loss: 0.5192233324050903\n",
      "rank loss: 0.6753228902816772\n",
      "rank loss: 0.6306188702583313\n",
      "rank loss: 0.6707199215888977\n",
      "rank loss: 0.7807222008705139\n",
      "rank loss: 0.6329970359802246\n",
      "rank loss: 0.6884057521820068\n",
      "rank loss: 0.7268710136413574\n",
      "rank loss: 0.7431788444519043\n",
      "rank loss: 0.5831739902496338\n",
      "rank loss: 0.5463195443153381\n",
      "rank loss: 0.6340363025665283\n",
      "rank loss: 0.6693034768104553\n",
      "rank loss: 0.609899640083313\n",
      "rank loss: 0.6354235410690308\n",
      "rank loss: 0.8190970420837402\n",
      "rank loss: 0.7251089811325073\n",
      "rank loss: 0.6918449997901917\n",
      "rank loss: 0.5612717270851135\n",
      "rank loss: 0.6416782736778259\n",
      "rank loss: 0.6350231170654297\n",
      "rank loss: 0.6610043048858643\n",
      "rank loss: 0.6478026509284973\n",
      "rank loss: 0.6444799900054932\n",
      "rank loss: 0.7117397785186768\n",
      "rank loss: 0.6289036273956299\n",
      "rank loss: 0.6835954785346985\n",
      "rank loss: 0.727340042591095\n",
      "rank loss: 0.7078953385353088\n",
      "rank loss: 0.6025688648223877\n",
      "rank loss: 0.6620236039161682\n",
      "rank loss: 0.7643910050392151\n",
      "rank loss: 0.5506279468536377\n",
      "rank loss: 0.6918096542358398\n",
      "rank loss: 0.6421416997909546\n",
      "rank loss: 0.6941789984703064\n",
      "rank loss: 0.7485069036483765\n",
      "rank loss: 0.6517234444618225\n",
      "rank loss: 0.8014329075813293\n",
      "rank loss: 0.6496092677116394\n",
      "rank loss: 0.6096060276031494\n",
      "rank loss: 0.536637008190155\n",
      "rank loss: 0.7259924411773682\n",
      "rank loss: 0.6545275449752808\n",
      "rank loss: 0.6654058694839478\n",
      "rank loss: 0.7194057106971741\n",
      "rank loss: 0.6038721799850464\n",
      "rank loss: 0.7065640687942505\n",
      "rank loss: 0.5205593705177307\n",
      "rank loss: 0.6723049283027649\n",
      "rank loss: 0.7194897532463074\n",
      "rank loss: 0.6656251549720764\n",
      "rank loss: 0.6115875840187073\n",
      "rank loss: 0.7376593351364136\n",
      "rank loss: 0.6457136273384094\n",
      "rank loss: 0.6093028783798218\n",
      "rank loss: 0.6653038859367371\n",
      "rank loss: 0.6520519852638245\n",
      "rank loss: 0.613615870475769\n",
      "rank loss: 0.5687196254730225\n",
      "rank loss: 0.6978420615196228\n",
      "rank loss: 0.646117091178894\n",
      "rank loss: 0.6926645040512085\n",
      "rank loss: 0.8143912553787231\n",
      "rank loss: 0.6578648090362549\n",
      "rank loss: 0.5784918665885925\n",
      "rank loss: 0.6356370449066162\n",
      "rank loss: 0.7172613143920898\n",
      "rank loss: 0.8375624418258667\n",
      "rank loss: 0.679559588432312\n",
      "rank loss: 0.6434899568557739\n",
      "rank loss: 0.8175187706947327\n",
      "rank loss: 0.7283597588539124\n",
      "rank loss: 0.6177857518196106\n",
      "rank loss: 0.5548602938652039\n",
      "rank loss: 0.572368860244751\n",
      "rank loss: 0.6436416506767273\n",
      "rank loss: 0.7509183287620544\n",
      "rank loss: 0.7199119925498962\n",
      "rank loss: 0.6479599475860596\n",
      "rank loss: 0.6691229343414307\n",
      "rank loss: 0.7047966122627258\n",
      "rank loss: 0.6451947093009949\n",
      "rank loss: 0.6370512843132019\n",
      "rank loss: 0.5790812373161316\n",
      "rank loss: 0.5943660736083984\n",
      "rank loss: 0.6436397433280945\n",
      "rank loss: 0.695873498916626\n",
      "rank loss: 0.7290075421333313\n",
      "Time Cost: 1215.0848050117493 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_t = time.time()\n",
    "for i in range(1000):\n",
    "    # One Step Forward\n",
    "    X1, X1_len, X1_id, X2, X2_len, X2_id, Y, F = \\\n",
    "        pair_gen.get_batch(data1=loader.query_data, data2=loader.doc_data)\n",
    "    X1, X1_len, X2, X2_len, Y, F = \\\n",
    "        to_device(X1, X1_len, X2, X2_len, Y, F, device=select_device)\n",
    "    X1, X2, X1_len, X2_len, X2_pos = select_net(X1, X2, X1_len, X2_len, X1_id, X2_id)\n",
    "    X2, X2_len = utils.data_adaptor(X2, X2_len, select_net, rank_net, letor_config)\n",
    "    output = rank_net(X1, X2, X1_len, X2_len, X2_pos)\n",
    "    \n",
    "    # Update Rank Net\n",
    "    rank_loss = rank_net.pair_loss(output, Y)\n",
    "    print('rank loss:', rank_loss.item())\n",
    "    rank_optimizer.zero_grad()\n",
    "    rank_loss.backward()\n",
    "    rank_optimizer.step()\n",
    "    \n",
    "end_t = time.time()\n",
    "print('Time Cost: %s s' % (end_t-start_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(select_net, \"qcentric.model\")\n",
    "torch.save(rank_net, \"deeprank.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[./data/letor/r5w/relation.test.fold1.txt]\n",
      "\tInstance size: 13652\n",
      "List Instance Count: 336\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(50, 20) (50, 2000) (50,)\n",
      "(51, 20) (51, 2000) (51,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(46, 20) (46, 2000) (46,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(50, 20) (50, 2000) (50,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(50, 20) (50, 2000) (50,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(41, 20) (41, 2000) (41,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(80, 20) (80, 2000) (80,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(45, 20) (45, 2000) (45,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(41, 20) (41, 2000) (41,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(50, 20) (50, 2000) (50,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(79, 20) (79, 2000) (79,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(41, 20) (41, 2000) (41,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(41, 20) (41, 2000) (41,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(50, 20) (50, 2000) (50,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(45, 20) (45, 2000) (45,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(41, 20) (41, 2000) (41,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(48, 20) (48, 2000) (48,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(53, 20) (53, 2000) (53,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(50, 20) (50, 2000) (50,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(60, 20) (60, 2000) (60,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "[Test] 0.39690111471051653\n"
     ]
    }
   ],
   "source": [
    "select_net_e = torch.load(f='qcentric.model')\n",
    "rank_net_e = torch.load(f='deeprank.model')\n",
    "\n",
    "list_gen = ListGenerator(rel_file=Letor07Path+'/relation.test.fold%d.txt'%(letor_config['fold']),\n",
    "                         config=letor_config)\n",
    "map_v = 0.0\n",
    "map_c = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X1, X1_len, X1_id, X2, X2_len, X2_id, Y, F in \\\n",
    "        list_gen.get_batch(data1=loader.query_data, data2=loader.doc_data):\n",
    "        print(X1.shape, X2.shape, Y.shape)\n",
    "        X1, X1_len, X2, X2_len, Y, F = to_device(X1, X1_len, X2, X2_len, Y, F, device=select_device)\n",
    "        X1, X2, X1_len, X2_len, X2_pos = select_net_e(X1, X2, X1_len, X2_len, X1_id, X2_id)\n",
    "        X2, X2_len = utils.data_adaptor(X2, X2_len, select_net, rank_net, letor_config)\n",
    "        #print(X1.shape, X2.shape, Y.shape)\n",
    "        pred = rank_net_e(X1, X2, X1_len, X2_len, X2_pos)\n",
    "        map_o = utils.eval_MAP(pred.tolist(), Y.tolist())\n",
    "        #print(pred.shape, Y.shape)\n",
    "        map_v += map_o\n",
    "        map_c += 1.0\n",
    "    map_v /= map_c\n",
    "\n",
    "print('[Test]', map_v)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Reward v0: 0.4359386539000405\n",
    "Reward v1: 0.42572616969349864\n",
    "Reward v2: 0.4245778777643799"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nips",
   "language": "python",
   "name": "nips"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
