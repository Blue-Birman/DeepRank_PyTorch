{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from importlib import reload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeprank.dataset import DataLoader, PairGenerator, ListGenerator\n",
    "from deeprank import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff7ed768b10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1234\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[./data/letor/r5w/word_dict.txt]\n",
      "\tWord dict size: 193367\n",
      "[./data/letor/r5w/qid_query.txt]\n",
      "\tData size: 1692\n",
      "[./data/letor/r5w/docid_doc.txt]\n",
      "\tData size: 65323\n",
      "[./data/letor/r5w/embed_wiki-pdc_d50_norm]\n",
      "\tEmbedding size: 109282\n",
      "[./data/letor/r5w/embed.idf]\n",
      "\tEmbedding size: 193367\n",
      "Generate numpy embed: (193368, 50)\n",
      "Generate numpy embed: (193368, 1)\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader('./config/letor07_mp_fold1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "letor_config = json.loads(open('./config/letor07_mp_fold1.model').read())\n",
    "#device = torch.device(\"cuda\")\n",
    "#device = torch.device(\"cpu\")\n",
    "select_device = torch.device(\"cpu\")\n",
    "rank_device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[./data/letor/r5w/relation.train.fold1.txt]\n",
      "\tInstance size: 47828\n",
      "Pair Instance Count: 325439\n"
     ]
    }
   ],
   "source": [
    "Letor07Path = letor_config['data_dir']\n",
    "\n",
    "letor_config['fill_word'] = loader._PAD_\n",
    "letor_config['embedding'] = loader.embedding\n",
    "letor_config['feat_size'] = loader.feat_size\n",
    "letor_config['vocab_size'] = loader.embedding.shape[0]\n",
    "letor_config['embed_dim'] = loader.embedding.shape[1]\n",
    "letor_config['pad_value'] = loader._PAD_\n",
    "\n",
    "pair_gen = PairGenerator(rel_file=Letor07Path + '/relation.train.fold%d.txt'%(letor_config['fold']), \n",
    "                         config=letor_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeprank import select_module\n",
    "from deeprank import rank_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryCentricNet()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letor_config['max_match'] = 20\n",
    "letor_config['win_size'] = 5\n",
    "select_net = select_module.QueryCentricNet(config=letor_config, out_device=rank_device)\n",
    "select_net = select_net.to(select_device)\n",
    "select_net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nletor_config['q_limit'] = 20\\nletor_config['d_limit'] = 2000\\nletor_config['max_match'] = 20\\nletor_config['win_size'] = 5\\nletor_config['finetune_embed'] = True\\nletor_config['lr'] = 0.0001\\nselect_net = select_module.PointerNet(config=letor_config)\\nselect_net = select_net.to(device)\\nselect_net.embedding.weight.data.copy_(torch.from_numpy(loader.embedding))\\nselect_net.train()\\nselect_optimizer = optim.RMSprop(select_net.parameters(), lr=letor_config['lr'])\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "letor_config['q_limit'] = 20\n",
    "letor_config['d_limit'] = 2000\n",
    "letor_config['max_match'] = 20\n",
    "letor_config['win_size'] = 5\n",
    "letor_config['finetune_embed'] = True\n",
    "letor_config['lr'] = 0.0001\n",
    "select_net = select_module.PointerNet(config=letor_config)\n",
    "select_net = select_net.to(device)\n",
    "select_net.embedding.weight.data.copy_(torch.from_numpy(loader.embedding))\n",
    "select_net.train()\n",
    "select_optimizer = optim.RMSprop(select_net.parameters(), lr=letor_config['lr'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "letor_config[\"dim_q\"] = 1\n",
    "letor_config[\"dim_d\"] = 1\n",
    "letor_config[\"dim_weight\"] = 1\n",
    "letor_config[\"c_reduce\"] = [1, 1]\n",
    "letor_config[\"k_reduce\"] = [1, 50]\n",
    "letor_config[\"s_reduce\"] = 1\n",
    "letor_config[\"p_reduce\"] = [0, 0]\n",
    "\n",
    "letor_config[\"c_en_conv_out\"] = 4\n",
    "letor_config[\"k_en_conv\"] = 3\n",
    "letor_config[\"s_en_conv\"] = 1\n",
    "letor_config[\"p_en_conv\"] = 1\n",
    "\n",
    "letor_config[\"en_pool_out\"] = [1, 1]\n",
    "letor_config[\"en_leaky\"] = 0.2\n",
    "\n",
    "letor_config[\"dim_gru_hidden\"] = 3\n",
    "\n",
    "letor_config['lr'] = 0.005\n",
    "letor_config['finetune_embed'] = False\n",
    "\n",
    "rank_net = rank_module.DeepRankNet(config=letor_config)\n",
    "rank_net = rank_net.to(rank_device)\n",
    "rank_net.embedding.weight.data.copy_(torch.from_numpy(loader.embedding))\n",
    "rank_net.qw_embedding.weight.data.copy_(torch.from_numpy(loader.idf_embedding))\n",
    "rank_net.train()\n",
    "rank_optimizer = optim.Adam(rank_net.parameters(), lr=letor_config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(*variables, device):\n",
    "    return (torch.from_numpy(variable).to(device) for variable in variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_text(x):\n",
    "    print(' '.join([loader.word_dict[w.item()] for w in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anniversary whiskeytown dam dedication whiskeytown dam anniversary celebration september visitor help celebrate anniversary president john kennedy dedication whiskeytown dam weekend free special event sept long history seminar turtle bay saturday cover topic multiple perspective water issue recreation overview president kennedy life speech re living exciting moments president person years ago sunday formal ceremony start whiskeytown dam music shasta high school band short speech big dam party follow formal ceremony vendor special interest booth line dam live music provided band billy richards coaster penny lane shadow press release superintendent jim milestone observe good time visitor learned lot national park service whiskeytown local support made happen years ago years ago local support made anniversary celebration happen thank help make event success local support event friends whiskeytown mcconnell foundation turtle bay city red raba bus bureau reclamation forest service bureau land management hill win river casino enterprise pacific west graphics whiskeytown employee vendor exhibitor countless individual historic photo gallery links kennedy memorial kennedy speech overview anniversary event held president john kennedy dedicated whiskeytown dam september turn trip california fact whiskeytown dam central valley project cvp result profound change california economy significant recreational opportunity available decided commemorate significant event chose commemorate anniversary wait involved way original event pass away able attend speak experience september event interest know share history creation whiskeytown seminar activity held turtle bay bout attendee focus whiskeytown central valley project cvp original kennedy dedication september dedication celebration began hour big dam party follow formal presentation total attendee vendor booth dam area east end dam kennedy memorial drive closed road dam stage live band recreation related booth dam educational exhibit history creation development whiskeytown original dedication event video original ceremony visit commemorative poster lapel pin memorabilia available purchase event available whiskeytown visitor center supplies www nps gov whi exp htm $ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40th president $$ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "0 september visitor help celebrate anniversary president john kennedy dedication whiskeytown dam\n",
      "1 perspective water issue recreation overview president kennedy life speech re living\n",
      "2 speech re living exciting moments president person years ago sunday formal\n",
      "3 speech overview anniversary event held president john kennedy dedicated whiskeytown dam\n",
      "4 "
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for dimension 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0a680ee84d2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mshow_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for dimension 0 with size 4"
     ]
    }
   ],
   "source": [
    "X1, X1_len, X1_id, X2, X2_len, X2_id, Y, F = \\\n",
    "        pair_gen.get_batch(data1=loader.query_data, data2=loader.doc_data)\n",
    "X1, X1_len, X2, X2_len, Y, F = \\\n",
    "        to_device(X1, X1_len, X2, X2_len, Y, F, device=rank_device)\n",
    "\n",
    "show_text(X2[0])\n",
    "\n",
    "X1, X2_new, X1_len, X2_len_new, X2_pos = select_net(X1, X2, X1_len, X2_len, X1_id, X2_id)\n",
    "\n",
    "show_text(X1[0])\n",
    "for i in range(5):\n",
    "    print(i, end=' ')\n",
    "    show_text(X2_new[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 13.,  38.,  46., 165.], device='cuda:0'), tensor([ 1., 11., 13., 37.], device='cuda:0'), tensor([2.4200e+02, 3.0900e+02, 3.2700e+02, 3.3500e+02, 4.2700e+02, 5.8000e+02,\n",
      "        1.0820e+03, 1.1360e+03, 1.1660e+03, 1.2480e+03, 1.4300e+03, 1.5600e+03,\n",
      "        1.5780e+03, 1.8690e+03, 1.0000e+00, 9.0000e+00, 3.1000e+01, 4.3000e+01,\n",
      "        6.5000e+01, 1.9700e+02, 4.3800e+02, 7.2900e+02, 9.5300e+02, 1.1980e+03,\n",
      "        1.4460e+03, 1.6940e+03, 1.9490e+03, 9.5600e+02, 1.0320e+03, 1.6220e+03,\n",
      "        9.0500e+02, 9.0900e+02, 9.2600e+02, 9.5700e+02, 9.7600e+02, 1.0330e+03,\n",
      "        1.1220e+03], device='cuda:0'), tensor([49., 50.], device='cuda:0'), tensor([ 58., 152.,  59., 153.], device='cuda:0'), tensor([ 4.,  5., 18., 19., 55., 56.], device='cuda:0'), tensor([  3.,  52.,  78.,  87., 111., 143., 193., 199., 260.,   4.,  53.,  79.,\n",
      "         84.,  88., 102., 112., 115., 144., 180., 194., 200., 233., 261., 286.,\n",
      "        338., 341., 347., 357., 408.], device='cuda:0'), tensor([ 43.,  59., 113.,  44.,  52.,  60., 112., 114.], device='cuda:0'), tensor([  2.,  18.,  29.,  82.,  84.,  97., 105., 111., 166.], device='cuda:0'), tensor([ 3., 59.], device='cuda:0'), tensor([  40.,  858.,  867., 1121.,   96.,  108.,  117.,  125.,  162.,  170.,\n",
      "         188.,  194.,  197.,  202.,  236.,  242.,  251.,  256.,  282.,  300.,\n",
      "         307.,  312.,  315.,  321.,  508.,  531., 1179., 1881.],\n",
      "       device='cuda:0'), tensor([12.,  2., 11.], device='cuda:0'), tensor([52.,  2., 22., 38., 42., 57., 99., 58., 63., 71.], device='cuda:0'), tensor([ 93.,  30.,   0.,   5.,   8.,  14.,  20.,  23.,  27.,  33.,  37.,  48.,\n",
      "         50.,  52.,  60.,  66.,  74.,  76.,  90.,  94., 115., 133.],\n",
      "       device='cuda:0'), tensor([ 4., 37.], device='cuda:0'), tensor([256., 258., 342., 389., 450., 489., 506., 572., 718., 734., 742., 747.,\n",
      "        766., 774., 352., 261.], device='cuda:0'), tensor([   4.,   25.,  177.,  237.,  248.,  489.,  508.,  562., 1005., 1100.,\n",
      "        1142., 1403., 1428., 1460., 1946.,    5.,   26.,  178.,  238.,  249.,\n",
      "         490.,  509.,  563., 1006., 1101., 1143., 1404., 1429., 1461., 1947.,\n",
      "           7.,   28.,  180.,  239.,  250.,  329.,  474.,  476.,  492.,  511.,\n",
      "         565., 1007., 1406., 1430., 1463., 1948.], device='cuda:0'), tensor([41., 74., 36., 39., 42., 45., 49., 51., 55., 58., 61., 64., 68., 72.,\n",
      "        37., 40., 43., 46.], device='cuda:0'), tensor([ 10.,  34.,  35.,  40.,  54.,  56.,  71., 145., 150., 189., 108., 143.,\n",
      "        156., 166.], device='cuda:0'), tensor([  35.,   45.,   75.,   84.,  116.,  157.,  168.,  206.,  210.,  229.,\n",
      "         254.,  264.,  268.,  270.,  442.,  450.,  468.,  530.,  541.,  552.,\n",
      "         263.,  318.,  861., 1155., 1422., 1655., 1673., 1939.],\n",
      "       device='cuda:0'), tensor([   4.,   13.,   45.,  108.,  152.,  195.,  287.,  334.,  379.,  538.,\n",
      "         679.,  725.,  770.,  815.,  859.,  921.,  967., 1012., 1059., 1109.,\n",
      "          18.,   42.,  105.,  149.,  192.,  237.,  284.,  331.,  376.,  423.,\n",
      "         480.,  535.,  582.,  630.,  676.,  722.,  767.,  812.,  856.,  918.,\n",
      "           0.,   16.,   24.,   44.,   49.,   65.,  107.,  112.,  128.,  151.,\n",
      "         156.,  171.,  194.,  199.,  214.,  239.,  244.,  262.,  286.,  291.],\n",
      "       device='cuda:0'), tensor([ 24.,  29.,  45.,  96., 104., 156., 162., 209., 214., 237., 258., 278.,\n",
      "        282., 292., 323., 342., 344., 392., 424., 452.,  19.,  30.,  37.,  46.,\n",
      "         51.,  61.,  93., 200., 269., 324., 339., 419., 435., 438., 503., 318.,\n",
      "        350., 363., 382., 522.,  43.,  91.,  98., 199., 208., 239., 257., 280.,\n",
      "        338., 449., 483., 517.], device='cuda:0'), tensor([ 184.,  194.,  210.,  241.,  300.,  327.,  527.,  643.,  807., 1553.,\n",
      "        1593., 1654., 1703., 1737., 1855., 1920., 1940.,   36.,  113.,  182.,\n",
      "         958., 1927.], device='cuda:0'), tensor([177., 144., 149., 155., 163., 167., 171., 176., 180., 195., 214., 247.,\n",
      "        504., 558., 566., 574., 596., 616., 647.], device='cuda:0'), tensor([  1.,   3.,   9.,  13.,  27.,  41.,  50.,  82.,  88., 106., 144., 183.,\n",
      "        191., 216., 242., 304., 332.,  18., 146., 167., 229., 249., 279., 289.],\n",
      "       device='cuda:0'), tensor([ 260.,  276., 1025., 1105., 1115., 1150., 1619., 1721.,   65.,   74.,\n",
      "          84.,   92.,  115.,  123.,  132.,  140.,  170.,  178.,  187.,  210.,\n",
      "         966.,  976.,  985.,  993., 1001., 1018., 1027., 1035.],\n",
      "       device='cuda:0'), tensor([28., 55., 63.], device='cuda:0'), tensor([ 30., 303., 514.], device='cuda:0'), tensor([ 34.,  29.,  35.,  37.,   1.,  41.,  91.,  93.,  96.,  99.,   2.,  42.,\n",
      "         92.,  94.,  97., 100.,  15.,  17.,  73.,  79.,  82.], device='cuda:0'), tensor([  3.,  28., 414.,   0.,   8.,  13.,  17., 411., 423.,   1.,   9.,  14.,\n",
      "         18., 412., 424.,   4.,  29.,  31., 102., 427., 440.], device='cuda:0'), tensor([  7.,  10.,  13.,  27.,  35.,  58.,  74.,  84., 110., 117., 147.,   8.,\n",
      "         11.,  14.,  28.,  36.,  59.,  75.,  85., 111., 118., 130., 148.,  42.],\n",
      "       device='cuda:0'), tensor([  0.,  23.,  25.,  31.,  47.,  59.,  67.,  75.,  78.,  80.,  87., 102.,\n",
      "        112., 117., 119., 143., 146., 160., 162., 168.,   1.,  24.,  26.,  32.,\n",
      "         48.,  60.,  68.,  76.,  79.,  81.,  88., 103., 113., 118., 120., 144.,\n",
      "        147., 161., 163., 169.], device='cuda:0'), tensor([ 38., 197., 231., 245., 500., 501., 508., 531., 553., 561., 606., 766.,\n",
      "        772., 800., 830., 852., 873., 888., 889., 897.,   3.,  24.,  37.,  49.,\n",
      "         78.,  83.,  99., 107., 123., 180., 219., 224., 244., 276., 303., 307.,\n",
      "        338., 387., 507., 528.], device='cuda:0'), tensor([0., 9.], device='cuda:0'), tensor([0.0000e+00, 1.0900e+02, 1.9200e+02, 1.9800e+02, 2.2700e+02, 2.3200e+02,\n",
      "        2.3600e+02, 3.7000e+02, 3.7600e+02, 3.8200e+02, 4.4600e+02, 4.5000e+02,\n",
      "        5.4900e+02, 6.4300e+02, 6.5900e+02, 7.5500e+02, 9.1200e+02, 9.6800e+02,\n",
      "        9.7900e+02, 1.0110e+03, 1.0000e+00, 1.1000e+02, 1.9300e+02, 1.9900e+02,\n",
      "        2.0500e+02, 2.2800e+02, 2.3300e+02, 2.3700e+02, 2.4200e+02, 2.9600e+02,\n",
      "        2.9700e+02, 3.7100e+02, 3.7700e+02, 3.8300e+02, 4.5800e+02, 4.6800e+02,\n",
      "        5.5000e+02, 6.4400e+02, 6.6000e+02, 7.5600e+02, 2.5000e+01, 5.4000e+01,\n",
      "        7.7000e+01, 8.6000e+01, 9.7000e+01, 1.4300e+02, 2.3800e+02, 2.5900e+02,\n",
      "        2.7700e+02, 5.4300e+02, 5.5100e+02, 6.6100e+02, 7.5700e+02, 9.1400e+02,\n",
      "        1.0130e+03, 1.1740e+03, 1.2950e+03, 1.3640e+03, 1.5680e+03, 1.5830e+03,\n",
      "        2.4000e+01, 5.3000e+01, 7.5000e+01, 8.5000e+01, 9.6000e+01, 1.4200e+02,\n",
      "        6.6200e+02, 6.9400e+02, 7.1500e+02, 7.2000e+02, 7.4800e+02, 7.6100e+02,\n",
      "        7.9100e+02, 8.1200e+02, 8.2400e+02, 8.2700e+02, 8.3900e+02, 8.9500e+02,\n",
      "        9.0600e+02, 9.1600e+02], device='cuda:0'), tensor([ 104.,  277.,  331.,  336.,  606.,  611.,  633.,  655.,  660.,  682.,\n",
      "        1509., 1966.,   21.,   67.,  105.,  278.,  330.,  332.,  337.,  607.,\n",
      "         612.,  634.,  656.,  661.,  683.,  821., 1510., 1841., 1857., 1967.,\n",
      "         106.,  279.,  333.,  338.,  608.,  613.,  635.,  657.,  662.,  684.,\n",
      "        1511., 1968.,   40.,  103.,  192.,  203.,  267.,  268.,  276.,  294.,\n",
      "         342.,  572.,  583.,  595.,  632.,  649.,  654.,  681.,  727.,  730.,\n",
      "         742.,  755.], device='cuda:0'), tensor([24., 49., 25., 31., 39., 50.], device='cuda:0'), tensor([317.,   2.,  31.,  48.,  57.,  67.,  76.,  87.,  95., 104., 113., 123.,\n",
      "        132., 141., 152., 163., 172., 182., 192., 201., 211.], device='cuda:0'), tensor([   7.,   45.,   95.,  100.,  101.,  109.,  122.,  134.,  151.,  153.,\n",
      "         170.,  189.,  207.,  216.,  259.,  335.,  351.,  371.,  405.,  407.,\n",
      "         340.,  416.,  966., 1011., 1111., 1137., 1142., 1184., 1305., 1511.,\n",
      "        1564., 1670., 1812., 1856.], device='cuda:0'), tensor([  5.,  11., 469.], device='cuda:0'), tensor([580., 589., 612., 619., 626., 630., 638., 653., 670., 688.,  42., 117.,\n",
      "        123., 397., 404., 409., 415., 419., 423., 451., 456., 463., 477., 490.,\n",
      "        501., 526., 551., 562., 610., 614., 392., 335., 344., 400., 426., 435.,\n",
      "        605., 615., 618., 621., 624., 636., 814., 826., 836., 884., 886., 905.,\n",
      "        914., 917., 943.], device='cuda:0'), tensor([ 8., 12., 13., 14., 16.,  9.], device='cuda:0'), tensor([620., 844., 848., 853., 776., 783., 792., 808., 814.,  33.,  64., 502.,\n",
      "        526., 580., 587., 750., 815., 861., 816.], device='cuda:0'), tensor([  7.,  12.,  14.,  15.,  18.,  27.,  43.,  68.,  87.,  94., 100., 108.,\n",
      "         78.,  91.], device='cuda:0'), tensor([  7.,  20.,  37., 203., 207., 307., 331.,   8.,  21.,  38.,  62.,  89.,\n",
      "         97., 115., 129., 132., 168., 199., 204., 208., 220., 249., 257., 308.,\n",
      "        317., 319., 321.], device='cuda:0'), tensor([ 15.,  23.,  36.,  46.,  49.,  58.,  61., 155., 183., 210., 237., 332.,\n",
      "         32.,  35.,  48.,  60.,  93., 104., 120., 127., 153., 167., 205., 238.,\n",
      "        269.], device='cuda:0'), tensor([1235.,   29.,   37.,   43.,  121.,  136.,  227.,  302.,  323.,  328.,\n",
      "         365.,  381.,  397.,  416.,  421.,  579.,  593.,  607.,  698.,  759.,\n",
      "         768.,    0.,    9.,   30.,  329.,  835., 1078., 1197., 1284., 1291.,\n",
      "        1305.,   31.,   41.,   54.,   57.,  259.,  274.,  292.,  298.,  335.,\n",
      "         448.,  500.,  505.,  510.,  512.,  516.,  529.,  541.,  552.,  559.,\n",
      "         569.,   18.,   24.,   32.,   42.,   58.,  100.,  135.,  275.,  293.,\n",
      "         299.,  336.,  449.,  490.,  499.,  504.,  506.,  511.,  513.,  517.,\n",
      "         542.], device='cuda:0'), tensor([12.,  1., 28.,  0., 15., 17., 51., 39., 40.], device='cuda:0'), tensor([ 30.,  34.,  61.,  69.,  83., 100., 108., 147., 191., 213., 243., 258.,\n",
      "        317., 324., 348., 354., 371., 393., 275.,  41.,  48.,  73.,  91., 135.,\n",
      "        142., 163., 201., 250., 286., 298., 312., 329., 363., 378., 382.],\n",
      "       device='cuda:0'), tensor([ 3., 59.,  6., 63., 10., 19., 35., 44.], device='cuda:0'), tensor([ 24.,  35., 257., 260., 285., 288., 261.,   1.,  18.,  22.,  26.,  58.,\n",
      "         71., 259., 266., 284., 287.], device='cuda:0'), tensor([  37.,  108.,  222.,  236.,  242.,  333.,  339.,  352.,  386.,  404.,\n",
      "         419.,  428.,  432.,  694.,  814.,  854.,  864.,  902.,  980., 1055.],\n",
      "       device='cuda:0'), tensor([ 30.,  34.,  61.,  69.,  83., 100., 108., 147., 191., 213., 243., 258.,\n",
      "        317., 324., 348., 354., 371., 393., 275.,  41.,  48.,  73.,  91., 135.,\n",
      "        142., 163., 201., 250., 286., 298., 312., 329., 363., 378., 382.],\n",
      "       device='cuda:0'), tensor([  7.,  13.,  37.,  82., 123., 167.,   6.,  12.,  36.,  81., 122., 166.],\n",
      "       device='cuda:0'), tensor([  6.,  20.,  25.,  34.,  98., 111., 124., 131., 184., 259., 273., 336.,\n",
      "        343., 356., 477., 479., 494., 498., 527., 558.,   7.,  26.,  35.,  69.,\n",
      "         73.,  99., 112., 125., 478., 481., 495., 499., 521., 528., 545., 570.,\n",
      "        575., 597., 617., 629., 416.], device='cuda:0'), tensor([  0.,  37.,  40.,  51.,  60., 103.,  61.], device='cuda:0'), tensor([ 36.,  82.,  84.,  88., 101., 119., 122., 126., 144., 157., 179., 184.,\n",
      "        197., 209., 215., 216., 231., 248.,  12.,  22.], device='cuda:0'), tensor([ 1., 13., 21., 40., 47., 52.], device='cuda:0'), tensor([205.,   3.,  54.,  83.,  98., 109., 128., 136., 139., 145., 151., 177.,\n",
      "        260., 264., 277., 434., 540., 553., 618., 624.,   4.,  55.,  84., 110.,\n",
      "        137., 140., 146., 152., 178., 261., 435., 541., 619., 625.,   5.,  56.,\n",
      "         86., 111., 117., 138., 142., 148., 154., 163., 179., 199., 224., 436.,\n",
      "        542., 620., 626.], device='cuda:0'), tensor([   0.,    9.,   85.,  177.,  188.,  222.,  584.,  589.,  868.,    2.,\n",
      "          11.,   87.,  179.,  224.,  271.,  310.,  329.,  350.,  407.,  481.,\n",
      "         490.,  592.,  706.,  812.,  971.,  978.,  998., 1009., 1177.,    3.,\n",
      "          12.,   88.,  180.,  225.,  272.,  311.,  972.,  979., 1178., 1188.,\n",
      "        1201., 1310., 1806.,    5.,   14.,   90.,  182.,  227.,  274.,  313.,\n",
      "         328.,  374.,  413.,  423.,  562.,  564.,  566.,  691.,  905.,  974.,\n",
      "         981., 1160., 1163.], device='cuda:0'), tensor([  1.,  47.,  52.,  54.,  68.,  90.,  94., 189., 197., 204., 211., 383.,\n",
      "        387., 437., 442., 444.,   2.,  55.,  69.,  91.,  99., 105., 289., 303.,\n",
      "        336., 344., 346., 361., 363., 384., 388., 445., 471.,   3.,  32.,  56.,\n",
      "         70.,  92., 106., 112., 241., 290., 304., 337., 345., 347., 362., 364.,\n",
      "        385., 389., 446., 458.], device='cuda:0'), tensor([1.0000e+00, 4.9000e+01, 6.9000e+01, 5.3400e+02, 5.6700e+02, 5.7400e+02,\n",
      "        9.6500e+02, 9.8200e+02, 1.0650e+03, 1.1720e+03, 1.1890e+03, 1.1940e+03,\n",
      "        1.2060e+03, 1.2440e+03, 1.2690e+03, 1.3220e+03, 1.3290e+03, 2.0000e+00,\n",
      "        2.5000e+01, 3.7000e+01, 4.7000e+01, 5.5000e+01, 7.0000e+01, 9.4000e+01,\n",
      "        1.2200e+02, 1.8500e+02, 1.9100e+02, 2.2000e+02, 2.3300e+02, 2.6100e+02,\n",
      "        3.9500e+02, 4.1300e+02, 4.4000e+02, 5.0200e+02, 6.8600e+02, 8.3200e+02,\n",
      "        9.6600e+02, 3.0000e+00, 2.6000e+01, 3.4000e+01, 3.8000e+01, 4.8000e+01,\n",
      "        5.6000e+01, 7.1000e+01, 9.5000e+01, 1.2300e+02, 1.8600e+02, 1.9200e+02,\n",
      "        2.2100e+02, 2.3400e+02, 2.4200e+02, 2.5700e+02, 2.6200e+02, 3.9600e+02,\n",
      "        4.0500e+02, 4.1400e+02, 4.2500e+02], device='cuda:0'), tensor([   4.,    8.,   13.,   21.,   33.,   39.,   41.,   61.,   66.,   67.,\n",
      "          73.,   78.,   88.,  106.,  181.,  201.,  244.,  274.,  291.,  313.,\n",
      "         646., 1106., 1123., 1547.], device='cuda:0'), tensor([1.0000e+00, 2.0000e+00, 1.1000e+01, 7.8000e+01, 1.3500e+02, 1.7300e+02,\n",
      "        2.0800e+02, 2.1800e+02, 2.9300e+02, 2.9500e+02, 4.8700e+02, 5.0800e+02,\n",
      "        5.5800e+02, 5.7200e+02, 5.8000e+02, 6.3800e+02, 6.8100e+02, 7.5300e+02,\n",
      "        7.9800e+02, 8.5300e+02, 1.5630e+03, 1.9910e+03], device='cuda:0'), tensor([  1.,  51.,  52.,  53.,  68.,  71.,  97., 111., 121., 157., 169., 307.,\n",
      "        314., 335., 379., 380., 381., 388., 390., 643.,   4.,  24.,  40.,  70.,\n",
      "         88., 101., 103., 115., 123., 131., 132., 139., 174., 177., 317., 338.,\n",
      "        396., 399., 409., 456.], device='cuda:0'), tensor([  1.,  19.,  31., 185., 191., 206., 223., 255., 264., 272., 313., 398.,\n",
      "        415., 689., 693., 708., 720., 726., 731., 733.,   3.,   6.,   8.,  12.,\n",
      "         21.,  33.,  42.,  46.,  92.,  94.,  96., 100., 108., 126., 130., 152.,\n",
      "        162., 168., 175., 189.], device='cuda:0'), tensor([  5.,  64.,  98., 106., 148., 155., 171., 175., 187., 200., 214., 227.,\n",
      "        235., 255., 316., 343., 355., 364., 378., 395.], device='cuda:0'), tensor([  7.,  19.,  47.,  77.,  95., 108., 112., 130., 188., 214., 225., 264.,\n",
      "        275., 349., 389., 425., 530., 546., 564., 575.], device='cuda:0'), tensor([242., 265., 274., 277., 282., 298., 325., 342., 373., 390., 418., 510.,\n",
      "        630., 634., 638., 641., 646., 662., 840., 858., 961., 973.],\n",
      "       device='cuda:0'), tensor([], device='cuda:0'), tensor([ 143.,  169.,  841.,  847.,  862.,  934., 1138.,  192.,  205.,  341.,\n",
      "         361.,  394.,  476.,  630.,  644.,  674., 1129., 1190., 1200.,  214.,\n",
      "         219.,  343.,  429.,  215.,  220.,  344.], device='cuda:0'), tensor([ 63., 148.,   0.,   7.,  15.,  98., 131.], device='cuda:0'), tensor([  0.,  47.,  49.,  66.,  72.,  88.,  96., 103., 116., 140., 153., 169.,\n",
      "        186., 199., 211., 214., 228., 245., 272., 287.,  74.,  90., 105., 118.,\n",
      "        142., 155., 171., 188., 201., 216., 274., 289., 304.,   3.,  30.,  75.,\n",
      "         91., 101., 106., 119., 143., 156., 166., 172., 182., 189., 202., 212.,\n",
      "        217., 230., 247., 270., 275.,  76., 107., 120., 144., 157., 173., 190.,\n",
      "        203., 218., 276., 291., 306., 344.], device='cuda:0'), tensor([  3.,  47.,  60., 109., 152., 160., 199., 240., 286., 324., 327., 353.,\n",
      "        425., 468., 512., 570., 589., 645., 663., 679.,   2.,  46., 100., 124.,\n",
      "        135., 159., 181., 201., 214., 221., 268., 288., 395., 471., 492., 504.,\n",
      "        528., 535., 564.,  29.,  55., 101., 104., 113., 125., 134., 140., 150.,\n",
      "        182., 194., 200., 207., 212., 222., 235., 262., 269., 281., 289., 114.,\n",
      "        126., 173., 430., 519., 582., 649., 668., 683., 138., 183., 209., 223.,\n",
      "        270., 290., 321., 398.], device='cuda:0'), tensor([  17.,   27.,   38.,   40.,   42.,   50.,   52.,   63.,   65.,   70.,\n",
      "          77.,  104.,  281.,  293.,  309.,  332.,  354.,  379.,  402.,  467.,\n",
      "           2.,   18.,   51.,   72.,  121.,  241.,  278.,  291.,  314.,  336.,\n",
      "         365.,  394.,  408.,  431.,  434.,  452.,  461.,  472.,  491.,  510.,\n",
      "        1932.], device='cuda:0'), tensor([  19.,   36.,   59.,   87.,   91.,  441.,  483.,  488.,  546.,  563.,\n",
      "         586.,  614.,  618.,  968., 1010., 1015., 1073., 1086., 1105., 1109.,\n",
      "          20.,   37.,   60.,   92.,   98.,  307.,  359.,  388.,  399.,  408.,\n",
      "         474.,  547.,  564.,  587.,  619.,  625.,  834.,  886.,  915.,  926.,\n",
      "        1614.], device='cuda:0'), tensor([115., 132., 157., 116., 133., 147., 158., 162., 112., 120.,   1.,   9.,\n",
      "         16., 108., 213., 224., 228., 229., 231., 234.], device='cuda:0'), tensor([ 118.,  154.,  172., 1343., 1349.,  849., 1485.], device='cuda:0'), tensor([  0.,  52.,  70.,  88.,  93.,  98., 104., 110., 114., 119., 149., 214.,\n",
      "        220., 232., 245., 272., 282., 302., 330., 347.,   2.,  17.,  54.,  72.,\n",
      "         90., 134., 140., 157., 207., 215., 221., 230., 234., 247., 276., 297.,\n",
      "        345., 360., 393., 604.], device='cuda:0'), tensor([ 35.,  40.,  46.,  52.,  63.,  67.,  74.,  80.,  91.,  96., 107., 119.,\n",
      "        125., 130., 140., 165., 173., 180., 190., 194., 141.], device='cuda:0'), tensor([  5.,   9.,  24.,  31.,  38.,  46.,  69.,  74., 102., 120., 137., 141.,\n",
      "        190., 212., 221., 247., 260., 273., 333., 340., 419., 431., 444., 461.,\n",
      "        471., 544., 244.], device='cuda:0'), tensor([  8.,  27.,  29.,  34.,  45.,  55.,  95.,  46.,  62.,  88., 111.],\n",
      "       device='cuda:0'), tensor([316., 334., 340.,  39., 233., 265., 271., 317., 328., 347., 379., 481.],\n",
      "       device='cuda:0'), tensor([0., 5.], device='cuda:0'), tensor([2.3000e+01, 3.2000e+01, 1.7400e+02, 2.1600e+02, 3.4400e+02, 5.0700e+02,\n",
      "        5.2000e+02, 5.9400e+02, 6.2400e+02, 6.3600e+02, 7.7700e+02, 1.0170e+03,\n",
      "        1.0220e+03, 1.0260e+03, 1.0290e+03, 1.0370e+03, 1.0620e+03, 1.0970e+03,\n",
      "        1.1090e+03, 1.1390e+03, 0.0000e+00, 4.0000e+00, 7.0000e+00, 2.0000e+01,\n",
      "        2.4000e+01, 2.8000e+01, 3.4000e+01, 4.0000e+01, 5.3000e+01, 8.4000e+01,\n",
      "        9.6000e+01, 1.1300e+02, 1.1700e+02, 1.2500e+02, 1.3100e+02, 1.3400e+02,\n",
      "        1.7000e+02, 1.7500e+02, 1.7800e+02, 1.9000e+02, 1.0000e+00, 5.0000e+00,\n",
      "        8.0000e+00, 2.1000e+01, 2.5000e+01, 2.9000e+01, 3.5000e+01, 4.1000e+01,\n",
      "        5.4000e+01, 6.0000e+01, 6.2000e+01, 6.5000e+01, 7.5000e+01, 8.5000e+01,\n",
      "        9.7000e+01, 1.1400e+02, 1.1800e+02, 1.2600e+02, 1.3200e+02, 1.3500e+02],\n",
      "       device='cuda:0'), tensor([1.7600e+02, 2.0500e+02, 2.3500e+02, 2.5300e+02, 3.1400e+02, 3.4800e+02,\n",
      "        3.7100e+02, 3.7500e+02, 5.9100e+02, 6.1000e+02, 6.4800e+02, 7.8400e+02,\n",
      "        7.9600e+02, 9.1300e+02, 1.0790e+03, 1.1210e+03, 1.1670e+03, 1.1970e+03,\n",
      "        1.2790e+03, 1.0000e+00, 7.8000e+01, 1.2300e+02, 1.3400e+02, 1.4400e+02,\n",
      "        1.5100e+02, 1.5800e+02, 1.7700e+02, 1.8400e+02, 2.0000e+02, 2.0600e+02,\n",
      "        2.2900e+02, 2.3600e+02, 2.5400e+02, 2.8000e+02, 3.1500e+02, 3.3700e+02,\n",
      "        3.4900e+02, 3.7200e+02, 3.9300e+02, 2.0000e+00, 7.9000e+01, 1.1000e+02,\n",
      "        1.2400e+02, 1.3500e+02, 1.4500e+02, 1.5200e+02, 1.5900e+02, 1.7800e+02,\n",
      "        1.8500e+02, 2.0100e+02, 2.0700e+02, 2.1100e+02, 2.3000e+02, 2.3700e+02,\n",
      "        2.5500e+02, 2.6200e+02, 2.8100e+02, 3.1600e+02, 3.3800e+02],\n",
      "       device='cuda:0'), tensor([  4.,  14.,  19.,  71.,  75., 106., 135., 212., 242., 245., 248., 253.,\n",
      "        258., 301., 329., 344., 345., 366., 430., 440.,  69., 441., 444., 448.,\n",
      "        454., 461., 467., 472., 478., 483., 494., 836.], device='cuda:0'), tensor([3., 5.], device='cuda:0'), tensor([ 20.,  33.,  43.,  55.,  70., 100., 103., 112., 122., 146., 154., 180.,\n",
      "        188., 201., 218., 249., 266., 271., 336., 453., 129., 387., 411., 422.,\n",
      "        426., 447., 456., 459., 654.], device='cuda:0'), tensor([ 46., 211., 216., 251., 261., 266., 325., 330., 334., 338., 343., 350.,\n",
      "        372., 378., 389., 408., 413., 418., 460., 465., 218., 268., 345., 352.,\n",
      "        359., 366., 374., 380., 430., 435., 449., 454., 500., 506.],\n",
      "       device='cuda:0'), tensor([  2.,  14.,  17., 136., 182.,   1.,  40.,  47.,  61., 146., 174.],\n",
      "       device='cuda:0'), tensor([  4.,  33.,  47.,  67., 115.,   5.,  34.,  48.,  68.], device='cuda:0'), tensor([ 60., 161.,  35.,  54., 235.,   1.,   5.,  16.,  18.,  21.,  24.,  34.,\n",
      "         42.,  98., 105., 234., 239., 246., 248.,  28., 102., 123., 228.],\n",
      "       device='cuda:0'), tensor([  52.,  700.,  739.,  751.,  756.,  760.,  767.,  770.,  775.,  777.,\n",
      "         781.,  783.,  786.,  790.,  800., 1040., 1388.,    3.,    7.,  109.,\n",
      "         118.,  125.,  132.,  152.,  154.,  157.,  162.,  166.,  177.,  179.,\n",
      "         209.,  211.,  228.,  230.,  261.,  263.,  279.,   18.,   59.,   63.,\n",
      "          65.,   69.,   73.,   78.,   81.,  284.,  859.,  932.,  987., 1057.,\n",
      "        1095., 1107., 1146., 1218.,  773.], device='cuda:0'), tensor([ 443.,  474.,  641.,  659.,  713.,  744.,  764.,  807.,  849., 1317.,\n",
      "         387.,  471.,  484.,  567.,  585.,  648.,  699.,  711.,  742.,  790.,\n",
      "         874.,  915.,  996., 1496., 1528., 1641., 1819., 1826., 1861.,   12.,\n",
      "          28.,   45.,  166.,  228.,  239.,  255.,  278.,  392.,  434.,  462.,\n",
      "         534.,  542.,  552.,  646.,  663.,  666.,  679.,  685.,  689.],\n",
      "       device='cuda:0'), tensor([], device='cuda:0'), tensor([ 64., 135., 237., 249., 278., 390., 641., 874.,  65., 238., 250., 279.],\n",
      "       device='cuda:0'), tensor([ 62.,  67.,  75.,  83.,  95.,  96., 100., 119., 121., 140., 150., 189.,\n",
      "        244., 250., 255., 264., 295., 349., 361., 384.], device='cuda:0'), tensor([14.,  4., 11.,  3., 10., 21., 26.], device='cuda:0'), tensor([0., 3.], device='cuda:0'), tensor([  37.,   49.,   71.,   81.,  116.,  154.,  179.,  189.,  200.,  223.,\n",
      "         240.,  262.,  268.,  314.,  427.,  455.,  472.,  476.,  504.,  505.,\n",
      "         469.,  474.,  484.,  490.,  923.,  925.,  929.,  931.,  936.,  938.,\n",
      "         961.,  966.,  991.,  993., 1008., 1010., 1030., 1032., 1077., 1079.],\n",
      "       device='cuda:0'), tensor([  27.,   47.,  335.,  355.,  383.,  400.,  406.,  412.,  421.,  436.,\n",
      "         446.,  452.,  704.,  711.,  731.,  758.,  803.,  829.,  889.,  898.,\n",
      "         201.,  408.,  588.,  765.,  985., 1184., 1264.], device='cuda:0'), tensor([  91.,  147.,  232.,  234.,  250.,  253.,  268.,  280.,  363.,  372.,\n",
      "         378.,  411.,  419.,  431.,  467.,  495.,  589.,  610.,  636.,  681.,\n",
      "          97.,  464.,  469.,  476.,  485.,  492.,  605., 1668., 1673., 1680.,\n",
      "        1689., 1696., 1789., 1892.], device='cuda:0'), tensor([ 17.,  25.,  37.,  73., 101.,  70.,  75.,  82.,  91.,  98.],\n",
      "       device='cuda:0'), tensor([515., 518., 525., 516., 519., 526.,  17.,  19.,  30.,  80., 517., 634.,\n",
      "        645., 657., 688., 729., 738., 747., 757., 758.], device='cuda:0'), tensor([115.,  38.,  45.,  49.,  50.,  52.,  53.,  55.,  57.,  65.,  68.,  70.,\n",
      "         72.,  80.,  84.,  91.,  93.,  95., 101., 108., 110.], device='cuda:0'), tensor([510., 569., 576., 582., 600.], device='cuda:0'), tensor([398.], device='cuda:0'), tensor([   7.,   31.,  196.,  212.,  226.,  302.,  354.,  565.,  587.,  784.,\n",
      "        1226., 1241., 1356., 1549., 1563., 1715., 1857.,    8.,   32.,  197.,\n",
      "         213.,  215.,  227.,  303.,  355.,  356.,  566.,  582.,  588.,  788.,\n",
      "         836., 1227., 1242., 1243., 1357., 1360., 1550.,    5.,   21.,   29.,\n",
      "         153.,  172.,  178.,  187.,  194.,  224.,  241.,  280.,  288.,  348.,\n",
      "         352.,  366.,  382.,  387.,  412.,  420.,  430.,  645., 1103., 1417.,\n",
      "         158.,  162.,  165.,  168.,  283.,  311.,  337.,  630., 1090., 1095.,\n",
      "        1115., 1137., 1151., 1159., 1175., 1512., 1521., 1532., 1539., 1548.],\n",
      "       device='cuda:0'), tensor([  27.,   51.,   78.,   82.,   94.,  136.,  393.,  408.,  452.,  826.,\n",
      "         999., 1157., 1388., 1604., 1640., 1653., 1671., 1703., 1716., 1724.,\n",
      "          28.,   52.,   79.,   83.,   95.,  137.,  394.,  409.,  453.,  827.,\n",
      "        1000., 1158., 1389., 1605., 1641., 1654., 1672., 1704., 1717., 1725.,\n",
      "         519.,  776., 1068., 1332., 1355., 1391., 1595., 1612., 1632., 1684.,\n",
      "        1691., 1751., 1913., 1990., 1044.,  517.,  524.,  735.,  787.,  818.,\n",
      "         898.,  906.,  910.,  913.,  953.,  979.,  997., 1003., 1637., 1664.,\n",
      "        1721., 1768., 1777., 1787., 1800.], device='cuda:0'), tensor([103., 109.,  18.,  94., 101., 120., 135.], device='cuda:0'), tensor([10., 11.], device='cuda:0'), tensor([  31.,  330.,  241.,  259.,  295.,  301.,  311.,  344.,  349.,  423.,\n",
      "         434.,  498.,  603.,  653.,  720.,  854.,  946., 1115., 1251., 1360.,\n",
      "          34.,   69.,  172.,  206.,  242.,  260.,  296.,  345.,  356.,  380.,\n",
      "         464.,  499.,  508.,  536.,  548.,  568.,  576.,  585.,  604.,  631.],\n",
      "       device='cuda:0'), tensor([  2.,   5., 110., 117.,   1.,   4.,   8.,  31.,  47.,  67.,  76.,  92.,\n",
      "        112., 114.], device='cuda:0'), tensor([ 11.,  28.,  33.,  39.,  64.,  85.,  93., 107., 112., 122., 129., 102.,\n",
      "        125.], device='cuda:0'), tensor([ 841.,  847.,  883.,  897.,  914.,  812.,  907., 1553.,  645.,  843.,\n",
      "         996., 1207., 1809., 1897., 1975.], device='cuda:0'), tensor([  6.,   8.,  14.,  25.,  30.,  44.,  51.,  80.,  96., 127., 148., 176.,\n",
      "        196., 218., 227., 241., 253., 321., 367., 410.,   7.,   9.,  26.,  31.,\n",
      "         97., 104., 107., 128., 149., 177., 197., 208., 219., 228., 242.,  24.,\n",
      "        129., 150., 178., 180., 198., 229., 243., 318., 328.], device='cuda:0'), tensor([  0.,  76.,  87.,  13.,  44.,  50.,  55.,  84.,  97., 101.,   1.,  45.,\n",
      "         77.,  85.,  88.,  98., 102.,   4.,  46.,  80.,  86.,  91.,  96., 100.,\n",
      "        106., 110., 111.], device='cuda:0'), tensor([ 11.,  33.,  78., 185., 187., 192., 214., 221., 223., 232., 255., 286.,\n",
      "        291., 313., 325., 328., 339., 347., 354., 359.], device='cuda:0'), tensor([  5.,   6.,  32.,  33.,  55.,  56.,  84.,  85., 104., 107., 129., 133.,\n",
      "        191., 198., 199., 212., 213., 217., 218., 236.], device='cuda:0'), tensor([ 1.,  5., 10., 17., 23., 37.,  2.,  6., 11., 18., 24., 38.,  4.,  3.,\n",
      "         7., 12., 30., 34.], device='cuda:0'), tensor([  0.,   4.,   7.,  22.,  28.,  36.,  77.,  95.,   1.,   5.,   8.,  23.,\n",
      "         29.,  37.,  52.,  78.,  96.,   3.,  10.,  19.,  31.,  42.,  58.,  64.,\n",
      "         72., 102., 110.], device='cuda:0'), tensor([  6.,  72.,  84.,  86., 100., 109., 123., 166., 168., 172., 174., 181.,\n",
      "        183., 187., 189., 196., 198., 202., 204., 245.,   7.,  73.,  85., 101.,\n",
      "        167., 173., 182., 188., 197., 203., 246., 274., 354., 381., 554., 571.,\n",
      "        674., 682., 690., 738., 964.], device='cuda:0'), tensor([  4.,  15.,  74.,  93.,   5.,  16.,  75.,  94., 128.], device='cuda:0'), tensor([   5.,   75.,   87.,  138.,  219.,  326.,  372.,  422.,  796., 1304.,\n",
      "           6.,   37.,   43.,   69.,   80.,   85.,  114.,  141.,  152.,  164.,\n",
      "         197.,  214.,  252.,  262.,  322.,  345.,  367.,  385.,  501.,  515.],\n",
      "       device='cuda:0'), tensor([  11.,   20.,   31.,   75.,  114.,  120.,  139.,  154.,  180.,  186.,\n",
      "         195.,  201.,  255.,  274.,  303.,  343.,  358.,  380.,  402.,  427.,\n",
      "          42.,  172.,  919.,   56.,  214.,  225.,  228.,  231.,  245.,  262.,\n",
      "         611.,  650.,  707.,  712., 1085.], device='cuda:0'), tensor([  16.,   35.,   46.,  104.,  126.,  178.,  179.,  190.,  208.,  217.,\n",
      "         223.,  278.,  344.,  371.,  390.,  409.,  464.,  472.,  514.,  581.,\n",
      "        1014., 1661., 1697., 1799., 1800.], device='cuda:0'), tensor([ 108.,  113.,  118.,  138.,  162.,  170.,  175.,  220.,  229.,  260.,\n",
      "         270.,  297.,  327.,  341.,  361.,  396.,  416.,  632.,  638.,  677.,\n",
      "         350.,  942., 1000.], device='cuda:0'), tensor([ 13.,  64.,  73., 149., 151., 170., 228., 277.,   0.,   9.,  49.,  66.,\n",
      "         84.,  97., 195., 229., 243., 347.], device='cuda:0'), tensor([  0.,   5.,  23.,  44.,  61.,  67.,  83., 125., 147., 207., 239., 249.,\n",
      "        271., 297., 330., 337., 354., 374., 406., 428., 157., 180., 186., 189.,\n",
      "        222., 277., 292., 350., 439., 459., 467., 470., 495., 576., 581., 614.,\n",
      "        642., 880., 895., 916.], device='cuda:0'), tensor([ 43.,  62.,  77.,  89.,  90., 110., 128., 142., 157., 169., 170., 191.,\n",
      "        209., 232., 250., 265., 270., 304., 319., 324., 274., 348.,   1.,  26.,\n",
      "         75., 155., 243., 246., 263., 275., 279., 284., 290., 294., 317., 349.,\n",
      "        353., 358., 364., 368., 391., 475.,   2.,  12.,  27.,  57.,  72., 152.,\n",
      "        244., 247., 260., 276., 280., 285., 295., 302., 314., 350., 354., 359.,\n",
      "        369., 376.,  32., 248., 281., 286., 292., 297., 355., 360., 366., 371.,\n",
      "        411., 495., 347., 362.], device='cuda:0'), tensor([   0.,   85.,  116.,  138.,  166.,  168.,  187.,  210.,  214.,  224.,\n",
      "         226.,  229.,  234.,  239.,  264.,  272.,  327.,  335.,  338.,  344.,\n",
      "          27.,  280.,  301.,  304.,  306.,  308.,  312.,  315.,  434.,  447.,\n",
      "         468.,  490.,  498.,  527.,  564.,  596.,  607.,  612.,  617.,  623.,\n",
      "          24.,  311.,  491.,  516.,  532.,  618., 1331., 1439., 1460., 1529.,\n",
      "        1545., 1771., 1928.,  287.,  507., 1288., 1390., 1400., 1429., 1431.,\n",
      "        1440., 1462., 1467., 1470., 1476., 1530., 1546., 1712., 1718., 1816.,\n",
      "        1929., 1936., 1939.,  858.,  874., 1499., 1503., 1615., 1619., 1625.,\n",
      "        1639.], device='cuda:0'), tensor([ 13.,  25.,  53.,  63.,  72.,  77.,  89., 121., 137., 141., 145., 152.,\n",
      "        180., 185., 195.], device='cuda:0'), tensor([113., 118., 123., 125., 127., 129., 131., 133., 135., 137., 139., 141.,\n",
      "        143., 145., 147., 149., 151., 153., 155., 158.], device='cuda:0'), tensor([342., 724.,   5.,   8.,  26.,  91., 113., 200., 271., 286., 312., 317.,\n",
      "        327., 337., 360., 422., 426., 435., 441., 453., 463., 474.,   6.,   9.,\n",
      "         28., 103., 127., 201., 272., 287., 313., 318., 328., 338., 361., 423.,\n",
      "        427., 436., 442., 454., 464., 475.,   7.,  10.,  29.,  56.,  66.,  81.,\n",
      "         99., 110., 142., 170., 172., 175., 202., 211., 274., 288., 314., 319.,\n",
      "        329., 339.], device='cuda:0'), tensor([ 84.,   1.,   7.,  18.,  27.,  32.,  43.,  55.,  68., 107., 113., 126.,\n",
      "        138.,   2.,   8.,  20.,  28.,  33.,  44.,  56.,  70.,  79., 108., 114.,\n",
      "        127., 139.,   9.,  45.,  49.,  81.], device='cuda:0'), tensor([  4.,   9.,  13.,  15.,  31.,  44.,  76.,  85.,  94., 108., 125., 135.,\n",
      "        139., 149.], device='cuda:0'), tensor([  1.,  13.,  21.,  52.,  85., 112.], device='cuda:0'), tensor([  35.,   40.,  102.,  929.,  940., 1216., 1422., 1525., 1532., 1543.,\n",
      "        1574., 1683., 1697., 1705.,   41.,  103.,  113.,  564.,  941., 1217.,\n",
      "        1526., 1533., 1544., 1575., 1592., 1684., 1698., 1706., 1751., 1799.,\n",
      "        1813., 1857., 1879., 1923.,  488.,  503.,  513., 1791.,  231.,  358.,\n",
      "         489.,  504.,  514.,  895.,  903.,  912.,  925.,  952.,  970.,  982.,\n",
      "        1182., 1231., 1427., 1600., 1733., 1738., 1748., 1792.],\n",
      "       device='cuda:0'), tensor([  0.,  11.,  24.,  35.,  53.,  78.,  90.,  96., 130., 304., 310., 353.,\n",
      "        361., 379., 413.,   1.,  12.,  25.,  36.,  54.,  91.,  97., 110., 131.,\n",
      "        305., 311., 362., 380., 409., 414., 237., 280., 291.], device='cuda:0'), tensor([  2.,  22.,  39.,  43.,  46., 225., 236., 240., 307., 333., 365., 385.,\n",
      "        481., 528., 534., 561., 635., 795.,   3.,  23.,  40.,  44.,  47.,  84.,\n",
      "        188., 237., 241., 308., 366., 386., 482., 529., 535., 562., 636., 796.,\n",
      "        396., 415., 489., 533., 619.], device='cuda:0'), tensor([1834., 1838., 1842., 1847., 1862., 1865., 1868., 1949., 1973., 1835.,\n",
      "        1950.,  304., 1822.], device='cuda:0'), tensor([ 33.,  37.,  40.,  42.,  89., 109., 134., 140., 143., 183.,  30., 152.,\n",
      "        159., 232., 270., 286., 313.], device='cuda:0'), tensor([ 22., 119.,   7.,  79.,  86.,  93., 128., 168.,   8.,  17.,  20., 129.],\n",
      "       device='cuda:0'), tensor([  4.,  15., 121., 125., 132., 138.,   2.,  29., 128.,  18.,  35.,  50.,\n",
      "         59.,  76.,  93., 129.,   0.,  47.,  48.,  49.,  57.,  89.,  92.,  95.,\n",
      "        120., 140.], device='cuda:0'), tensor([   4.,   15.,   71.,  238.,  251.,  256.,  315.,  549.,  563.,  570.,\n",
      "         573.,  611.,  674.,  708.,  711.,  730.,  740.,  760.,  767.,  770.,\n",
      "           2.,   29.,   52.,   65.,   68.,   76.,   81.,   84.,   94.,   96.,\n",
      "         106.,  117.,  133.,  135.,  141.,  146.,  149.,  154.,  158.,  176.,\n",
      "          18.,   35.,  108.,  121.,  128.,  150.,  161.,  167.,  173.,  180.,\n",
      "         207.,  208.,  213.,  219.,  249.,  331.,  341.,  413.,  436.,  451.,\n",
      "         151.,  452.,  454.,  493.,  859.,  863., 1963., 1969., 1994.],\n",
      "       device='cuda:0'), tensor([638., 658., 640.], device='cuda:0'), tensor([  4.,   6.,  43.,  45.,  47.,  59.,  64.,  70.,  78.,  86.,  88., 106.],\n",
      "       device='cuda:0'), tensor([ 36.,  61.,  64.,  79.,  23., 209.,   2.,   3.,   9.,  15.,  18.,  19.,\n",
      "         24.,  25.,  29.,  37.,  43.,  62.,  63.,  80., 102., 106., 158., 190.,\n",
      "        210., 218.], device='cuda:0'), tensor([13., 14.], device='cuda:0'), tensor([254., 262., 282., 288., 365., 369., 384., 566., 568., 122., 143., 160.,\n",
      "        174., 176., 180., 331., 363., 368., 382., 389., 401., 415., 498., 515.,\n",
      "        521., 577.], device='cuda:0'), tensor([35.,  2.,  5., 24., 33., 40.], device='cuda:0'), tensor([ 118.,  123.,  158.,  178.,  219.,  230.,  290.,  309.,  380.,  383.,\n",
      "         385.,  390.,  394.,  409.,  417.,  433.,  452.,  456.,  461.,  467.,\n",
      "         122.,  218.,  997.,  102.,  105.,  112.,  128.,  142.,  162.,  169.,\n",
      "         249.,  406.,  678., 1003., 1030.], device='cuda:0'), tensor([3., 7.], device='cuda:0'), tensor([ 18., 755., 757.,  60., 111., 883., 895., 901., 961.], device='cuda:0'), tensor([  42.,   79.,   82.,   92.,  162.,  646., 1171., 1229., 1248., 1314.,\n",
      "        1326., 1340.,  128.,  164.,  178.,  197.,  205.,  214.,  707.,  757.,\n",
      "         776.,  979., 1065., 1183., 1199., 1330.], device='cuda:0'), tensor([  23.,   42.,  133.,  175.,  179.,  290.,  300.,  303.,  418.,  432.,\n",
      "         441.,  450.,  493.,  619.,  624.,  851.,  854.,  966., 1023., 1026.,\n",
      "          13.,   24.,   43.,  101.,  134.,  176.,  180.,  219.,  230.,  237.,\n",
      "         240.,  255.,  274.,  298.,  301.,  304.,  317.,  378.,  387.,  448.,\n",
      "         320.,  394.,  417.,  466.,  498.,  549.,  646.,  664.,  699.,  741.,\n",
      "         770.,  788.,  834.,  843.,  849.,  890.,  946.,  948.,  956., 1033.,\n",
      "          41.,   57.,  156.,  186.,  313.,  364.,  374.,  437.,  499.,  528.,\n",
      "         548.,  703.,  709.,  742.,  807.,  869.,  885.,  893., 1047., 1202.],\n",
      "       device='cuda:0'), tensor([   5.,  245.,  298.,  394.,  411.,  417.,  583.,  866., 1245., 1249.,\n",
      "        1266., 1270., 1423., 1637., 1659., 1774., 1870.,    6.,  230.,  246.,\n",
      "         305.,  395.,  412.,  573.,  584.,  594.,  625.,  666.,  867.,  918.,\n",
      "        1246., 1250., 1267., 1271., 1360., 1424., 1591.,   17.,  115.,  332.,\n",
      "         696.,  714.,  747.,  757.,  816.,  887.,  890.,  892.,  919.,  923.,\n",
      "         926., 1133., 1139., 1140., 1149., 1151., 1323.,  391.,  427.,  435.,\n",
      "         438.,  440.,  444.,  457.,  464.,  483.,  519.,  525.,  580.,  604.,\n",
      "         610.,  680.,  711.,  712.,  722.,  791.,  796.], device='cuda:0'), tensor([  25.,   77.,  129.,  140.,  235.,  505.,  918., 1309., 1323., 1500.,\n",
      "        1522.,   26.,   78.,  130.,  141.,  236.,  506.,  919., 1310., 1324.,\n",
      "        1501., 1523.,   27.,   79.,  131.,  142.,  237.,  507.,  920., 1253.,\n",
      "        1311., 1325., 1502., 1524.], device='cuda:0'), tensor([  64.,   72.,  511.,  898.,  962., 1248.,    6.,  110.,  129.,  295.,\n",
      "         304.,  324.,  382.,  430.,  512.,  899.,  928.,  963., 1165., 1260.,\n",
      "        1309., 1328., 1433., 1442., 1454.,    8.,  130.,  141.,  148.,  159.,\n",
      "         167.,  239.,  286.,  296.,  376.,  383.,  431.,  473.,  513.,  672.,\n",
      "         716.,  786.,  827.,  866.,  876.], device='cuda:0'), tensor([257.,   7.,  10.,  19.,  44.,  45.,  50.,  67.,  79.,  88., 104., 145.,\n",
      "        146., 147., 153., 154., 189., 269., 305.,   0.,  48.,  84., 293.,   1.,\n",
      "         40.,  49.,  75.,  80.,  85., 236., 241., 285., 294.], device='cuda:0'), tensor([ 413.,  412.,  628.,    5.,    7.,   24.,   34.,   49.,   77.,  181.,\n",
      "         198.,  200.,  203.,  210.,  222.,  231.,  247.,  261.,  266.,  286.,\n",
      "         312.,  380.,  406.,  952., 1046., 1089., 1482.,  171.,  188.,  193.,\n",
      "         225.,  269.,  275.,  314.,  376.,  409.,  426.,  569.,  572.,  576.,\n",
      "         620.,  770.,  798.,  803.,  841.,  953., 1007.], device='cuda:0'), tensor([ 65.,  86., 128., 285., 305., 685.,  12.,  81., 149., 178., 202., 249.,\n",
      "        259., 292., 339., 750.,  13.,  82., 150., 203., 250., 260., 269., 293.,\n",
      "        751.,  14., 251., 155., 743.], device='cuda:0'), tensor([   7.,   99.,  140.,  171.,  243.,  311.,  353.,  379.,  408.,  559.,\n",
      "         648.,  701.,  748.,  752.,  779., 1004., 1015., 1052., 1079., 1258.,\n",
      "           8.,  100.,  141.,  172.,  244.,  312.,  325.,  354.,  380.,  409.,\n",
      "         649.,  702.,  749.,  753.,  780.,  914.,  920.,  944., 1005., 1016.,\n",
      "         101.,  173.,  355.,  381.,  410.,  650.,  700.,  750.,  754., 1006.,\n",
      "        1017., 1291., 1541., 1792.], device='cuda:0'), tensor([ 88.,  92., 174., 196., 234., 271., 316.,  57.,  84.,  90., 100., 122.,\n",
      "        173., 215., 260., 478.], device='cuda:0'), tensor([163.,   2.,  22.,  81.,  91., 102., 116., 124., 141., 189.],\n",
      "       device='cuda:0'), tensor([ 768.,    5.,   12.,   16.,   24.,   31.,   41.,   60.,   65.,   79.,\n",
      "         120.,  135.,  159.,  175.,  210.,  248.,  609.,  643.,  647.,  713.,\n",
      "         787.,  141.,  218.,  254.,  834.,  945.,  967., 1000., 1057., 1107.,\n",
      "         142.,  219.,  255.,  835.,  946.,  968., 1001., 1058., 1108.],\n",
      "       device='cuda:0'), tensor([ 523.,   15.,  171.,  172.,  227.,  336.,  367.,  539.,  570.,  571.,\n",
      "         589.,  590.,  718.,  758.,  852.,  853., 1163., 1437., 1448., 1449.,\n",
      "        1530.,  114., 1793.,  115., 1794.,   17.,   23.,   37.,   43.,   53.,\n",
      "          61.,   89.,   91.,  116.,  124.,  165.,  187.,  205.,  211.,  219.,\n",
      "         244.,  250.,  266.,  274.,  318.], device='cuda:0'), tensor([  4.,  88., 122.,   1.,   7.,  11.,  19.,  22.,  49.,  68.],\n",
      "       device='cuda:0'), tensor([39., 68., 74., 83., 37., 65., 72., 81.], device='cuda:0'), tensor([  3.,  24.,  60., 100., 148., 155.,   4.,  25.,  61.,  87., 101., 116.,\n",
      "        123.], device='cuda:0'), tensor([36., 72., 79., 82., 86., 77., 83.], device='cuda:0'), tensor([ 1., 24., 38.,  2., 25., 39., 40.], device='cuda:0'), tensor([ 0., 12., 18., 38., 79.,  1., 13., 19., 39., 80.], device='cuda:0'), tensor([  1.,  50.,  55.,  56.,  67.,  73.,  90., 102., 118., 131., 148., 153.,\n",
      "        171., 224., 235., 242., 257., 261., 306., 365.,  76.,  91., 101., 175.,\n",
      "        191.], device='cuda:0'), tensor([  6.,  22.,  34.,  48.,  51.,  70.,  88.,  94., 110., 141., 153., 187.,\n",
      "        190., 200., 211., 227., 240., 252., 260., 268., 301., 195.],\n",
      "       device='cuda:0'), tensor([ 152.,  229.,  258.,  289.,  299.,  312.,  363.,  418.,  434.,  514.,\n",
      "         560.,  612.,  624.,  626.,  711.,  718.,  805., 1544., 1570., 1590.,\n",
      "         593.,  602.,  636.,  754.,  763.,  766.,  974., 1008., 1035., 1123.,\n",
      "        1132., 1158., 1388., 1389., 1407., 1623., 1947.,   49.,   79.,   93.,\n",
      "         100.,  110.,  114.,  128.,  139.,  146.,  154.,  163.,  177.,  180.,\n",
      "         186.,  191.,  199.,  206.,  223.,  230.,  238.,  320.,  373.,  394.,\n",
      "         401.,  503.,  529.,  565.,  572.,  580.,  586.,  595.,  650.,  673.,\n",
      "         675.,  698.,  708.,  721.,  761.,  810.,  813.], device='cuda:0'), tensor([ 58.,  64.,  97., 107., 191., 206.,   3.,   5.,  14.,  18.,  55.,  57.,\n",
      "         61.,  69.,  74.,  83.,  87., 113., 126., 138., 144., 152., 157., 163.,\n",
      "        175., 179.,  28.,  40.,  90.,  96., 173., 190., 202., 205., 208., 215.,\n",
      "        219.], device='cuda:0'), tensor([ 17.,   0.,   2.,   3.,  21.,  24.,  25.,  33.,  37.,  44.,  49.,  51.,\n",
      "         57.,  60.,  66.,  73.,  80.,  90.,  92., 107., 113.], device='cuda:0'), tensor([0., 8.], device='cuda:0'), tensor([  11.,   26.,   43.,   73.,   84.,   88.,   94.,  109.,  140.,  152.,\n",
      "         156.,  178.,  376.,  390.,  411.,  442.,  448.,  461.,  521.,  532.,\n",
      "          72.,   97.,  158.,  253.,  325.,  400.,  427.,  482.,  613.,  860.,\n",
      "         974., 1016., 1028., 1085., 1156., 1209., 1251.,   13.,   28.,   46.,\n",
      "          75.,   93.,  146.,  159.,  201.,  203.,  205.,  213.,  220.,  236.,\n",
      "         267.,  268.,  276.,  281.,  286.,  295.,  298.], device='cuda:0'), tensor([1326.,  224.,  287.,  437.,  490.,  491.,  499.,  512.,  552.,  592.,\n",
      "         659., 1373., 1393., 1464., 1479., 1498., 1533., 1589., 1714., 1747.,\n",
      "        1856.,    9.,   35.,   57.,   87.,   90.,  102.,  103.,  109.,  124.,\n",
      "         140.,  147.,  185.,  216.,  223.,  232.,  235.,  249.,  284.,  286.,\n",
      "         303.], device='cuda:0'), tensor([ 1.,  4.,  2.,  5., 52.], device='cuda:0'), tensor([ 1.,  4.,  7., 10., 13., 16., 19., 24., 27., 30., 33., 37., 41., 45.,\n",
      "        48., 52., 56., 59., 63., 67.,  6.], device='cuda:0'), tensor([   5.,   28.,   33.,  167.,  273.,  332.,  379.,  443.,  667.,  684.,\n",
      "         694.,  720., 1253., 1395., 1459., 1464., 1469., 1616., 1625., 1633.,\n",
      "          29.,   34.,  168.,  274.,  333.,  380.,  444.,  601.,  652.,  668.,\n",
      "         673.,  721.,  773., 1254., 1396., 1460., 1465., 1470., 1617., 1626.,\n",
      "           0.,    9.,   13.,   16.,   36.,   50.,   61.,  125.,  232.,  690.,\n",
      "         698.,  702., 1265., 1272., 1309., 1317., 1318., 1343., 1357., 1375.],\n",
      "       device='cuda:0'), tensor([ 18.,  53.,  58.,  71., 202., 222., 228., 299., 365., 400., 405., 418.,\n",
      "        549., 569., 575., 646., 712., 747., 752., 765.], device='cuda:0'), tensor([ 51., 221., 254., 275., 318., 322., 333., 405., 413., 455., 501., 505.,\n",
      "        555., 577., 595.,  52., 222., 255., 285., 343., 363., 429., 456., 481.,\n",
      "        525., 539., 543., 552., 570., 578., 603., 223., 256., 457., 579.,  55.,\n",
      "        162., 224., 257., 458., 580.,  39.,  44.,  56., 225., 258., 359., 388.,\n",
      "        403., 459., 581.,  51., 221., 254., 275., 318., 322., 333., 405., 413.,\n",
      "        455., 501., 505., 555., 577., 595.,  52., 222., 255., 285., 343., 363.,\n",
      "        429., 456., 481., 525., 539., 543., 552., 570., 578., 603.,  39.,  44.,\n",
      "         56., 225., 258., 359., 388., 403., 459., 581.], device='cuda:0'), tensor([ 88., 115., 141., 147., 199., 206., 262., 285.,  45.,  90.,  97., 121.,\n",
      "        124., 142., 154., 164., 169., 177., 183., 189., 200., 214., 275., 293.,\n",
      "         47.,  98., 155., 176., 201.,  48., 156., 202., 203.,  88., 115., 141.,\n",
      "        147., 199., 206., 262., 285.,  45.,  90.,  97., 121., 124., 142., 154.,\n",
      "        164., 169., 177., 183., 189., 200., 214., 275., 293., 203.],\n",
      "       device='cuda:0'), tensor([1546., 1551., 1555., 1571.,   37.,   80.,  102.,  130.,  151.,  200.,\n",
      "         253.,  259.,  282.,  288.,  296.,  326.,  333.,  392.,  565.,  610.,\n",
      "         632.,  716.,  780.,  848.,   38.,  103.,  283.,  297.,  327.,  566.,\n",
      "         717.,  849.,  883.,  887., 1217., 1365., 1507., 1534., 1548., 1624.,\n",
      "        1663., 1732., 1750., 1757.], device='cuda:0'), tensor([862., 867., 871., 875., 888.,   9.,  19.,  28.,  34.,  41.,  52.,  58.,\n",
      "         71.,  80., 102., 141., 148., 152., 162., 170., 185., 198., 223., 470.,\n",
      "        485.,  10.,  20.,  42.,  53.,  59.,  72.,  81., 103., 142., 149., 163.,\n",
      "        171., 186., 199., 486., 569., 574., 595., 611., 647.], device='cuda:0'), tensor([112.,  79.], device='cuda:0'), tensor([36., 39., 40., 41., 44., 46., 49., 51.], device='cuda:0'), tensor([0.0000e+00, 7.0000e+00, 9.2000e+01, 1.0700e+02, 1.1100e+02, 1.1700e+02,\n",
      "        2.0800e+02, 2.2200e+02, 2.6400e+02, 2.6600e+02, 2.7000e+02, 2.7500e+02,\n",
      "        2.9000e+02, 3.3100e+02, 3.6400e+02, 3.7200e+02, 3.7600e+02, 3.8300e+02,\n",
      "        3.9200e+02, 4.0700e+02, 1.0000e+00, 4.0000e+00, 5.0000e+00, 2.0000e+01,\n",
      "        4.4000e+01, 8.6000e+01, 9.4000e+01, 1.1500e+02, 1.1800e+02, 1.9000e+02,\n",
      "        2.0500e+02, 2.5900e+02, 3.8000e+02, 3.8800e+02, 5.4000e+02, 5.5500e+02,\n",
      "        5.6300e+02, 5.8900e+02, 6.4400e+02, 6.5700e+02, 2.8000e+01, 4.1000e+01,\n",
      "        5.3800e+02, 6.7000e+02, 7.3400e+02, 7.6200e+02, 7.8400e+02, 8.0100e+02,\n",
      "        8.3300e+02, 8.9400e+02, 1.3320e+03, 1.4960e+03, 1.5010e+03, 1.6460e+03],\n",
      "       device='cuda:0'), tensor([ 0.,  9.,  1.,  5., 10.,  3., 11.], device='cuda:0'), tensor([ 336.,  496.,  504.,  576.,  702.,  714.,  363.,  372.,  378.,  460.,\n",
      "         473.,  514., 1428.,  457.,  461.,  470.,  474.,  476.,  513.,  515.,\n",
      "         517.,  684.,  686.,  688.,  692.,  710.,  720.,   34.,  341.,  354.,\n",
      "         384.,  393.,  420.,  435.,  438.,  441.,  450.,  465.,  468.,  492.,\n",
      "         503.,  526.,  535.,  546.,  550.,  557.,  602.], device='cuda:0'), tensor([1222., 1428., 1812., 1839., 1996.,   27.,  345.,  362.,  371.,  964.,\n",
      "         970., 1004., 1006., 1175., 1180., 1184., 1190., 1202., 1204., 1206.,\n",
      "        1209., 1211., 1213., 1223., 1225.,   35.,   37.,  303.,  347.,  352.,\n",
      "         360.,  369.,  492.,  498.,  982., 1054., 1102., 1123., 1235., 1239.,\n",
      "        1240., 1244., 1256., 1272., 1281.], device='cuda:0'), tensor([  4.,  16.,  33.,  65.,  81., 152., 166., 208., 290., 315., 342., 387.,\n",
      "        400., 478., 558., 584., 669., 727.,   5.,  17.,  28.,  51., 116., 134.,\n",
      "        219., 229., 239., 301., 323., 406., 511.], device='cuda:0'), tensor([   3.,   13.,  543.,  674.,  687.,  700.,  731.,  889.,  903.,  953.,\n",
      "         970.,  977., 1018., 1030., 1038., 1039., 1065., 1073., 1153., 1263.,\n",
      "           0.,   10.,  346.,  347.,  389.,  481.,  482.,  527.,  540.,  671.,\n",
      "         689.,  743.,  758.,  763.,  767.,  806.,  815.,  887.,  900.,  964.],\n",
      "       device='cuda:0'), tensor([ 13.,  14.,  16.,  42.,  51.,  59.,  74.,  77., 100., 132., 149., 165.,\n",
      "        170., 190., 204., 214., 218., 225., 240., 246.], device='cuda:0'), tensor([], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "print(X2_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1 = X1[:1]\n",
    "# X1_len = X1_len[:1]\n",
    "# X2 = X2[:1]\n",
    "# X2_len = X2_len[:1]\n",
    "# X1_id = X1_id[:1]\n",
    "# X2_id = X2_id[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_text(X2[0])\n",
    "# X1, X2_new, X1_len, X2_len_new = select_net(X1, X2, X1_len, X2_len, X1_id, X2_id)\n",
    "# show_text(X1[0])\n",
    "# for i in range(5):\n",
    "#     print(i, end=' ')\n",
    "#     show_text(X2_new[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank loss: 1.2440992593765259\n",
      "rank loss: 0.8229913711547852\n",
      "rank loss: 1.0240232944488525\n",
      "rank loss: 0.9202936887741089\n",
      "rank loss: 0.9478692412376404\n",
      "rank loss: 0.9125843644142151\n",
      "rank loss: 0.8528660535812378\n",
      "rank loss: 0.9710915684700012\n",
      "rank loss: 0.9428439140319824\n",
      "rank loss: 0.7923462986946106\n",
      "rank loss: 0.946742057800293\n",
      "rank loss: 0.6975569128990173\n",
      "rank loss: 0.7819538116455078\n",
      "rank loss: 0.8927761912345886\n",
      "rank loss: 0.7025883197784424\n",
      "rank loss: 0.8276301622390747\n",
      "rank loss: 0.7975870370864868\n",
      "rank loss: 0.7904086709022522\n",
      "rank loss: 0.7325917482376099\n",
      "rank loss: 0.7261925935745239\n",
      "rank loss: 0.7801852226257324\n",
      "rank loss: 0.6245788335800171\n",
      "rank loss: 0.7126272320747375\n",
      "rank loss: 0.7462284564971924\n",
      "rank loss: 0.7206588387489319\n",
      "rank loss: 0.7828032374382019\n",
      "rank loss: 0.7821846008300781\n",
      "rank loss: 0.6067131757736206\n",
      "rank loss: 0.6634309887886047\n",
      "rank loss: 0.6104261875152588\n",
      "rank loss: 0.9049686789512634\n",
      "rank loss: 0.638395369052887\n",
      "rank loss: 0.7226206660270691\n",
      "rank loss: 0.6957719922065735\n",
      "rank loss: 0.8506765365600586\n",
      "rank loss: 0.7192015051841736\n",
      "rank loss: 0.6862391233444214\n",
      "rank loss: 0.7497788667678833\n",
      "rank loss: 0.597584068775177\n",
      "rank loss: 0.6740573048591614\n",
      "rank loss: 0.6232136487960815\n",
      "rank loss: 0.7681015133857727\n",
      "rank loss: 0.5898695588111877\n",
      "rank loss: 0.6643608808517456\n",
      "rank loss: 0.8052259087562561\n",
      "rank loss: 0.6218456625938416\n",
      "rank loss: 0.8207953572273254\n",
      "rank loss: 0.7688977122306824\n",
      "rank loss: 0.6645971536636353\n",
      "rank loss: 0.7153121829032898\n",
      "rank loss: 0.6858443021774292\n",
      "rank loss: 0.787029504776001\n",
      "rank loss: 0.8725188970565796\n",
      "rank loss: 0.6507068276405334\n",
      "rank loss: 0.6556103229522705\n",
      "rank loss: 0.8978309631347656\n",
      "rank loss: 0.723688006401062\n",
      "rank loss: 0.6485400199890137\n",
      "rank loss: 0.8049335479736328\n",
      "rank loss: 0.6393804550170898\n",
      "rank loss: 0.7238069176673889\n",
      "rank loss: 0.7157621383666992\n",
      "rank loss: 0.6579652428627014\n",
      "rank loss: 0.6346930265426636\n",
      "rank loss: 0.7070406675338745\n",
      "rank loss: 0.7674100399017334\n",
      "rank loss: 0.7428968548774719\n",
      "rank loss: 0.7031596302986145\n",
      "rank loss: 0.6410322189331055\n",
      "rank loss: 0.6666056513786316\n",
      "rank loss: 0.6913569569587708\n",
      "rank loss: 0.7433634996414185\n",
      "rank loss: 0.7725082039833069\n",
      "rank loss: 0.5153768658638\n",
      "rank loss: 0.7379727959632874\n",
      "rank loss: 0.6126009821891785\n",
      "rank loss: 0.7027496099472046\n",
      "rank loss: 0.7403891086578369\n",
      "rank loss: 0.7824200391769409\n",
      "rank loss: 0.6571428775787354\n",
      "rank loss: 0.6719782948493958\n",
      "rank loss: 0.5103748440742493\n",
      "rank loss: 0.6698390245437622\n",
      "rank loss: 0.7053465843200684\n",
      "rank loss: 0.7335399985313416\n",
      "rank loss: 0.5014694333076477\n",
      "rank loss: 0.5639861822128296\n",
      "rank loss: 0.5520784854888916\n",
      "rank loss: 0.8920755386352539\n",
      "rank loss: 0.7539626359939575\n",
      "rank loss: 0.755633533000946\n",
      "rank loss: 0.6206505298614502\n",
      "rank loss: 0.7497553825378418\n",
      "rank loss: 0.6391770243644714\n",
      "rank loss: 0.5714777112007141\n",
      "rank loss: 0.6655421257019043\n",
      "rank loss: 0.6899400353431702\n",
      "rank loss: 0.7601068019866943\n",
      "rank loss: 0.6746046543121338\n",
      "rank loss: 0.6305593848228455\n",
      "rank loss: 0.6212027072906494\n",
      "rank loss: 0.6773606538772583\n",
      "rank loss: 0.5868635177612305\n",
      "rank loss: 0.6569311618804932\n",
      "rank loss: 0.6316206455230713\n",
      "rank loss: 0.6969811916351318\n",
      "rank loss: 0.7665066123008728\n",
      "rank loss: 0.7314639091491699\n",
      "rank loss: 0.738402247428894\n",
      "rank loss: 0.8195988535881042\n",
      "rank loss: 0.6317790150642395\n",
      "rank loss: 0.6606825590133667\n",
      "rank loss: 0.8311890959739685\n",
      "rank loss: 0.6475696563720703\n",
      "rank loss: 0.6188990473747253\n",
      "rank loss: 0.662055253982544\n",
      "rank loss: 0.6415798664093018\n",
      "rank loss: 0.7538232207298279\n",
      "rank loss: 0.7169276475906372\n",
      "rank loss: 0.5444081425666809\n",
      "rank loss: 0.8478977680206299\n",
      "rank loss: 0.8004050850868225\n",
      "rank loss: 0.7934326529502869\n",
      "rank loss: 0.7440927624702454\n",
      "rank loss: 0.6177713871002197\n",
      "rank loss: 0.5490125417709351\n",
      "rank loss: 0.6976748704910278\n",
      "rank loss: 0.7150028347969055\n",
      "rank loss: 0.6079630255699158\n",
      "rank loss: 0.7489631772041321\n",
      "rank loss: 0.5931352376937866\n",
      "rank loss: 0.6233872175216675\n",
      "rank loss: 0.6035869121551514\n",
      "rank loss: 0.7666019797325134\n",
      "rank loss: 0.6083728671073914\n",
      "rank loss: 0.5776520371437073\n",
      "rank loss: 0.7071179151535034\n",
      "rank loss: 0.6489529013633728\n",
      "rank loss: 0.7101670503616333\n",
      "rank loss: 0.6568520069122314\n",
      "rank loss: 0.6436822414398193\n",
      "rank loss: 0.6340804100036621\n",
      "rank loss: 0.642778754234314\n",
      "rank loss: 0.5648192167282104\n",
      "rank loss: 0.6694515347480774\n",
      "rank loss: 0.6815599799156189\n",
      "rank loss: 0.6408759355545044\n",
      "rank loss: 0.6861826777458191\n",
      "rank loss: 0.6834108829498291\n",
      "rank loss: 0.5521477460861206\n",
      "rank loss: 0.6695194840431213\n",
      "rank loss: 0.7779378890991211\n",
      "rank loss: 0.6500001549720764\n",
      "rank loss: 0.5649484992027283\n",
      "rank loss: 0.5352599620819092\n",
      "rank loss: 0.5958554744720459\n",
      "rank loss: 0.7846100926399231\n",
      "rank loss: 0.6749200820922852\n",
      "rank loss: 0.6503067016601562\n",
      "rank loss: 0.6003694534301758\n",
      "rank loss: 0.6962055563926697\n",
      "rank loss: 0.7391515374183655\n",
      "rank loss: 0.623138427734375\n",
      "rank loss: 0.6203383803367615\n",
      "rank loss: 0.627662181854248\n",
      "rank loss: 0.5498651266098022\n",
      "rank loss: 0.6776638627052307\n",
      "rank loss: 0.6647909283638\n",
      "rank loss: 0.6000465154647827\n",
      "rank loss: 0.605740487575531\n",
      "rank loss: 0.836161732673645\n",
      "rank loss: 0.5886103510856628\n",
      "rank loss: 0.5981857180595398\n",
      "rank loss: 0.627661943435669\n",
      "rank loss: 0.6447921395301819\n",
      "rank loss: 0.7295776009559631\n",
      "rank loss: 0.6941414475440979\n",
      "rank loss: 0.6098263263702393\n",
      "rank loss: 0.5078326463699341\n",
      "rank loss: 0.7209081649780273\n",
      "rank loss: 0.7317319512367249\n",
      "rank loss: 0.7038914561271667\n",
      "rank loss: 0.7256866097450256\n",
      "rank loss: 0.6196452379226685\n",
      "rank loss: 0.6483606696128845\n",
      "rank loss: 0.6605318784713745\n",
      "rank loss: 0.6860591173171997\n",
      "rank loss: 0.6457188129425049\n",
      "rank loss: 0.652768075466156\n",
      "rank loss: 0.6029199361801147\n",
      "rank loss: 0.6497268676757812\n",
      "rank loss: 0.6975792050361633\n",
      "rank loss: 0.7145440578460693\n",
      "rank loss: 0.6016995906829834\n",
      "rank loss: 0.5608247518539429\n",
      "rank loss: 0.6581915020942688\n",
      "rank loss: 0.7548954486846924\n",
      "rank loss: 0.5983032584190369\n",
      "rank loss: 0.6830407381057739\n",
      "rank loss: 0.474190890789032\n",
      "rank loss: 0.5566912889480591\n",
      "rank loss: 0.5983591079711914\n",
      "rank loss: 0.6871813535690308\n",
      "rank loss: 0.6339972019195557\n",
      "rank loss: 0.6946998238563538\n",
      "rank loss: 0.7188394665718079\n",
      "rank loss: 0.8059810400009155\n",
      "rank loss: 0.6906298398971558\n",
      "rank loss: 0.5458728075027466\n",
      "rank loss: 0.7195521593093872\n",
      "rank loss: 0.6260150074958801\n",
      "rank loss: 0.6863861083984375\n",
      "rank loss: 0.7400534152984619\n",
      "rank loss: 0.6141132712364197\n",
      "rank loss: 0.6115153431892395\n",
      "rank loss: 0.6783310770988464\n",
      "rank loss: 0.6176223754882812\n",
      "rank loss: 0.6290658712387085\n",
      "rank loss: 0.5435999035835266\n",
      "rank loss: 0.7192268371582031\n",
      "rank loss: 0.7591245770454407\n",
      "rank loss: 0.5513469576835632\n",
      "rank loss: 0.5692832469940186\n",
      "rank loss: 0.7061794996261597\n",
      "rank loss: 0.603985607624054\n",
      "rank loss: 0.5973588228225708\n",
      "rank loss: 0.55748051404953\n",
      "rank loss: 0.7310728430747986\n",
      "rank loss: 0.6708822250366211\n",
      "rank loss: 0.6502158045768738\n",
      "rank loss: 0.6676028966903687\n",
      "rank loss: 0.6833482384681702\n",
      "rank loss: 0.7520660161972046\n",
      "rank loss: 0.609302818775177\n",
      "rank loss: 0.6627857685089111\n",
      "rank loss: 0.7077106833457947\n",
      "rank loss: 0.49427303671836853\n",
      "rank loss: 0.847697913646698\n",
      "rank loss: 0.6485704183578491\n",
      "rank loss: 0.7463746666908264\n",
      "rank loss: 0.5944848656654358\n",
      "rank loss: 0.5556864142417908\n",
      "rank loss: 0.6977335810661316\n",
      "rank loss: 0.7075380682945251\n",
      "rank loss: 0.7195950150489807\n",
      "rank loss: 0.6088722944259644\n",
      "rank loss: 0.5626019835472107\n",
      "rank loss: 0.5628526210784912\n",
      "rank loss: 0.6666055917739868\n",
      "rank loss: 0.6218146085739136\n",
      "rank loss: 0.6609748601913452\n",
      "rank loss: 0.6372306942939758\n",
      "rank loss: 0.5386930108070374\n",
      "rank loss: 0.6164373755455017\n",
      "rank loss: 0.640087902545929\n",
      "rank loss: 0.6123031973838806\n",
      "rank loss: 0.6277716159820557\n",
      "rank loss: 0.6492937803268433\n",
      "rank loss: 0.6675507426261902\n",
      "rank loss: 0.5849136710166931\n",
      "rank loss: 0.679999828338623\n",
      "rank loss: 0.589767336845398\n",
      "rank loss: 0.5826858282089233\n",
      "rank loss: 0.6396178007125854\n",
      "rank loss: 0.5204975605010986\n",
      "rank loss: 0.5805791616439819\n",
      "rank loss: 0.5956391096115112\n",
      "rank loss: 0.7837774753570557\n",
      "rank loss: 0.6226849555969238\n",
      "rank loss: 0.7541450262069702\n",
      "rank loss: 0.564865231513977\n",
      "rank loss: 0.6624115109443665\n",
      "rank loss: 0.4851871132850647\n",
      "rank loss: 0.5966708064079285\n",
      "rank loss: 0.6408855319023132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank loss: 0.5214572548866272\n",
      "rank loss: 0.7345959544181824\n",
      "rank loss: 0.6837531924247742\n",
      "rank loss: 0.725126326084137\n",
      "rank loss: 0.572644054889679\n",
      "rank loss: 0.5535497665405273\n",
      "rank loss: 0.6656034588813782\n",
      "rank loss: 0.5773030519485474\n",
      "rank loss: 0.6440893411636353\n",
      "rank loss: 0.6690893173217773\n",
      "rank loss: 0.7227907180786133\n",
      "rank loss: 0.7116897702217102\n",
      "rank loss: 0.5992423295974731\n",
      "rank loss: 0.7305021286010742\n",
      "rank loss: 0.6894205212593079\n",
      "rank loss: 0.5510427951812744\n",
      "rank loss: 0.5643130540847778\n",
      "rank loss: 0.5811717510223389\n",
      "rank loss: 0.6286486387252808\n",
      "rank loss: 0.5052006840705872\n",
      "rank loss: 0.5410985350608826\n",
      "rank loss: 0.8264406323432922\n",
      "rank loss: 0.7780131101608276\n",
      "rank loss: 0.677638590335846\n",
      "rank loss: 0.8041553497314453\n",
      "rank loss: 0.7314722537994385\n",
      "rank loss: 0.7255677580833435\n",
      "rank loss: 0.7017753720283508\n",
      "rank loss: 0.5449838638305664\n",
      "rank loss: 0.5712422728538513\n",
      "rank loss: 0.5553099513053894\n",
      "rank loss: 0.6157011985778809\n",
      "rank loss: 0.8042247295379639\n",
      "rank loss: 0.689866304397583\n",
      "rank loss: 0.5579885244369507\n",
      "rank loss: 0.5227994322776794\n",
      "rank loss: 0.5863366723060608\n",
      "rank loss: 0.5826295614242554\n",
      "rank loss: 0.601813018321991\n",
      "rank loss: 0.6249229907989502\n",
      "rank loss: 0.6125352382659912\n",
      "rank loss: 0.6577044725418091\n",
      "rank loss: 0.5782291889190674\n",
      "rank loss: 0.6033129692077637\n",
      "rank loss: 0.6798101663589478\n",
      "rank loss: 0.6665772795677185\n",
      "rank loss: 0.48962661623954773\n",
      "rank loss: 0.7000834345817566\n",
      "rank loss: 0.6289227604866028\n",
      "rank loss: 0.6253677010536194\n",
      "rank loss: 0.6893185973167419\n",
      "rank loss: 0.6424604058265686\n",
      "rank loss: 0.6359390020370483\n",
      "rank loss: 0.6372842788696289\n",
      "rank loss: 0.6104004383087158\n",
      "rank loss: 0.5891364216804504\n",
      "rank loss: 0.49453574419021606\n",
      "rank loss: 0.43087372183799744\n",
      "rank loss: 0.6419385671615601\n",
      "rank loss: 0.6824475526809692\n",
      "rank loss: 0.4567055404186249\n",
      "rank loss: 0.5599691867828369\n",
      "rank loss: 0.5325854420661926\n",
      "rank loss: 0.5656765103340149\n",
      "rank loss: 0.6893409490585327\n",
      "rank loss: 0.6148716807365417\n",
      "rank loss: 0.6392168402671814\n",
      "rank loss: 0.5234069228172302\n",
      "rank loss: 0.894956648349762\n",
      "rank loss: 0.5858187079429626\n",
      "rank loss: 0.663970947265625\n",
      "rank loss: 0.6033505797386169\n",
      "rank loss: 0.6533446311950684\n",
      "rank loss: 0.7860424518585205\n",
      "rank loss: 0.6241350173950195\n",
      "rank loss: 0.6314749717712402\n",
      "rank loss: 0.5624822378158569\n",
      "rank loss: 0.6909539699554443\n",
      "rank loss: 0.5696889162063599\n",
      "rank loss: 0.5425935387611389\n",
      "rank loss: 0.6348133683204651\n",
      "rank loss: 0.540854275226593\n",
      "rank loss: 0.6057292819023132\n",
      "rank loss: 0.5493369698524475\n",
      "rank loss: 0.6204784512519836\n",
      "rank loss: 0.6203626394271851\n",
      "rank loss: 0.7380543351173401\n",
      "rank loss: 0.5722305178642273\n",
      "rank loss: 0.6470512747764587\n",
      "rank loss: 0.5053719282150269\n",
      "rank loss: 0.5456103682518005\n",
      "rank loss: 0.6694517135620117\n",
      "rank loss: 0.5654348134994507\n",
      "rank loss: 0.7118911743164062\n",
      "rank loss: 0.6071518063545227\n",
      "rank loss: 0.6735199093818665\n",
      "rank loss: 0.6638251543045044\n",
      "rank loss: 0.6204723715782166\n",
      "rank loss: 0.7196683287620544\n",
      "rank loss: 0.8134771585464478\n",
      "rank loss: 0.5459374189376831\n",
      "rank loss: 0.6341087818145752\n",
      "rank loss: 0.5847015380859375\n",
      "rank loss: 0.5060599446296692\n",
      "rank loss: 0.6107646226882935\n",
      "rank loss: 0.6308137774467468\n",
      "rank loss: 0.6331291794776917\n",
      "rank loss: 0.6951324343681335\n",
      "rank loss: 0.5874184966087341\n",
      "rank loss: 0.5879519581794739\n",
      "rank loss: 0.6407280564308167\n",
      "rank loss: 0.6365108489990234\n",
      "rank loss: 0.6697092056274414\n",
      "rank loss: 0.6136928200721741\n",
      "rank loss: 0.5314486026763916\n",
      "rank loss: 0.6095393300056458\n",
      "rank loss: 0.6400590538978577\n",
      "rank loss: 0.5414653420448303\n",
      "rank loss: 0.6821395754814148\n",
      "rank loss: 0.636947751045227\n",
      "rank loss: 0.4576088488101959\n",
      "rank loss: 0.6896036863327026\n",
      "rank loss: 0.49596473574638367\n",
      "rank loss: 0.6024177670478821\n",
      "rank loss: 0.6380476355552673\n",
      "rank loss: 0.6449868083000183\n",
      "rank loss: 0.636564314365387\n",
      "rank loss: 0.6007084250450134\n",
      "rank loss: 0.6712693572044373\n",
      "rank loss: 0.6146095991134644\n",
      "rank loss: 0.5794331431388855\n",
      "rank loss: 0.538013219833374\n",
      "rank loss: 0.5025626420974731\n",
      "rank loss: 0.7013617753982544\n",
      "rank loss: 0.5949779152870178\n",
      "rank loss: 0.6693633794784546\n",
      "rank loss: 0.6889306306838989\n",
      "rank loss: 0.5727329254150391\n",
      "rank loss: 0.5942408442497253\n",
      "rank loss: 0.47933414578437805\n",
      "rank loss: 0.6428157091140747\n",
      "rank loss: 0.5533363819122314\n",
      "rank loss: 0.5773743987083435\n",
      "rank loss: 0.620851993560791\n",
      "rank loss: 0.5259279012680054\n",
      "rank loss: 0.5207778215408325\n",
      "rank loss: 0.6095184683799744\n",
      "rank loss: 0.5226948261260986\n",
      "rank loss: 0.6083025932312012\n",
      "rank loss: 0.580160915851593\n",
      "rank loss: 0.47885262966156006\n",
      "rank loss: 0.6360970735549927\n",
      "rank loss: 0.6105557680130005\n",
      "rank loss: 0.53983074426651\n",
      "rank loss: 0.5670233368873596\n",
      "rank loss: 0.59978187084198\n",
      "rank loss: 0.637985348701477\n",
      "rank loss: 0.6001848578453064\n",
      "rank loss: 0.5990176796913147\n",
      "rank loss: 0.6009010672569275\n",
      "rank loss: 0.7333129644393921\n",
      "rank loss: 0.7497989535331726\n",
      "rank loss: 0.6204794645309448\n",
      "rank loss: 0.580868124961853\n",
      "rank loss: 0.5820795893669128\n",
      "rank loss: 0.5493431091308594\n",
      "rank loss: 0.6844908595085144\n",
      "rank loss: 0.7241042852401733\n",
      "rank loss: 0.671880304813385\n",
      "rank loss: 0.6401603817939758\n",
      "rank loss: 0.6460983157157898\n",
      "rank loss: 0.6182152628898621\n",
      "rank loss: 0.5934262871742249\n",
      "rank loss: 0.6010951995849609\n",
      "rank loss: 0.5421529412269592\n",
      "rank loss: 0.4824028015136719\n",
      "rank loss: 0.6546196341514587\n",
      "rank loss: 0.62633216381073\n",
      "rank loss: 0.5897296667098999\n",
      "rank loss: 0.6552426218986511\n",
      "rank loss: 0.5870454907417297\n",
      "rank loss: 0.6807305812835693\n",
      "rank loss: 0.7070840001106262\n",
      "rank loss: 0.474953830242157\n",
      "rank loss: 0.6132183074951172\n",
      "rank loss: 0.5398140549659729\n",
      "rank loss: 0.5703791975975037\n",
      "rank loss: 0.6045340299606323\n",
      "rank loss: 0.5986158847808838\n",
      "rank loss: 0.5300924181938171\n",
      "rank loss: 0.5195063948631287\n",
      "rank loss: 0.46780499815940857\n",
      "rank loss: 0.8999868631362915\n",
      "rank loss: 0.6367818117141724\n",
      "rank loss: 0.5732428431510925\n",
      "rank loss: 0.7316388487815857\n",
      "rank loss: 0.5495551824569702\n",
      "rank loss: 0.7531367540359497\n",
      "rank loss: 0.5172849893569946\n",
      "rank loss: 0.7468598484992981\n",
      "rank loss: 0.6686868667602539\n",
      "rank loss: 0.64499831199646\n",
      "rank loss: 0.541696310043335\n",
      "rank loss: 0.722154974937439\n",
      "rank loss: 0.6649061441421509\n",
      "rank loss: 0.6043626070022583\n",
      "rank loss: 0.5031648278236389\n",
      "rank loss: 0.5598386526107788\n",
      "rank loss: 0.6868464350700378\n",
      "rank loss: 0.526007354259491\n",
      "rank loss: 0.6346697211265564\n",
      "rank loss: 0.6808161735534668\n",
      "rank loss: 0.5604357123374939\n",
      "rank loss: 0.5179497003555298\n",
      "rank loss: 0.7023367285728455\n",
      "rank loss: 0.6681867837905884\n",
      "rank loss: 0.581626832485199\n",
      "rank loss: 0.6484377980232239\n",
      "rank loss: 0.6887736320495605\n",
      "rank loss: 0.5846593379974365\n",
      "rank loss: 0.5583004355430603\n",
      "rank loss: 0.5611717700958252\n",
      "rank loss: 0.5775772929191589\n",
      "rank loss: 0.45599615573883057\n",
      "rank loss: 0.6090721487998962\n",
      "rank loss: 0.6418495774269104\n",
      "rank loss: 0.6192474365234375\n",
      "rank loss: 0.5869032144546509\n",
      "rank loss: 0.5568737387657166\n",
      "rank loss: 0.5076123476028442\n",
      "rank loss: 0.6861097812652588\n",
      "rank loss: 0.5799488425254822\n",
      "rank loss: 0.4677561819553375\n",
      "rank loss: 0.5707340836524963\n",
      "rank loss: 0.5164376497268677\n",
      "rank loss: 0.6423307657241821\n",
      "rank loss: 0.6774745583534241\n",
      "rank loss: 0.5286232233047485\n",
      "rank loss: 0.5807572603225708\n",
      "rank loss: 0.526725172996521\n",
      "rank loss: 0.6139131188392639\n",
      "rank loss: 0.5629901885986328\n",
      "rank loss: 0.5863733887672424\n",
      "rank loss: 0.4833923280239105\n",
      "rank loss: 0.5849801898002625\n",
      "rank loss: 0.6144890189170837\n",
      "rank loss: 0.6525876522064209\n",
      "rank loss: 0.6519935727119446\n",
      "rank loss: 0.4840858280658722\n",
      "rank loss: 0.7518343925476074\n",
      "rank loss: 0.6099937558174133\n",
      "rank loss: 0.5132930278778076\n",
      "rank loss: 0.7450626492500305\n",
      "rank loss: 0.5341077446937561\n",
      "rank loss: 0.6204671859741211\n",
      "rank loss: 0.6542835831642151\n",
      "rank loss: 0.553019106388092\n",
      "rank loss: 0.6702417135238647\n",
      "rank loss: 0.5655002593994141\n",
      "rank loss: 0.6120678782463074\n",
      "rank loss: 0.5197299718856812\n",
      "rank loss: 0.5478416085243225\n",
      "rank loss: 0.6092565059661865\n",
      "rank loss: 0.5888276100158691\n",
      "rank loss: 0.5580365061759949\n",
      "rank loss: 0.5911743640899658\n",
      "rank loss: 0.6213680505752563\n",
      "rank loss: 0.49591535329818726\n",
      "rank loss: 0.612488865852356\n",
      "rank loss: 0.6247293949127197\n",
      "rank loss: 0.6349310278892517\n",
      "rank loss: 0.5369970798492432\n",
      "rank loss: 0.47342944145202637\n",
      "rank loss: 0.6138781309127808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank loss: 0.6184329986572266\n",
      "rank loss: 0.5074203610420227\n",
      "rank loss: 0.5009621977806091\n",
      "rank loss: 0.6265016198158264\n",
      "rank loss: 0.5759097933769226\n",
      "rank loss: 0.5756366848945618\n",
      "rank loss: 0.6621382236480713\n",
      "rank loss: 0.6761197447776794\n",
      "rank loss: 0.5883328914642334\n",
      "rank loss: 0.5905451774597168\n",
      "rank loss: 0.6056548357009888\n",
      "rank loss: 0.5463549494743347\n",
      "rank loss: 0.6404637098312378\n",
      "rank loss: 0.7027940154075623\n",
      "rank loss: 0.6647210717201233\n",
      "rank loss: 0.5131840109825134\n",
      "rank loss: 0.61524498462677\n",
      "rank loss: 0.5861734747886658\n",
      "rank loss: 0.5971730947494507\n",
      "rank loss: 0.4795689284801483\n",
      "rank loss: 0.5862950086593628\n",
      "rank loss: 0.6167292594909668\n",
      "rank loss: 0.6143447160720825\n",
      "rank loss: 0.582001805305481\n",
      "rank loss: 0.6951585412025452\n",
      "rank loss: 0.603801965713501\n",
      "rank loss: 0.7294945120811462\n",
      "rank loss: 0.6180760264396667\n",
      "rank loss: 0.5084499716758728\n",
      "rank loss: 0.5336926579475403\n",
      "rank loss: 0.5536542534828186\n",
      "rank loss: 0.5721023678779602\n",
      "rank loss: 0.5547928810119629\n",
      "rank loss: 0.632355809211731\n",
      "rank loss: 0.5480716824531555\n",
      "rank loss: 0.44114091992378235\n",
      "rank loss: 0.49143117666244507\n",
      "rank loss: 0.5899538993835449\n",
      "rank loss: 0.6363067030906677\n",
      "rank loss: 0.5174705982208252\n",
      "rank loss: 0.5345631837844849\n",
      "rank loss: 0.7518062591552734\n",
      "rank loss: 0.5143353343009949\n",
      "rank loss: 0.5736023783683777\n",
      "rank loss: 0.6875472664833069\n",
      "rank loss: 0.7237136960029602\n",
      "rank loss: 0.6696240305900574\n",
      "rank loss: 0.5636712312698364\n",
      "rank loss: 0.45905500650405884\n",
      "rank loss: 0.522714376449585\n",
      "rank loss: 0.715470552444458\n",
      "rank loss: 0.5976311564445496\n",
      "rank loss: 0.4994349479675293\n",
      "rank loss: 0.6638716459274292\n",
      "rank loss: 0.5972625613212585\n",
      "rank loss: 0.6537092328071594\n",
      "rank loss: 0.6119136810302734\n",
      "rank loss: 0.4501342177391052\n",
      "rank loss: 0.583972692489624\n",
      "rank loss: 0.618328332901001\n",
      "rank loss: 0.5889119505882263\n",
      "rank loss: 0.5624268054962158\n",
      "rank loss: 0.5766793489456177\n",
      "rank loss: 0.7200031876564026\n",
      "rank loss: 0.5985130667686462\n",
      "rank loss: 0.5435989499092102\n",
      "rank loss: 0.6193836331367493\n",
      "rank loss: 0.602634847164154\n",
      "rank loss: 0.72625732421875\n",
      "rank loss: 0.6795852780342102\n",
      "rank loss: 0.5891470909118652\n",
      "rank loss: 0.6370534896850586\n",
      "rank loss: 0.552428662776947\n",
      "rank loss: 0.6109991669654846\n",
      "rank loss: 0.6181321740150452\n",
      "rank loss: 0.5699795484542847\n",
      "rank loss: 0.5487346053123474\n",
      "rank loss: 0.505006730556488\n",
      "rank loss: 0.4296201467514038\n",
      "rank loss: 0.4754174053668976\n",
      "rank loss: 0.607640266418457\n",
      "rank loss: 0.5206592679023743\n",
      "rank loss: 0.6972207427024841\n",
      "rank loss: 0.5948558449745178\n",
      "rank loss: 0.5874366760253906\n",
      "rank loss: 0.6466091275215149\n",
      "rank loss: 0.5351391434669495\n",
      "rank loss: 0.6683930158615112\n",
      "rank loss: 0.5352246761322021\n",
      "rank loss: 0.618751049041748\n",
      "rank loss: 0.5012379288673401\n",
      "rank loss: 0.590632438659668\n",
      "rank loss: 0.6446759104728699\n",
      "rank loss: 0.6062040328979492\n",
      "rank loss: 0.6448010206222534\n",
      "rank loss: 0.6658264994621277\n",
      "rank loss: 0.6175107955932617\n",
      "rank loss: 0.5270527005195618\n",
      "rank loss: 0.5913041830062866\n",
      "rank loss: 0.5464759469032288\n",
      "rank loss: 0.5822446942329407\n",
      "rank loss: 0.6383266448974609\n",
      "rank loss: 0.5520867109298706\n",
      "rank loss: 0.6607476472854614\n",
      "rank loss: 0.40427976846694946\n",
      "rank loss: 0.8921526074409485\n",
      "rank loss: 0.5570176243782043\n",
      "rank loss: 0.659054696559906\n",
      "rank loss: 0.698436439037323\n",
      "rank loss: 0.6668864488601685\n",
      "rank loss: 0.5959926247596741\n",
      "rank loss: 0.5810298919677734\n",
      "rank loss: 0.5796512365341187\n",
      "rank loss: 0.556941568851471\n",
      "rank loss: 0.6260780096054077\n",
      "rank loss: 0.6424365639686584\n",
      "rank loss: 0.5803472995758057\n",
      "rank loss: 0.5851472616195679\n",
      "rank loss: 0.6985645294189453\n",
      "rank loss: 0.7502471208572388\n",
      "rank loss: 0.6555708050727844\n",
      "rank loss: 0.6117693185806274\n",
      "rank loss: 0.6179690361022949\n",
      "rank loss: 0.5635221004486084\n",
      "rank loss: 0.7463240027427673\n",
      "rank loss: 0.5426671504974365\n",
      "rank loss: 0.5482625961303711\n",
      "rank loss: 0.600611686706543\n",
      "rank loss: 0.8244651556015015\n",
      "rank loss: 0.5572248101234436\n",
      "rank loss: 0.5099024772644043\n",
      "rank loss: 0.6747524738311768\n",
      "rank loss: 0.5549834966659546\n",
      "rank loss: 0.6433934569358826\n",
      "rank loss: 0.5371917486190796\n",
      "rank loss: 0.5455824136734009\n",
      "rank loss: 0.7100297212600708\n",
      "rank loss: 0.5222744941711426\n",
      "rank loss: 0.5375985503196716\n",
      "rank loss: 0.6514747738838196\n",
      "rank loss: 0.7333886623382568\n",
      "rank loss: 0.7967159748077393\n",
      "rank loss: 0.5974269509315491\n",
      "rank loss: 0.6474511027336121\n",
      "rank loss: 0.4963943362236023\n",
      "rank loss: 0.5453624129295349\n",
      "rank loss: 0.5601117014884949\n",
      "rank loss: 0.6969782710075378\n",
      "rank loss: 0.46689558029174805\n",
      "rank loss: 0.7079838514328003\n",
      "rank loss: 0.5958368182182312\n",
      "rank loss: 0.6062502264976501\n",
      "rank loss: 0.6727917194366455\n",
      "rank loss: 0.6078102588653564\n",
      "rank loss: 0.631653368473053\n",
      "rank loss: 0.6346092224121094\n",
      "rank loss: 0.5961617231369019\n",
      "rank loss: 0.6432594060897827\n",
      "rank loss: 0.5822364687919617\n",
      "rank loss: 0.4611976444721222\n",
      "rank loss: 0.4914308190345764\n",
      "rank loss: 0.599615216255188\n",
      "rank loss: 0.6366140842437744\n",
      "rank loss: 0.6157158017158508\n",
      "rank loss: 0.6998213529586792\n",
      "rank loss: 0.5971397161483765\n",
      "rank loss: 0.5480889678001404\n",
      "rank loss: 0.5192388892173767\n",
      "rank loss: 0.6581830978393555\n",
      "rank loss: 0.582824170589447\n",
      "rank loss: 0.6128110289573669\n",
      "rank loss: 0.5825735926628113\n",
      "rank loss: 0.5988906621932983\n",
      "rank loss: 0.572070300579071\n",
      "rank loss: 0.6628511548042297\n",
      "rank loss: 0.6388939619064331\n",
      "rank loss: 0.5988813638687134\n",
      "rank loss: 0.6350087523460388\n",
      "rank loss: 0.7300096750259399\n",
      "rank loss: 0.6351019740104675\n",
      "rank loss: 0.6673908829689026\n",
      "rank loss: 0.6821096539497375\n",
      "rank loss: 0.6486024856567383\n",
      "rank loss: 0.602831244468689\n",
      "rank loss: 0.5372716188430786\n",
      "rank loss: 0.6096758842468262\n",
      "rank loss: 0.6450441479682922\n",
      "rank loss: 0.7419288754463196\n",
      "rank loss: 0.4778490364551544\n",
      "rank loss: 0.5094389319419861\n",
      "rank loss: 0.6504392027854919\n",
      "rank loss: 0.47297582030296326\n",
      "rank loss: 0.6468738913536072\n",
      "rank loss: 0.6395460963249207\n",
      "rank loss: 0.646245002746582\n",
      "rank loss: 0.5085287094116211\n",
      "rank loss: 0.6310669779777527\n",
      "rank loss: 0.5432586073875427\n",
      "rank loss: 0.7547257542610168\n",
      "rank loss: 0.6142258644104004\n",
      "rank loss: 0.48605599999427795\n",
      "rank loss: 0.6752131581306458\n",
      "rank loss: 0.6311169862747192\n",
      "rank loss: 0.6006457209587097\n",
      "rank loss: 0.4880816638469696\n",
      "rank loss: 0.6235492825508118\n",
      "rank loss: 0.4791252017021179\n",
      "rank loss: 0.5522701144218445\n",
      "rank loss: 0.6282883286476135\n",
      "rank loss: 0.6737765073776245\n",
      "rank loss: 0.5462601184844971\n",
      "rank loss: 0.5181819200515747\n",
      "rank loss: 0.49609676003456116\n",
      "rank loss: 0.5971574783325195\n",
      "rank loss: 0.575393795967102\n",
      "rank loss: 0.702235996723175\n",
      "rank loss: 0.5107231736183167\n",
      "rank loss: 0.634570300579071\n",
      "rank loss: 0.5368734002113342\n",
      "rank loss: 0.657570481300354\n",
      "rank loss: 0.5808947086334229\n",
      "rank loss: 0.5604522824287415\n",
      "rank loss: 0.5946366190910339\n",
      "rank loss: 0.6404856443405151\n",
      "rank loss: 0.658988356590271\n",
      "rank loss: 0.5560711622238159\n",
      "rank loss: 0.5030056834220886\n",
      "rank loss: 0.5950220227241516\n",
      "rank loss: 0.49962595105171204\n",
      "rank loss: 0.5113527774810791\n",
      "rank loss: 0.5345250368118286\n",
      "rank loss: 0.599452018737793\n",
      "rank loss: 0.5162262320518494\n",
      "rank loss: 0.5238072276115417\n",
      "rank loss: 0.48166823387145996\n",
      "rank loss: 0.6527479290962219\n",
      "rank loss: 0.5989460349082947\n",
      "rank loss: 0.5518885850906372\n",
      "rank loss: 0.6141066551208496\n",
      "rank loss: 0.6950079202651978\n",
      "rank loss: 0.5894407033920288\n",
      "rank loss: 0.6437702178955078\n",
      "rank loss: 0.5518302917480469\n",
      "rank loss: 0.6203332543373108\n",
      "rank loss: 0.6562574505805969\n",
      "rank loss: 0.680164635181427\n",
      "rank loss: 0.5093585848808289\n",
      "rank loss: 0.5029905438423157\n",
      "rank loss: 0.7081340551376343\n",
      "rank loss: 0.5217329263687134\n",
      "rank loss: 0.48162809014320374\n",
      "rank loss: 0.5676015615463257\n",
      "rank loss: 0.47882071137428284\n",
      "rank loss: 0.5916886925697327\n",
      "rank loss: 0.5564243197441101\n",
      "rank loss: 0.5870479345321655\n",
      "rank loss: 0.6464381814002991\n",
      "rank loss: 0.5265538096427917\n",
      "rank loss: 0.5070034861564636\n",
      "rank loss: 0.5556443929672241\n",
      "rank loss: 0.5555896759033203\n",
      "rank loss: 0.6116732358932495\n",
      "rank loss: 0.42181694507598877\n",
      "rank loss: 0.6075692772865295\n",
      "rank loss: 0.618545413017273\n",
      "rank loss: 0.6329010128974915\n",
      "rank loss: 0.48127079010009766\n",
      "rank loss: 0.768819272518158\n",
      "rank loss: 0.5511385202407837\n",
      "rank loss: 0.42242270708084106\n",
      "rank loss: 0.42763930559158325\n",
      "rank loss: 0.5666214227676392\n",
      "rank loss: 0.6453397870063782\n",
      "rank loss: 0.6254563927650452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank loss: 0.7561215758323669\n",
      "rank loss: 0.5722865462303162\n",
      "rank loss: 0.4913157820701599\n",
      "rank loss: 0.558089017868042\n",
      "rank loss: 0.6448110938072205\n",
      "rank loss: 0.612916111946106\n",
      "rank loss: 0.5131893157958984\n",
      "rank loss: 0.5444574356079102\n",
      "rank loss: 0.4998959004878998\n",
      "rank loss: 0.5485565066337585\n",
      "rank loss: 0.5516345500946045\n",
      "rank loss: 0.6016167402267456\n",
      "rank loss: 0.639536440372467\n",
      "rank loss: 0.5804027318954468\n",
      "rank loss: 0.5594169497489929\n",
      "rank loss: 0.6146384477615356\n",
      "rank loss: 0.4861469268798828\n",
      "rank loss: 0.416134238243103\n",
      "rank loss: 0.8312742710113525\n",
      "rank loss: 0.6786713004112244\n",
      "rank loss: 0.5881346464157104\n",
      "rank loss: 0.6396169066429138\n",
      "rank loss: 0.43064650893211365\n",
      "rank loss: 0.5152058005332947\n",
      "rank loss: 0.46038588881492615\n",
      "rank loss: 0.6111962795257568\n",
      "rank loss: 0.46416160464286804\n",
      "rank loss: 0.727708101272583\n",
      "rank loss: 0.593927264213562\n",
      "rank loss: 0.6528255939483643\n",
      "rank loss: 0.5448377728462219\n",
      "rank loss: 0.6585985422134399\n",
      "rank loss: 0.5267945528030396\n",
      "rank loss: 0.6585323810577393\n",
      "rank loss: 0.6769504547119141\n",
      "rank loss: 0.6465094089508057\n",
      "rank loss: 0.5749237537384033\n",
      "rank loss: 0.612128734588623\n",
      "rank loss: 0.5275895595550537\n",
      "rank loss: 0.478982537984848\n",
      "rank loss: 0.49102506041526794\n",
      "rank loss: 0.6519361138343811\n",
      "rank loss: 0.5005003213882446\n",
      "rank loss: 0.66489577293396\n",
      "rank loss: 0.5649515390396118\n",
      "rank loss: 0.5917685627937317\n",
      "rank loss: 0.6089734435081482\n",
      "rank loss: 0.4737705886363983\n",
      "rank loss: 0.48295173048973083\n",
      "rank loss: 0.5672869682312012\n",
      "rank loss: 0.6197073459625244\n",
      "rank loss: 0.7378418445587158\n",
      "rank loss: 0.4723227322101593\n",
      "rank loss: 0.5173442959785461\n",
      "rank loss: 0.5931850671768188\n",
      "rank loss: 0.572117269039154\n",
      "rank loss: 0.5474472641944885\n",
      "rank loss: 0.46916893124580383\n",
      "rank loss: 0.6240032911300659\n",
      "rank loss: 0.4821450412273407\n",
      "rank loss: 0.5696508884429932\n",
      "rank loss: 0.6574133038520813\n",
      "rank loss: 0.7981567978858948\n",
      "rank loss: 0.6132945418357849\n",
      "rank loss: 0.6323904395103455\n",
      "rank loss: 0.5465260744094849\n",
      "rank loss: 0.6044207811355591\n",
      "rank loss: 0.48618847131729126\n",
      "rank loss: 0.7277059555053711\n",
      "rank loss: 0.6153057217597961\n",
      "rank loss: 0.578751266002655\n",
      "rank loss: 0.4560578167438507\n",
      "rank loss: 0.5259663462638855\n",
      "rank loss: 0.494836688041687\n",
      "rank loss: 0.669127345085144\n",
      "rank loss: 0.6074480414390564\n",
      "rank loss: 0.6135276556015015\n",
      "rank loss: 0.5184528827667236\n",
      "rank loss: 0.6236132383346558\n",
      "rank loss: 0.5711992383003235\n",
      "rank loss: 0.5966299176216125\n",
      "rank loss: 0.6481045484542847\n",
      "rank loss: 0.6124857068061829\n",
      "rank loss: 0.5500056743621826\n",
      "rank loss: 0.5904924869537354\n",
      "rank loss: 0.5225329399108887\n",
      "rank loss: 0.5357780456542969\n",
      "rank loss: 0.5645251870155334\n",
      "rank loss: 0.6821000576019287\n",
      "rank loss: 0.5361097455024719\n",
      "rank loss: 0.6172894239425659\n",
      "rank loss: 0.5058383941650391\n",
      "rank loss: 0.580265998840332\n",
      "rank loss: 0.6342681050300598\n",
      "rank loss: 0.6536848545074463\n",
      "rank loss: 0.5142913460731506\n",
      "rank loss: 0.547608494758606\n",
      "rank loss: 0.5499419569969177\n",
      "rank loss: 0.5528458952903748\n",
      "rank loss: 0.5106073617935181\n",
      "rank loss: 0.5816094875335693\n",
      "rank loss: 0.5379308462142944\n",
      "rank loss: 0.5436549782752991\n",
      "rank loss: 0.5887671709060669\n",
      "rank loss: 0.582744836807251\n",
      "rank loss: 0.5486374497413635\n",
      "rank loss: 0.7415382266044617\n",
      "rank loss: 0.48788702487945557\n",
      "rank loss: 0.5204042196273804\n",
      "rank loss: 0.7239530682563782\n",
      "rank loss: 0.5921370387077332\n",
      "rank loss: 0.4380427598953247\n",
      "rank loss: 0.5980404615402222\n",
      "rank loss: 0.5394381880760193\n",
      "rank loss: 0.6364801526069641\n",
      "rank loss: 0.3798960745334625\n",
      "rank loss: 0.6524484157562256\n",
      "rank loss: 0.6198761463165283\n",
      "rank loss: 0.613425612449646\n",
      "rank loss: 0.5656070113182068\n",
      "rank loss: 0.6832817196846008\n",
      "rank loss: 0.5114327669143677\n",
      "rank loss: 0.5734972953796387\n",
      "rank loss: 0.6296449899673462\n",
      "rank loss: 0.5154216289520264\n",
      "rank loss: 0.5901980400085449\n",
      "rank loss: 0.7108048796653748\n",
      "rank loss: 0.46697908639907837\n",
      "rank loss: 0.5847225785255432\n",
      "rank loss: 0.6288083791732788\n",
      "rank loss: 0.4815778136253357\n",
      "rank loss: 0.4103148281574249\n",
      "rank loss: 0.7152631878852844\n",
      "rank loss: 0.565238893032074\n",
      "rank loss: 0.4714294373989105\n",
      "rank loss: 0.7190419435501099\n",
      "rank loss: 0.6247562766075134\n",
      "rank loss: 0.6843271255493164\n",
      "rank loss: 0.4838768243789673\n",
      "rank loss: 0.5110020041465759\n",
      "rank loss: 0.5214651823043823\n",
      "rank loss: 0.6775897741317749\n",
      "rank loss: 0.5412587523460388\n",
      "rank loss: 0.4894298017024994\n",
      "rank loss: 0.47506004571914673\n",
      "rank loss: 0.4418046474456787\n",
      "rank loss: 0.3666473925113678\n",
      "rank loss: 0.607703447341919\n",
      "rank loss: 0.6041910648345947\n",
      "rank loss: 0.5532131195068359\n",
      "rank loss: 0.6179569959640503\n",
      "rank loss: 0.6446533203125\n",
      "rank loss: 0.7925102114677429\n",
      "rank loss: 0.44970542192459106\n",
      "rank loss: 0.7045936584472656\n",
      "rank loss: 0.4668939411640167\n",
      "rank loss: 0.45984914898872375\n",
      "rank loss: 0.5986989736557007\n",
      "rank loss: 0.5746367573738098\n",
      "rank loss: 0.4939366281032562\n",
      "rank loss: 0.5885307788848877\n",
      "rank loss: 0.38029441237449646\n",
      "rank loss: 0.6589481830596924\n",
      "rank loss: 0.6676148772239685\n",
      "rank loss: 0.5105597376823425\n",
      "rank loss: 0.5492029190063477\n",
      "rank loss: 0.5772187113761902\n",
      "rank loss: 0.5149022340774536\n",
      "rank loss: 0.5195131301879883\n",
      "rank loss: 0.5631735920906067\n",
      "rank loss: 0.5240097641944885\n",
      "rank loss: 0.6331295371055603\n",
      "rank loss: 0.5877016186714172\n",
      "rank loss: 0.6182906627655029\n",
      "rank loss: 0.5242790579795837\n",
      "rank loss: 0.6732178926467896\n",
      "rank loss: 0.5248640179634094\n",
      "Time Cost: 1352.1010537147522 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_t = time.time()\n",
    "for i in range(1000):\n",
    "    # One Step Forward\n",
    "    X1, X1_len, X1_id, X2, X2_len, X2_id, Y, F = \\\n",
    "        pair_gen.get_batch(data1=loader.query_data, data2=loader.doc_data)\n",
    "    X1, X1_len, X2, X2_len, Y, F = \\\n",
    "        to_device(X1, X1_len, X2, X2_len, Y, F, device=select_device)\n",
    "    X1, X2, X1_len, X2_len, X2_pos = select_net(X1, X2, X1_len, X2_len, X1_id, X2_id)\n",
    "    X2, X2_len = utils.data_adaptor(X2, X2_len, select_net, rank_net, letor_config)\n",
    "    output = rank_net(X1, X2, X1_len, X2_len, X2_pos)\n",
    "    \n",
    "    # Update Rank Net\n",
    "    rank_loss = rank_net.pair_loss(output, Y)\n",
    "    print('rank loss:', rank_loss.item())\n",
    "    rank_optimizer.zero_grad()\n",
    "    rank_loss.backward()\n",
    "    rank_optimizer.step()\n",
    "    \n",
    "end_t = time.time()\n",
    "print('Time Cost: %s s' % (end_t-start_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(select_net, \"qcentric.model\")\n",
    "torch.save(rank_net, \"deeprank.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[./data/letor/r5w/relation.test.fold1.txt]\n",
      "\tInstance size: 13652\n",
      "List Instance Count: 336\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(50, 20) (50, 2000) (50,)\n",
      "(51, 20) (51, 2000) (51,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(46, 20) (46, 2000) (46,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(50, 20) (50, 2000) (50,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(50, 20) (50, 2000) (50,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(41, 20) (41, 2000) (41,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(80, 20) (80, 2000) (80,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(45, 20) (45, 2000) (45,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(41, 20) (41, 2000) (41,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(50, 20) (50, 2000) (50,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(79, 20) (79, 2000) (79,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(41, 20) (41, 2000) (41,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(41, 20) (41, 2000) (41,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(50, 20) (50, 2000) (50,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(45, 20) (45, 2000) (45,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(41, 20) (41, 2000) (41,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(48, 20) (48, 2000) (48,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(53, 20) (53, 2000) (53,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(50, 20) (50, 2000) (50,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(60, 20) (60, 2000) (60,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "(40, 20) (40, 2000) (40,)\n",
      "[Test] 0.5028438732210702\n"
     ]
    }
   ],
   "source": [
    "select_net_e = torch.load(f='qcentric.model')\n",
    "rank_net_e = torch.load(f='deeprank.model')\n",
    "\n",
    "list_gen = ListGenerator(rel_file=Letor07Path+'/relation.test.fold%d.txt'%(letor_config['fold']),\n",
    "                         config=letor_config)\n",
    "map_v = 0.0\n",
    "map_c = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X1, X1_len, X1_id, X2, X2_len, X2_id, Y, F in \\\n",
    "        list_gen.get_batch(data1=loader.query_data, data2=loader.doc_data):\n",
    "        print(X1.shape, X2.shape, Y.shape)\n",
    "        X1, X1_len, X2, X2_len, Y, F = to_device(X1, X1_len, X2, X2_len, Y, F, device=select_device)\n",
    "        X1, X2, X1_len, X2_len, X2_pos = select_net_e(X1, X2, X1_len, X2_len, X1_id, X2_id)\n",
    "        X2, X2_len = utils.data_adaptor(X2, X2_len, select_net, rank_net, letor_config)\n",
    "        #print(X1.shape, X2.shape, Y.shape)\n",
    "        pred = rank_net_e(X1, X2, X1_len, X2_len, X2_pos)\n",
    "        map_o = utils.eval_MAP(pred.tolist(), Y.tolist())\n",
    "        #print(pred.shape, Y.shape)\n",
    "        map_v += map_o\n",
    "        map_c += 1.0\n",
    "    map_v /= map_c\n",
    "\n",
    "print('[Test]', map_v)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Reward v0: 0.4359386539000405\n",
    "Reward v1: 0.42572616969349864\n",
    "Reward v2: 0.4245778777643799"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nips",
   "language": "python",
   "name": "nips"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
